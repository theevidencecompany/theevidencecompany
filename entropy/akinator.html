<!doctype html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-950TETZ642"></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Statistical test selection • Interactive Decision Tree</title>

  <!-- Google Fonts: Inter 300–700 -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <script>
    // Disable right-click
    document.addEventListener('contextmenu', e => e.preventDefault());

    const redirectHome = () => {
      window.location.href = "index.html";
    };

    // DevTools detection
    setInterval(() => {
      if (
        window.outerWidth - window.innerWidth > 160 ||
        window.outerHeight - window.innerHeight > 160
      ) {
        redirectHome();
      }
    }, 500);

    // Disable DevTools shortcuts
    document.addEventListener('keydown', e => {
      if (
        e.key === "F12" ||
        (e.ctrlKey && e.shiftKey && ["I","J","C"].includes(e.key)) ||
        (e.ctrlKey && e.key === "U")
      ) {
        e.preventDefault();
        redirectHome();
      }
    });
  </script>    
  <style>
    :root
    {
      --brand-primary: #184B44;
      --brand-light: #2a6e64;
      --brand-accent: #e6fffa;
    }

    body
    {
      background-color: #f0fdfa;
      background-image: radial-gradient(#ccfbf1 0.0625rem, transparent 0.0625rem);
      background-size: 1.25rem 1.25rem;
      margin: 0; overflow-x: hidden;
      font-family: 'Inter', sans-serif;
      color: #334155;
      font-size: 1rem;
      line-height: 1.5rem;
    }

    /* Typography utilities (explicit scale; do not rely on defaults) */
    .text-xs { font-size: 0.75rem; line-height: 1rem; font-weight: 400; }
    .text-sm { font-size: 0.875rem; line-height: 1.25rem; }
    .text-base { font-size: 1rem; line-height: 1.5rem; font-weight: 400; }
    .text-xl { font-size: 1.25rem; line-height: 1.75rem; font-weight: 700; }
    .text-3xl { font-size: 1.875rem; line-height: 2.25rem; font-weight: 700; }

    .font-light { font-weight: 300; }
    .font-normal { font-weight: 400; }
    .font-medium { font-weight: 500; }
    .font-semibold { font-weight: 600; }
    .font-bold { font-weight: 700; }

    /* Layout */
    .app { min-height: 100vh; display: flex; flex-direction: column; }
    header {
      position: sticky; top: 0; z-index: 10;
      backdrop-filter: blur(10px);
      background: rgba(240, 253, 250, 0.75);
      border-bottom: 1px solid rgba(20, 184, 166, 0.18);
    }
    .header-inner {
      width: 100%; margin: 0 auto;
      display: grid; grid-template-columns: auto 1fr auto;
      gap: 1rem; align-items: center;
      padding: 0.875rem 0;
    }
    .brand { 
      display: flex; 
      align-items: center; 
      justify-content: flex-start;
      gap: 0; 
      min-width: 0; 
      width: fit-content;
      padding: 0.5rem;
      justify-self: start;
      border-left: 0.0625rem solid rgba(24, 75, 68, 0.15);
    }
    .title-wrap { 
      min-width: 0; 
      text-align: center;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
    }
    .logo {
      width: min(11rem, 100%); 
      height: auto; 
      max-height: 2.0625rem; 
      border-radius: 01 rem;
      padding-left: 1rem;
      object-fit: contain;
      flex-shrink: 0;
    }
    .logo-fallback {
      display: none;
      align-items: center;
      gap: 0.5rem;
      font-family: "Gungsuh", serif;
      font-size: 1.875rem; /* matches text-3xl */
      line-height: 2.25rem;
      font-weight: 700;
      color: var(--brand-primary);
    }
    .logo-fallback .omega { font-size: 1.40625rem; line-height: 2.25rem; }
    .subtitle {
      margin: 0.125rem 0 0 0;
      opacity: 0.88;
    }
    .actions {
      display: flex; align-items: center; gap: 0.625rem; justify-self: end;
      border-right: 0.12rem solid rgba(24, 75, 68, 0.15);
      padding-right: 1rem;
    }

    main {
      max-width: 75rem;
      margin: 1.125rem auto 0 auto;
      padding: 0 1rem 1.25rem 1rem;
      width: 100%;
      flex: 1;
      display: grid;
      grid-template-columns: 1.15fr 0.85fr;
      gap: 1rem;
      align-items: start;
    }

    .card {
      background: rgba(255,255,255,0.70);
      border: 0.0625rem solid rgba(20, 184, 166, 0.18);
      box-shadow: 0 1.125rem 2.5rem rgba(2, 44, 34, 0.10);
      border-radius: 1.375rem;
      overflow: hidden;
    }
    .card-header {
      display: flex; align-items: center; justify-content: space-between;
      padding: 0.75rem 0.75rem 0.625rem 0.75rem;
      border-bottom: 0.0625rem solid rgba(20, 184, 166, 0.14);
      background: linear-gradient(180deg, rgba(230,255,250,0.95), rgba(230,255,250,0.55));
    }
    .pill {
      display: inline-flex; align-items: center; gap: 0.5rem;
      padding: 0.375rem 0.625rem;
      border-radius: 999px;
      background: rgba(24,75,68,0.06);
      border: 0.0625rem solid rgba(24,75,68,0.12);
    }
    .kbd {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.75rem; line-height: 1rem;
      padding: 0.125rem 0.375rem; border-radius: 0.5rem;
      border: 0.0625rem solid rgba(2, 44, 34, 0.18);
      background: rgba(255,255,255,0.7);
    }

    /* Inputs & buttons */
    .input {
      appearance: none;
      border: 0.0625rem solid rgba(24,75,68,0.18);
      background: rgba(255,255,255,0.75);
      border-radius: 0.875rem;
      padding: 0.625rem 0.75rem;
      outline: none;
      width: 100%;
      transition: box-shadow 160ms ease, border-color 160ms ease, transform 160ms ease;
    }
    .input:focus {
      border-color: rgba(24,75,68,0.35);
      box-shadow: 0 0 0 0.25rem rgba(24,75,68,0.12);
    }

    .btn {
      border: 0.0625rem solid rgba(24,75,68,0.18);
      background: rgba(255,255,255,0.70);
      border-radius: 0.875rem;
      padding: 0.625rem 0.75rem;
      cursor: pointer;
      transition: transform 120ms ease, box-shadow 160ms ease, border-color 160ms ease;
      display: inline-flex;
      align-items: center;
      gap: 0.625rem;
      user-select: none;
      color: #0f172a;
    }
    .btn:hover {
      transform: translateY(-0.0625rem);
      border-color: rgba(24,75,68,0.28);
      box-shadow: 0 0.75rem 1.5rem rgba(2, 44, 34, 0.10);
    }
    .btn:active { transform: translateY(0); }
    .btn-primary {
      background: linear-gradient(180deg, rgba(24,75,68,0.95), rgba(24,75,68,0.86));
      color: white;
      border-color: rgba(24,75,68,0.65);
    }
    .btn-primary:hover {
      box-shadow: 0 0.875rem 1.75rem rgba(24,75,68,0.22);
    }
    .btn-ghost {
      background: rgba(230,255,250,0.55);
      border-color: rgba(24,75,68,0.18);
    }
    .btn-row { display: flex; gap: 0.625rem; flex-wrap: wrap; }
    .icon-btn {
      width: 1.875rem; height: 1.875rem;
      min-width: 1.875rem; min-height: 1.875rem;
      border-radius: 0.75rem;
      border: 0.0625rem solid rgba(24,75,68,0.18);
      background: rgba(255,255,255,0.78);
      display: inline-flex; align-items: center; justify-content: center;
      cursor: pointer;
      transition: transform 120ms ease, box-shadow 160ms ease;
    }
    .icon-btn:hover { transform: translateY(-0.0625rem); box-shadow: 0 0.625rem 1.25rem rgba(2,44,34,0.10); }
    .icon {
      width: 1rem; height: 1rem;
      display: inline-block;
      color: var(--brand-primary);
    }

    /* Graph viewport */
    .graph-wrap { height: calc(100vh - 10.75rem); min-height: 35vh; }
    .graph-tools { display: flex; gap: 0.625rem; align-items: center; }
    .graph-canvas {
      position: relative;
      height: 100%;
      background: radial-gradient(75vw 37.5vh at 30% 20%, rgba(24,75,68,0.08), transparent 60%),
                  radial-gradient(50vw 32.5vh at 80% 65%, rgba(42,110,100,0.08), transparent 55%);
    }
    #graphSvg {
      width: 100%;
      height: 100%;
      display: block;
    }
    .legend {
      position: absolute;
      left: 0.5rem; bottom: 4.375rem;
      display: grid;
      gap: 0.375rem;
      padding: 0.625rem 0.75rem;
      border-radius: 1.125rem;
      background: rgba(255,255,255,0.74);
      border: 0.0625rem solid rgba(20, 184, 166, 0.18);
      box-shadow: 0 1.125rem 2.5rem rgba(2, 44, 34, 0.12);
      max-width: calc(100% - 1rem);
    }
    .legend-item { display: flex; align-items: center; gap: 0.5rem; }
    .dot {
      width: 0.625rem; height: 0.625rem; border-radius: 999px;
      background: rgba(24,75,68,0.75);
      flex-shrink: 0;
    }
    .dot.leaf { background: rgba(42,110,100,0.75); }
    .dot.active { box-shadow: 0 0 0 0.25rem rgba(24,75,68,0.18); }

    /* Nodes & edges */
    .edge {
      fill: none;
      stroke: rgba(42,110,100,0.28);
      stroke-width: 0.125rem;
    }
    .edge.active {
      stroke: rgba(24,75,68,0.9);
      stroke-width: 0.1875rem;
      filter: drop-shadow(0 0.5rem 0.625rem rgba(24,75,68,0.22));
      stroke-dasharray: 0.625rem 0.5rem;
      animation: dash 1.2s linear infinite;
    }
    @keyframes dash {
      to { stroke-dashoffset: -2.25rem; }
    }

    .node-ring {
      fill: none;
      stroke: rgba(24,75,68,0.30);
      stroke-width: 0.125rem;
      opacity: 0;
    }
    .node.active .node-ring {
      opacity: 1;
      animation: pulse 1.4s ease-out infinite;
    }
    @keyframes pulse {
      0% { transform: scale(1); opacity: 0.9; }
      100% { transform: scale(1.25); opacity: 0; }
    }

    .node-body {
      fill: rgba(255,255,255,0.88);
      stroke: rgba(24,75,68,0.26);
      stroke-width: 0.09375rem;
      filter: drop-shadow(0 0.625rem 0.875rem rgba(2,44,34,0.12));
    }
    .node.question .node-body {
      fill: rgba(230,255,250,0.95);
      stroke: rgba(24,75,68,0.26);
    }
    .node.leaf .node-body {
      fill: rgba(255,255,255,0.92);
      stroke: rgba(42,110,100,0.28);
    }
    .node.test .node-body {
      fill: rgba(230,255,250,0.90);
      stroke: rgba(42,110,100,0.40);
      stroke-dasharray: 0.375rem 0.25rem;
    }
    .node.test .node-text { font-weight: 600; }

    .node-text {
      fill: #0f172a;
      font-size: 0.75rem;
      line-height: 1rem;
      font-family: 'Inter', sans-serif;
      pointer-events: none;
    }


    /* Muted (de-emphasized) nodes in the current focus window */
    .node.is-muted .node-body {
      fill: rgba(226,232,240,0.78);
      stroke: rgba(148,163,184,0.70);
      filter: none;
    }
    .node.is-muted.question .node-body {
      fill: rgba(226,232,240,0.78);
      stroke: rgba(148,163,184,0.70);
    }
    .node.is-muted.leaf .node-body {
      fill: rgba(226,232,240,0.78);
      stroke: rgba(148,163,184,0.70);
    }
    .node.is-muted .node-text { fill: rgba(100,116,139,0.78); }
    .node.is-muted .node-ring {
      stroke: rgba(148,163,184,0.60);
      animation: none !important;
    }

    .edge.is-muted {
      stroke: rgba(148,163,184,0.55);
      stroke-width: 2;
      filter: none;
    }

    /* Panel */
    .panel-wrap { height: calc(100vh - 10.75rem); min-height: 35vh; display: flex; flex-direction: column; }
    .panel-body { padding: 1rem; flex: 1; min-height: 0; overflow-y: auto; overflow-x: hidden; display: flex; flex-direction: column; gap: 0.75rem; }
    .crumbs {
      display: flex; flex-wrap: wrap; gap: 0.5rem;
    }
    .chip {
      display: inline-flex; align-items: center; gap: 0.5rem;
      padding: 0.375rem 0.625rem;
      border-radius: 999px;
      border: 0.0625rem solid rgba(24,75,68,0.16);
      background: rgba(255,255,255,0.65);
      cursor: pointer;
      transition: transform 120ms ease, box-shadow 160ms ease;
    }
    .chip:hover { transform: translateY(-0.0625rem); box-shadow: 0 0.75rem 1.5rem rgba(2,44,34,0.10); }
    .chip.active {
      border-color: rgba(24,75,68,0.32);
      background: rgba(230,255,250,0.70);
    }

    .question-title {
      display: flex; align-items: flex-start; justify-content: space-between; gap: 0.625rem;
    }
    .question-box {
      padding: 0.875rem;
      border-radius: 1.125rem;
      background: rgba(230,255,250,0.72);
      border: 0.0625rem solid rgba(24,75,68,0.14);
    }
    .helper {
      margin: 0.375rem 0 0 0;
      opacity: 0.82;
    }

    .options {
      display: grid;
      gap: 0.625rem;
    }
    .option-btn {
      width: 100%;
      justify-content: space-between;
      text-align: left;
    }
    .option-left { display: flex; gap: 0.625rem; align-items: center; min-width: 0; }
    .option-badge {
      width: 1.875rem; height: 1.875rem;
      min-width: 1.875rem; min-height: 1.875rem;
      border-radius: 0.75rem;
      background: rgba(24,75,68,0.10);
      border: 0.0625rem solid rgba(24,75,68,0.14);
      display: flex; align-items: center; justify-content: center;
      flex: 0 0 auto;
    }
    .option-text { min-width: 0; }
    .option-text .label {
      display: block;
      color: #0f172a;
      text-align: left;
    }

    .panel-footer {
      padding: 0.75rem 1rem 1rem 1rem;
      border-top: 0.0625rem solid rgba(20, 184, 166, 0.14);
      display: flex; flex-wrap: wrap;
      justify-content: space-between;
      background: linear-gradient(180deg, rgba(255,255,255,0.40), rgba(255,255,255,0.70));
    }

    /* Footer */
    footer {
      padding: 0.001rem 0.001rem 0.001rem 0.001rem;
      width: 100%;
      margin: 0 auto;
      opacity: 0.78;
    }

    /* Modal (overlay must match exactly) */
    .modal-overlay { background-color: rgba(0,0,0,0.5); backdrop-filter: blur(0.25rem); }
    .modal-overlay {
      position: fixed; inset: 0; z-index: 50;
      display: flex; align-items: center; justify-content: center;
      padding: 1.125rem;
    }
    .modal {
      width: min(47.5rem, 90vw);
      max-height: min(78vh, 47.5rem);
      overflow: auto;
      border-radius: 1.375rem;
      background: rgba(255,255,255,0.92);
      border: 0.0625rem solid rgba(20, 184, 166, 0.22);
      box-shadow: 0 1.875rem 4.375rem rgba(0,0,0,0.25);
    }
    .modal-head {
      display: flex; align-items: flex-start; justify-content: space-between; gap: 0.75rem;
      padding: 1rem 1rem 0.75rem 1rem;
      border-bottom: 0.0625rem solid rgba(20, 184, 166, 0.16);
      background: linear-gradient(180deg, rgba(230,255,250,0.90), rgba(255,255,255,0.92));
    }
    .modal-body { padding: 0.875rem 1rem 1rem 1rem; }

    .media-ph {
      width: 100%;
      aspect-ratio: 11 / 6;
      border-radius: 1.125rem;
      border: 0.0625rem dashed rgba(24,75,68,0.28);
      background: linear-gradient(135deg, rgba(230,255,250,0.92), rgba(255,255,255,0.86));
      display: flex; align-items: center; justify-content: center;
      box-shadow: 0 1.125rem 2.5rem rgba(2, 44, 34, 0.10);
      margin: 0 0 0.75rem 0;
    }
    .media-ph img {
      width: 100%;
      height: 100%;
      object-fit: contain;
      border-radius: inherit;
      display: block;
    }

        .ref-list {
      margin: 0.625rem 0 0 0; padding: 0 0 0 1.125rem;
    }
    .ref-list li { margin: 0.375rem 0; }
    .a {
      color: var(--brand-primary);
      text-decoration: none;
    }
    .a:hover { text-decoration: underline; }

    /* Footer */
    footer {
      border-top: 0.0625rem solid rgba(226, 232, 240, 1);
      background-color: #fff;
    }
    .footer-inner {
      max-width: 72rem;
      margin: 0 auto;
      padding: 1.5rem 1rem;
      text-align: center;
    }
    .footer-text {
      font-size: 0.875rem;
      line-height: 1.25rem;
      color: #475569;
    }

    
    /* ===== Motion / transitions (page load + content swaps) ===== */
    :root {
      --ease-out: cubic-bezier(0.2, 0.8, 0.2, 1);
    }

    /* Smooth initial page enter (body starts with class="preload") */
    header, main, footer, main > section.card {
      transition: opacity 420ms ease, transform 560ms var(--ease-out);
    }
    main > section.card { transition-delay: var(--enter-delay, 0ms); }

    body.preload header,
    body.preload main,
    body.preload footer {
      opacity: 0;
      transform: translateY(0.75rem);
    }
    body.preload main > section.card {
      opacity: 0;
      transform: translateY(1.25rem);
    }

    /* Content swaps */
    .anim-swap {
      animation: swapIn 260ms var(--ease-out) both;
    }
    .anim-stagger-item {
      animation: swapIn 260ms var(--ease-out) both;
      animation-delay: var(--delay, 0ms);
    }
    @keyframes swapIn {
      from { opacity: 0; transform: translateY(0.5rem); filter: blur(2px); }
      to   { opacity: 1; transform: translateY(0); filter: blur(0); }
    }

    /* Modal enter/exit */
    .modal-overlay { opacity: 0; transition: opacity 180ms ease; }
    .modal { opacity: 0; transform: translateY(0.75rem) scale(0.99); transition: opacity 200ms ease, transform 240ms var(--ease-out); }
    .modal-overlay.is-open { opacity: 1; }
    .modal-overlay.is-open .modal { opacity: 1; transform: translateY(0) scale(1); }
    .modal-overlay.is-closing { opacity: 0; }
    .modal-overlay.is-closing .modal { opacity: 0; transform: translateY(0.5rem) scale(0.99); }

    @media (prefers-reduced-motion: reduce) {
      header, main, footer, main > section.card, .modal-overlay, .modal { transition: none !important; }
      .anim-swap, .anim-stagger-item, .edge.active, .node.active .node-ring { animation: none !important; }
    }

    @media (max-width: 61.25rem) {
      main { grid-template-columns: 1fr; }
      .graph-wrap { height: 56vh; min-height: 26.25rem; }
      .panel-wrap { height: auto; min-height: 0; }
      .header-inner { grid-template-columns: 1fr; }
      .actions { justify-self: start; }
    }
  </style>
</head>

<body class="preload">
  <div class="app" id="app">
    <header>
      <div class="header-inner">
        <div class="brand">
          <img id="logo" class="logo" src="Entropy.png" alt="Entropy" />
          <div id="logoFallback" class="logo-fallback" aria-hidden="true">
            <span>Gungsuh</span><span class="omega">Ω</span>
          </div>
          <div style="min-width:0">
          </div>
        </div>

        <div class="title-wrap">
          <div class="text-3xl">Stat Test Akinator</div>
          <p class="subtitle text-sm font-normal">Pick an option — watch the Flow Lens animate your path and keep your citations tidy.</p>
        </div>

        <div class="actions">
<button class="btn btn-ghost text-sm font-medium" id="btnRefs">References</button>
          <button class="btn btn-primary text-sm font-medium" id="btnAbout">How it works</button>
        </div>
      </div>
    </header>

    <main>
      <section class="card graph-wrap" aria-label="Flowchart">
        <div class="card-header">
          <div class="pill">
            <span class="dot active"></span>
            <span class="text-sm font-medium">Flow Lens</span>
            <span class="text-xs">Animated map</span>
          </div>
          <div class="graph-tools" style="min-width: min(22.5rem, 90%); max-width: min(32.5rem, 100%);">
            <input id="searchInput" class="input text-sm font-normal" placeholder="Search nodes / tests (e.g., 'ROC', 'ANOVA', 'McNemar')  — press Enter" />
          
            <button class="btn btn-ghost text-sm font-medium" id="btnCenter" style="background-color:beige; color:darkgoldenrod">Centre</button>
</div>
        </div>
        <div class="graph-canvas">
          <svg id="graphSvg" role="img" aria-label="Decision tree flowchart"></svg>

          <div class="legend" aria-hidden="true">
            <div class="legend-item"><span class="dot"></span><span class="text-xs">Question</span></div>
            <div class="legend-item"><span class="dot leaf"></span><span class="text-xs">Recommendation</span></div>
            <div class="legend-item"><span class="dot active"></span><span class="text-xs">Current step</span></div>
            <div class="text-xs" style="opacity:0.8; margin-top: 0.25rem;">Tip: click a node for details.</div>
          </div>
        </div>
      </section>

      <section class="card panel-wrap" aria-label="Decision panel">
        <div class="card-header">
          <div class="pill">
            <span class="dot active"></span>
            <span class="text-sm font-medium">Decision Panel</span>
            <span class="text-xs" id="stepCount">Step</span>
          </div>
          <div class="btn-row">
            <button class="btn btn-ghost text-sm font-medium" id="btnBack">Back</button>
            <button class="btn btn-ghost text-sm font-medium" id="btnRestart" style="background-color:lightpink; color:brown;">Restart</button>
          </div>
        </div>

        <div class="panel-body">
          <div>
            <div class="text-xs" style="opacity: 0.78; margin-bottom: 0.5rem;">Your path</div>
            <div class="crumbs" id="crumbs"></div>
          </div>

          <div class="question-box" id="questionBox" aria-live="polite"></div>

          <div class="options" id="options"></div>
        </div>

        <div class="panel-footer">
          <div class="text-sm font-normal" style="opacity:0.86;">
            Click <span class="kbd">References</span> any time for the full bibliography.
          </div>
          <div class="btn-row" style="margin-top: 8px;">
            <button class="btn btn-primary text-sm font-medium" id="btnExport">Copy path summary</button>
          </div>
        </div>
      </section>
    </main>

    <footer>
      <div class="footer-inner">
        <div class="footer-text">© <span id="currentYear"></span> Entro.py | Threadminds</div>
      </div>
    </footer>
  </div>

  <div id="modalRoot"></div>

  <!-- Set current year in footer -->
  <script>
    document.getElementById('currentYear').textContent = new Date().getFullYear();
  </script>

  <!-- Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <script src="https://unpkg.com/dagre@0.8.5/dist/dagre.min.js"></script>

  <script>
  // Embedded flowchart data (JSON Graph Format)
  const FLOW = {
    "label": "Statistical test selection flowchart",
    "directed": true,
    "metadata": {
      "schema_note": "Nodes are questions or leaves; edges are answer-conditioned transitions. This is a JGF-style directed graph.",
      "references": {
        "JGF_SPEC": {
          "title": "JSON Graph Format Specification",
          "url": "https://jsongraphformat.info/"
        },
        "JGF_GITHUB": {
          "title": "json-graph-specification (GitHub)",
          "url": "https://github.com/jsongraph/json-graph-specification"
        },
        "NIST_EHANDBOOK": {
          "title": "NIST/SEMATECH e-Handbook of Statistical Methods",
          "url": "https://www.itl.nist.gov/div898/handbook/"
        },
        "UCLA_WHATSTAT": {
          "title": "UCLA OARC: What statistical analysis should I use?",
          "url": "https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/"
        },
        "NYU_CHOOSE_TEST": {
          "title": "NYU Quantitative Analysis Guide: Choosing a Statistical Test",
          "url": "https://guides.nyu.edu/quant/choose_test_1DV"
        },
        "NIST_NORMALITY": {
          "title": "NIST: Anderson-Darling and Shapiro-Wilk tests",
          "url": "https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm"
        },
        "NIST_BOXLJUNG": {
          "title": "NIST: Box-Ljung Test",
          "url": "https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm"
        },
        "LOGRANK_PMC": {
          "title": "The logrank test (Bland, 2004) - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/"
        },
        "BLAND_ALTMAN_PUBMED": {
          "title": "Bland & Altman (1986) agreement paper - PubMed",
          "url": "https://pubmed.ncbi.nlm.nih.gov/2868172/"
        },
        "DELONG_PUBMED": {
          "title": "DeLong et al. (1988) correlated ROC AUC comparison - PubMed",
          "url": "https://pubmed.ncbi.nlm.nih.gov/3203132/"
        },
        "ICC_PUBMED": {
          "title": "Shrout & Fleiss (1979) intraclass correlations - PubMed",
          "url": "https://pubmed.ncbi.nlm.nih.gov/18839484/"
        },
        "EGGER_BMJ": {
          "title": "Egger et al. (1997) publication bias test - BMJ",
          "url": "https://www.bmj.com/content/315/7109/629"
        },
        "BH_FDR": {
          "title": "Benjamini & Hochberg (1995) FDR procedure - JRSS B",
          "url": "https://academic.oup.com/jrsssb/article/57/1/289/7035855"
        },
        "GRANGER_1969": {
          "title": "Granger (1969) causality paper (PDF)",
          "url": "https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf"
        },
        "DIEBOLD_MARIANO_1995": {
          "title": "Diebold & Mariano (1995) Comparing Predictive Accuracy",
          "url": "https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599"
        },
        "KPSS_1992": {
          "title": "Kwiatkowski et al. (1992) stationarity test paper",
          "url": "https://www.sciencedirect.com/science/article/pii/030440769290104Y"
        },
        "COCHRANS_Q_NCSS": {
          "title": "NCSS: Cochran’s Q Test (binary matched sets) (PDF)",
          "url": "https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf"
        },
        "MCNEMAR_NCSS": {
          "title": "NCSS/PASS: McNemar test (paired proportions) (PDF)",
          "url": "https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf"
        },
        "COCHRANE_HANDBOOK": {
          "title": "Cochrane Handbook (Chapter 10: meta-analysis)",
          "url": "https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10"
        },
        "MANTEL_HAENSZEL_WHO": {
          "title": "IARC/WHO text describing Mantel–Haenszel method (PDF)",
          "url": "https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf"
        },
        "COCHRAN_Q_PUBMED": {
          "title": "Hoaglin (2016) paper on Q and heterogeneity - PubMed",
          "url": "https://pubmed.ncbi.nlm.nih.gov/26303773/"
        },
        "CA_TREND_SCIDIR": {
          "title": "Neuhäuser (1999) Cochran–Armitage trend test note",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917"
        },
        "ICH_E9_FDA_PDF": {
          "title": "International Council for Harmonisation (ICH) E9: Statistical Principles for Clinical Trials (FDA-hosted PDF; Step 5, 1998)",
          "url": "https://www.fda.gov/media/71336/download"
        },
        "ICH_E9R1_FDA_PDF": {
          "title": "ICH E9(R1) Addendum: Estimands and Sensitivity Analysis in Clinical Trials (FDA-hosted PDF; final, 2021)",
          "url": "https://www.fda.gov/media/148473/download"
        },
        "FDA_MULTIPLE_ENDPOINTS": {
          "title": "FDA Guidance for Industry: Multiple Endpoints in Clinical Trials (January 2017)",
          "url": "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/multiple-endpoints-clinical-trials"
        },
        "ISO_14155_2020": {
          "title": "ISO 14155:2020 — Clinical investigation of medical devices for human subjects — Good clinical practice (catalogue page)",
          "url": "https://www.iso.org/standard/71690.html"
        }
      }
    },
    "nodes": [
      {
        "id": "START",
        "label": "START",
        "metadata": {
          "kind": "start",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "Q0",
        "label": "What is the primary aim of your research question?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "A1",
        "label": "A) Compare groups/conditions (differences) — what is the outcome (dependent variable) type?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "A2",
        "label": "Outcome is continuous (interval/ratio): how many groups/conditions are you comparing?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "A3",
        "label": "One group: are you testing a population mean against a known value?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "L_A3",
        "label": "Use a 1-sample mean test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "One-sample t-test (σ unknown)",
            "One-sample z-test (σ known)"
          ],
          "notes": "Prefer t-test unless population σ is known and normality is plausible; for large n, t is robust."
        }
      },
      {
        "id": "A4",
        "label": "If not a mean: are you testing median/paired-symmetry/typical value shift?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A4_MED",
        "label": "Nonparametric 1-sample location test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Wilcoxon signed-rank (requires symmetric differences)",
            "Sign test"
          ],
          "notes": "Signed-rank has more power under symmetry; sign test is minimal-assumption."
        }
      },
      {
        "id": "L_A4_PERM",
        "label": "Distribution-free alternative",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "One-sample permutation/randomization test"
          ],
          "notes": "Use if you can justify exchangeability under the null."
        }
      },
      {
        "id": "A5",
        "label": "Two groups: are observations paired/repeated (same units measured twice or matched pairs)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "A7",
        "label": "Independent groups: is the outcome approximately normal within each group (or n large enough)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_NORMALITY",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "A8",
        "label": "Independent groups: are variances approximately equal (homoscedasticity)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A8_EQ",
        "label": "Parametric 2-group mean test (equal variances)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Student two-sample t-test (pooled variance)"
          ]
        }
      },
      {
        "id": "L_A8_NEQ",
        "label": "Parametric 2-group mean test (unequal variances)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Welch two-sample t-test"
          ]
        }
      },
      {
        "id": "L_A7_NONPARAM",
        "label": "Nonparametric 2-group location test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Mann–Whitney U / Wilcoxon rank-sum",
            "Brunner–Munzel (if stochastic dominance questionable)",
            "Permutation test for difference in means/medians"
          ],
          "notes": "Rank-sum targets stochastic ordering; Brunner–Munzel relaxes equal-shape assumption."
        }
      },
      {
        "id": "A9",
        "label": "Paired groups: are the within-pair differences approximately normal (or n large enough)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_NORMALITY",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A9_T",
        "label": "Parametric paired mean test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Paired t-test"
          ]
        }
      },
      {
        "id": "L_A9_NONPARAM",
        "label": "Nonparametric paired test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Wilcoxon signed-rank",
            "Sign test",
            "Paired permutation/randomization test"
          ]
        }
      },
      {
        "id": "A10",
        "label": "≥3 groups/conditions: what is the design structure?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "A11B",
        "label": "Between-subjects (independent groups): are residuals approximately normal?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_NORMALITY",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "A13",
        "label": "Between-subjects: are variances approximately equal across groups?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_A13_ANOVA",
        "label": "Parametric omnibus test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "One-way ANOVA"
          ]
        }
      },
      {
        "id": "L_A13_WELCH",
        "label": "Parametric omnibus test robust to unequal variances",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Welch ANOVA"
          ]
        }
      },
      {
        "id": "L_A11B_NONPARAM",
        "label": "Nonparametric omnibus test",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Kruskal–Wallis test",
            "Permutation ANOVA (randomization)",
            "Robust ANOVA / trimmed-means ANOVA"
          ],
          "notes": "Kruskal–Wallis targets stochastic ordering; permutation ANOVA uses exchangeability."
        }
      },
      {
        "id": "A14",
        "label": "If omnibus is significant and you need follow-up comparisons, what is your comparison plan?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "L_A14_PAIRWISE_EQ",
        "label": "All pairwise (equal-variance parametric path)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Tukey HSD"
          ]
        }
      },
      {
        "id": "L_A14_PAIRWISE_NEQ",
        "label": "All pairwise (unequal-variance parametric path)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Games–Howell"
          ]
        }
      },
      {
        "id": "L_A14_VS_CONTROL",
        "label": "Each group vs control",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Dunnett test"
          ]
        }
      },
      {
        "id": "L_A14_CONTRASTS",
        "label": "Planned contrasts / linear hypotheses",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "t contrasts within ANOVA/GLM framework",
            "Linear hypothesis tests with multiplicity control"
          ],
          "notes": "Use multiplicity control (e.g., Holm/BH) if many contrasts."
        }
      },
      {
        "id": "A11W",
        "label": "Within-subjects (repeated measures): do parametric assumptions (normality, sphericity) hold?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A11W_RM",
        "label": "Parametric repeated-measures omnibus",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Repeated-measures ANOVA (with GG/HF correction if needed)"
          ]
        }
      },
      {
        "id": "L_A11W_NONPARAM",
        "label": "Nonparametric/robust repeated-measures",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Friedman test",
            "Mixed-effects model (LMM)"
          ],
          "notes": "Friedman is nonparametric for ranked repeated measures; LMM handles missingness/complex covariance."
        }
      },
      {
        "id": "L_A10_MIXED",
        "label": "Mixed designs / hierarchical data",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Linear mixed-effects model (LMM)",
            "Generalized linear mixed model (GLMM)"
          ],
          "notes": "Test fixed effects via likelihood ratio / Wald / score tests; use appropriate covariance structure."
        }
      },
      {
        "id": "A16",
        "label": "Outcome is ordinal (Likert/ranks): how many groups/conditions and is pairing present?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "L_A16_1S",
        "label": "1-sample ordinal location",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Sign test",
            "Wilcoxon signed-rank"
          ]
        }
      },
      {
        "id": "L_A16_2IND",
        "label": "2 independent ordinal groups",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Mann–Whitney U / Wilcoxon rank-sum",
            "Brunner–Munzel"
          ]
        }
      },
      {
        "id": "L_A16_2PAIR",
        "label": "2 paired ordinal conditions",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Wilcoxon signed-rank",
            "Sign test"
          ]
        }
      },
      {
        "id": "L_A16_KIND",
        "label": "k independent ordinal groups",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Kruskal–Wallis"
          ]
        }
      },
      {
        "id": "L_A16_KPAIR",
        "label": "k paired ordinal conditions",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Friedman test"
          ]
        }
      },
      {
        "id": "A17",
        "label": "Outcome is binary (0/1): what is the design?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "A18",
        "label": "Independent 2x2: are expected cell counts sufficiently large?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A17_1S_EXACT",
        "label": "One-sample binary proportion",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Exact binomial test",
            "One-sample proportion z-test (large n)"
          ]
        }
      },
      {
        "id": "L_A17_PAIRED",
        "label": "Paired binary (before/after; matched)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "MCNEMAR_NCSS"
          ],
          "tests": [
            "McNemar test"
          ]
        }
      },
      {
        "id": "L_A18_CHI",
        "label": "Independent 2x2 large-sample association/difference in proportions",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Pearson chi-square test",
            "Two-proportion z-test"
          ],
          "notes": "Equivalent in 2x2 under standard conditions."
        }
      },
      {
        "id": "L_A18_FISHER",
        "label": "Independent 2x2 small-sample exact",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Fisher exact test",
            "Exact unconditional tests"
          ],
          "notes": "Prefer exact when expected counts are small."
        }
      },
      {
        "id": "L_A17_KIND",
        "label": "k independent proportions (≥2 groups)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Chi-square test of homogeneity/independence"
          ]
        }
      },
      {
        "id": "L_A17_KPAIR",
        "label": "k related proportions (repeated measures)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "COCHRANS_Q_NCSS"
          ],
          "tests": [
            "Cochran’s Q test"
          ]
        }
      },
      {
        "id": "A19",
        "label": "Outcome is nominal categorical (≥2 unordered categories): what is the question?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_A19_GOF",
        "label": "Does observed distribution match a specified distribution?",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Chi-square goodness-of-fit (multinomial)",
            "Exact multinomial tests (small n)"
          ]
        }
      },
      {
        "id": "L_A19_IND",
        "label": "Are two categorical variables associated?",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Chi-square test of independence",
            "Fisher–Freeman–Halton exact (RxC small n)"
          ]
        }
      },
      {
        "id": "L_A19_PAIREDK",
        "label": "Paired nominal with >2 categories",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Stuart–Maxwell test (marginal homogeneity)",
            "Bowker test (symmetry)"
          ]
        }
      },
      {
        "id": "A20",
        "label": "Outcome is counts/rates: do you have an exposure/offset (time at risk, area, etc.)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_A20_RATE",
        "label": "Rate comparison / incidence",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Poisson rate test / exact rate ratio test",
            "Poisson regression with offset"
          ]
        }
      },
      {
        "id": "A21",
        "label": "Are counts overdispersed or zero-inflated relative to Poisson?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_A21_NB",
        "label": "Use overdispersion-aware models",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Negative binomial regression",
            "Quasi-Poisson",
            "Zero-inflated Poisson/NB"
          ]
        }
      },
      {
        "id": "A22",
        "label": "Outcome is time-to-event: comparing survival curves between groups?",
        "metadata": {
          "kind": "question",
          "refs": [
            "LOGRANK_PMC"
          ]
        }
      },
      {
        "id": "L_A22_LOGRANK",
        "label": "Nonparametric survival curve comparison",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "LOGRANK_PMC"
          ],
          "tests": [
            "Log-rank test"
          ],
          "notes": "Compares survival distributions across ≥2 groups."
        }
      },
      {
        "id": "A23",
        "label": "Need covariate adjustment or time-varying predictors?",
        "metadata": {
          "kind": "question",
          "refs": [
            "LOGRANK_PMC"
          ]
        }
      },
      {
        "id": "L_A23_COX",
        "label": "Semi-parametric survival regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "LOGRANK_PMC"
          ],
          "tests": [
            "Cox proportional hazards model"
          ],
          "notes": "Test predictors via likelihood ratio / Wald / score tests."
        }
      },
      {
        "id": "L_A23_AFT",
        "label": "Parametric survival regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Accelerated failure time (AFT) models",
            "Parametric PH models (Weibull, log-logistic, etc.)"
          ]
        }
      },
      {
        "id": "A24",
        "label": "Multivariate continuous outcome (vectors) or many correlated outcomes?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_A24_HOT1",
        "label": "One-sample multivariate mean",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Hotelling’s T² (one-sample)"
          ]
        }
      },
      {
        "id": "L_A24_HOT2",
        "label": "Two-sample multivariate mean",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Hotelling’s T² (two-sample)"
          ]
        }
      },
      {
        "id": "L_A24_MANOVA",
        "label": "Multiple outcomes with group factor(s)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "MANOVA"
          ]
        }
      },
      {
        "id": "L_A24_PERMANOVA",
        "label": "Nonparametric multivariate distance-based",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "PERMANOVA (permutation MANOVA)"
          ]
        }
      },
      {
        "id": "B1",
        "label": "B) Association between variables — what are the variable types?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NYU_CHOOSE_TEST"
          ]
        }
      },
      {
        "id": "B2",
        "label": "Two continuous variables: is relationship approximately linear and variables approximately normal?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_B2_PEARSON",
        "label": "Linear association",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Pearson correlation",
            "Simple linear regression t-test on slope"
          ]
        }
      },
      {
        "id": "B3",
        "label": "If non-normal/nonlinear: is association monotonic?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_B3_RANK",
        "label": "Monotonic association",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Spearman rank correlation",
            "Kendall’s tau"
          ]
        }
      },
      {
        "id": "L_B3_NONLIN",
        "label": "General dependence",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Distance correlation (often via permutation)",
            "Mutual information tests (resampling)"
          ]
        }
      },
      {
        "id": "B4",
        "label": "Ordinal/Rank variables: paired ranks or independent?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_B4_ORD",
        "label": "Rank-based association",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Spearman",
            "Kendall"
          ],
          "notes": "For ordered categorical with ties, consider polychoric/polyserial correlation models."
        }
      },
      {
        "id": "B5",
        "label": "Binary vs continuous: is the continuous approximately normal within binary groups?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_B5_PBIS",
        "label": "Binary-continuous association",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Point-biserial correlation",
            "Two-sample t-test equivalence"
          ],
          "notes": "Point-biserial is Pearson with 0/1 coding."
        }
      },
      {
        "id": "B6",
        "label": "Nominal vs nominal: independent or paired?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_B6_CHI",
        "label": "Association of two categorical variables",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Chi-square test of independence",
            "Fisher exact (small counts)"
          ]
        }
      },
      {
        "id": "L_B6_PAIRED",
        "label": "Paired nominal association",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "MCNEMAR_NCSS"
          ],
          "tests": [
            "McNemar (2x2)",
            "Stuart–Maxwell/Bowker (>2 categories)"
          ]
        }
      },
      {
        "id": "B7",
        "label": "Binary proportions across ordered groups: testing for trend?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_B7_TREND",
        "label": "Trend in proportions",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "CA_TREND_SCIDIR",
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Cochran–Armitage trend test"
          ],
          "notes": "For ordered exposure/dose response."
        }
      },
      {
        "id": "C1",
        "label": "C) Prediction/Modeling — what is the response variable type?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "C2",
        "label": "Continuous response: linear model assumptions reasonable?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_C2_LM",
        "label": "Continuous regression/GLM",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK",
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Linear regression (OLS)",
            "ANCOVA (continuous + categorical predictors)",
            "Multiple regression"
          ],
          "notes": "Use robust SEs if heteroskedasticity."
        }
      },
      {
        "id": "L_C2_ALT",
        "label": "If assumptions violated or relationship nonlinear",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Robust regression (Huber/M-estimators)",
            "Quantile regression",
            "Generalized additive models (GAM)"
          ]
        }
      },
      {
        "id": "C3",
        "label": "Binary response: modeling probability of success?",
        "metadata": {
          "kind": "question",
          "refs": [
            "UCLA_WHATSTAT"
          ]
        }
      },
      {
        "id": "L_C3_LOGIT",
        "label": "Binary regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "UCLA_WHATSTAT"
          ],
          "tests": [
            "Logistic regression",
            "Probit regression"
          ]
        }
      },
      {
        "id": "C4",
        "label": "Count response: Poisson-like?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_C4_POIS",
        "label": "Count regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Poisson regression (with offset if needed)"
          ]
        }
      },
      {
        "id": "L_C4_NB",
        "label": "Overdispersion/zero inflation",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Negative binomial regression",
            "Quasi-Poisson",
            "Zero-inflated models"
          ]
        }
      },
      {
        "id": "C5",
        "label": "Ordinal response: ordered categories?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_C5_ORD",
        "label": "Ordinal regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Ordinal logistic (proportional odds)",
            "Ordinal probit"
          ]
        }
      },
      {
        "id": "C6",
        "label": "Nominal multiclass response: unordered categories?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_C6_MULTI",
        "label": "Multinomial regression",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Multinomial logistic regression"
          ]
        }
      },
      {
        "id": "C7",
        "label": "Time-to-event response: censored outcomes?",
        "metadata": {
          "kind": "question",
          "refs": [
            "LOGRANK_PMC"
          ]
        }
      },
      {
        "id": "L_C7_SURV",
        "label": "Survival modeling",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "LOGRANK_PMC"
          ],
          "tests": [
            "Cox proportional hazards",
            "AFT models"
          ],
          "notes": "Use partial likelihood inference; check PH assumptions."
        }
      },
      {
        "id": "C8",
        "label": "Clustered/longitudinal data: repeated measures or multi-level structure?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_C8_MIXED",
        "label": "Mixed model / random effects",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "LMM/GLMM"
          ],
          "notes": "Random intercepts/slopes; subject-specific inference."
        }
      },
      {
        "id": "L_C8_GEE",
        "label": "Marginal model / correlated outcomes",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Generalized Estimating Equations (GEE)"
          ],
          "notes": "Population-average inference."
        }
      },
      {
        "id": "D1",
        "label": "D) Agreement / reliability — what is the measurement scale?",
        "metadata": {
          "kind": "question",
          "refs": [
            "BLAND_ALTMAN_PUBMED",
            "ICC_PUBMED"
          ]
        }
      },
      {
        "id": "D2",
        "label": "Continuous measurements: agreement between two methods/raters?",
        "metadata": {
          "kind": "question",
          "refs": [
            "BLAND_ALTMAN_PUBMED",
            "ICC_PUBMED"
          ]
        }
      },
      {
        "id": "L_D2_BA",
        "label": "Method comparison",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "BLAND_ALTMAN_PUBMED"
          ],
          "tests": [
            "Bland–Altman analysis (limits of agreement)"
          ]
        }
      },
      {
        "id": "L_D2_ICC",
        "label": "Rater reliability (continuous)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "ICC_PUBMED"
          ],
          "tests": [
            "Intraclass correlation coefficient (ICC)"
          ],
          "notes": "Choose ICC form per design (Shrout & Fleiss)."
        }
      },
      {
        "id": "D3",
        "label": "Categorical labels: two raters or many raters?",
        "metadata": {
          "kind": "question",
          "refs": [
            "ICC_PUBMED"
          ]
        }
      },
      {
        "id": "L_D3_KAPPA2",
        "label": "Two raters categorical agreement",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "ICC_PUBMED"
          ],
          "tests": [
            "Cohen’s kappa (weighted kappa if ordinal)"
          ]
        }
      },
      {
        "id": "L_D3_KAPPAM",
        "label": "Many raters categorical agreement",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "ICC_PUBMED"
          ],
          "tests": [
            "Fleiss’ kappa",
            "Krippendorff’s alpha"
          ]
        }
      },
      {
        "id": "D4",
        "label": "Compare two classifiers on paired data?",
        "metadata": {
          "kind": "question",
          "refs": [
            "DELONG_PUBMED",
            "MCNEMAR_NCSS"
          ]
        }
      },
      {
        "id": "L_D4_MCNEMAR",
        "label": "Paired accuracy differences (binary correct/incorrect)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "MCNEMAR_NCSS"
          ],
          "tests": [
            "McNemar test"
          ]
        }
      },
      {
        "id": "L_D4_DELONG",
        "label": "Compare ROC AUCs (correlated curves)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "DELONG_PUBMED"
          ],
          "tests": [
            "DeLong test for AUC differences"
          ]
        }
      },
      {
        "id": "E1",
        "label": "E) Diagnostics / assumptions — what do you need to check?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK",
            "NIST_NORMALITY"
          ]
        }
      },
      {
        "id": "E2",
        "label": "Normality check for residuals/sample?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_NORMALITY"
          ]
        }
      },
      {
        "id": "L_E2_NORM",
        "label": "Normality tests",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_NORMALITY"
          ],
          "tests": [
            "Shapiro–Wilk",
            "Anderson–Darling",
            "Kolmogorov–Smirnov (with estimated params caution)"
          ]
        }
      },
      {
        "id": "E3",
        "label": "Equal variance / homoscedasticity check?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_E3_VAR",
        "label": "Variance equality tests",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Levene / Brown–Forsythe",
            "Bartlett (normality-sensitive)"
          ]
        }
      },
      {
        "id": "E4",
        "label": "Independence/autocorrelation in residuals?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_BOXLJUNG"
          ]
        }
      },
      {
        "id": "L_E4_RUNS",
        "label": "Randomness/independence",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Runs test"
          ]
        }
      },
      {
        "id": "L_E4_DW",
        "label": "Serial correlation in regression residuals",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Durbin–Watson",
            "Breusch–Godfrey"
          ]
        }
      },
      {
        "id": "L_E4_LJUNG",
        "label": "Overall autocorrelation across lags",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_BOXLJUNG"
          ],
          "tests": [
            "Box–Ljung test"
          ]
        }
      },
      {
        "id": "E5",
        "label": "Goodness-of-fit / nested model comparison?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_E5_GOF",
        "label": "Distribution/model GOF",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Chi-square GOF (categorical)",
            "Continuous GOF tests (AD/KS)",
            "Likelihood ratio test for nested models"
          ]
        }
      },
      {
        "id": "E6",
        "label": "Outliers/influence in regression?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_E6_OUT",
        "label": "Outlier/influence diagnostics",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Grubbs (single outlier, normal)",
            "Generalized ESD (Rosner)",
            "Cook’s distance / leverage"
          ],
          "notes": "Use influence diagnostics rather than deleting by default."
        }
      },
      {
        "id": "F1",
        "label": "F) Time series — what is the primary goal?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_BOXLJUNG",
            "GRANGER_1969",
            "DIEBOLD_MARIANO_1995",
            "KPSS_1992"
          ]
        }
      },
      {
        "id": "F2",
        "label": "Stationarity / unit root question?",
        "metadata": {
          "kind": "question",
          "refs": [
            "KPSS_1992"
          ]
        }
      },
      {
        "id": "L_F2_UNITROOT",
        "label": "Stationarity / unit root tests",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "KPSS_1992"
          ],
          "tests": [
            "ADF (Augmented Dickey–Fuller)",
            "KPSS"
          ],
          "notes": "ADF tests unit root null; KPSS tests stationarity null (complementary)."
        }
      },
      {
        "id": "F3",
        "label": "Model diagnostics: are ARIMA/ARMA residuals white noise?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_BOXLJUNG"
          ]
        }
      },
      {
        "id": "L_F3_LJUNG",
        "label": "Time series residual autocorrelation",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_BOXLJUNG"
          ],
          "tests": [
            "Box–Ljung test"
          ]
        }
      },
      {
        "id": "F4",
        "label": "Causality/predictive precedence between series?",
        "metadata": {
          "kind": "question",
          "refs": [
            "GRANGER_1969"
          ]
        }
      },
      {
        "id": "L_F4_GRANGER",
        "label": "Predictive causality",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "GRANGER_1969"
          ],
          "tests": [
            "Granger causality tests"
          ]
        }
      },
      {
        "id": "F5",
        "label": "Structural break / regime change?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_F5_BREAK",
        "label": "Break tests",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Chow test",
            "CUSUM/CUSUMSQ",
            "Bai–Perron multiple break tests"
          ]
        }
      },
      {
        "id": "F6",
        "label": "Compare forecast accuracy between two models?",
        "metadata": {
          "kind": "question",
          "refs": [
            "DIEBOLD_MARIANO_1995"
          ]
        }
      },
      {
        "id": "L_F6_DM",
        "label": "Forecast comparison",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "DIEBOLD_MARIANO_1995"
          ],
          "tests": [
            "Diebold–Mariano test"
          ]
        }
      },
      {
        "id": "G1",
        "label": "G) Combine results across studies (meta-analysis) — what is the outcome metric?",
        "metadata": {
          "kind": "question",
          "refs": [
            "EGGER_BMJ",
            "BH_FDR",
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_G1_CONT",
        "label": "Continuous effects (e.g., mean difference, SMD)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "COCHRANE_HANDBOOK"
          ],
          "tests": [
            "Inverse-variance fixed-effect meta-analysis",
            "Random-effects meta-analysis (e.g., DerSimonian–Laird, REML)"
          ]
        }
      },
      {
        "id": "L_G1_BIN",
        "label": "Binary effects (risk ratio/odds ratio)",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "COCHRANE_HANDBOOK",
            "MANTEL_HAENSZEL_WHO"
          ],
          "tests": [
            "Mantel–Haenszel fixed-effect pooling",
            "Inverse-variance pooling on log scale"
          ]
        }
      },
      {
        "id": "L_G1_HET",
        "label": "Assess heterogeneity",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "COCHRANE_HANDBOOK",
            "COCHRAN_Q_PUBMED"
          ],
          "tests": [
            "Cochran’s Q",
            "I²"
          ],
          "notes": "Use Q to test, I² to quantify heterogeneity."
        }
      },
      {
        "id": "L_G1_BIAS",
        "label": "Assess small-study/publication bias",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "EGGER_BMJ"
          ],
          "tests": [
            "Egger regression test",
            "Funnel plot asymmetry tests"
          ]
        }
      },
      {
        "id": "L_G1_MULTTEST",
        "label": "Multiple comparisons across many endpoints/studies",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "BH_FDR"
          ],
          "tests": [
            "Benjamini–Hochberg FDR control",
            "Holm/Hochberg familywise control"
          ]
        }
      },
      {
        "id": "H1",
        "label": "H) Custom/advanced inference — do you have an explicit likelihood model?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_H1_CLASSIC",
        "label": "Likelihood-based tests",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Likelihood ratio test (LRT)",
            "Wald test",
            "Score/Lagrange multiplier test"
          ],
          "notes": "Often asymptotically equivalent; choose based on robustness and computation."
        }
      },
      {
        "id": "H2",
        "label": "Can you justify exchangeability/randomization under the null (e.g., via design)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_H2_PERM",
        "label": "Resampling-based inference",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Permutation tests",
            "Randomization tests",
            "Bootstrap CIs/p-values"
          ],
          "notes": "Permutation requires valid null exchangeability; bootstrap targets sampling distribution."
        }
      },
      {
        "id": "H3",
        "label": "Are Bayesian methods acceptable for your question (priors, posterior inference)?",
        "metadata": {
          "kind": "question",
          "refs": [
            "NIST_EHANDBOOK"
          ]
        }
      },
      {
        "id": "L_H3_BF",
        "label": "Bayesian model comparison",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "Bayes factors",
            "Posterior model probabilities"
          ]
        }
      },
      {
        "id": "L_H3_MCMC",
        "label": "Bayesian estimation",
        "metadata": {
          "kind": "leaf",
          "refs": [
            "NIST_EHANDBOOK"
          ],
          "tests": [
            "MCMC/VI posterior inference",
            "Posterior predictive checks"
          ]
        }
      }
    ],
    "edges": [
      {
        "source": "START",
        "target": "Q0",
        "label": "Begin"
      },
      {
        "source": "Q0",
        "target": "A1",
        "label": "A) Differences between groups/conditions",
        "metadata": {
          "option_code": "A"
        }
      },
      {
        "source": "Q0",
        "target": "B1",
        "label": "B) Association/correlation",
        "metadata": {
          "option_code": "B"
        }
      },
      {
        "source": "Q0",
        "target": "C1",
        "label": "C) Prediction/modeling",
        "metadata": {
          "option_code": "C"
        }
      },
      {
        "source": "Q0",
        "target": "D1",
        "label": "D) Agreement/reliability",
        "metadata": {
          "option_code": "D"
        }
      },
      {
        "source": "Q0",
        "target": "E1",
        "label": "E) Diagnostics/assumptions",
        "metadata": {
          "option_code": "E"
        }
      },
      {
        "source": "Q0",
        "target": "F1",
        "label": "F) Time series",
        "metadata": {
          "option_code": "F"
        }
      },
      {
        "source": "Q0",
        "target": "G1",
        "label": "G) Meta-analysis/evidence synthesis",
        "metadata": {
          "option_code": "G"
        }
      },
      {
        "source": "Q0",
        "target": "H1",
        "label": "H) Advanced/custom inference",
        "metadata": {
          "option_code": "H"
        }
      },
      {
        "source": "A1",
        "target": "A2",
        "label": "Continuous outcome",
        "metadata": {
          "option_code": "continuous"
        }
      },
      {
        "source": "A1",
        "target": "A16",
        "label": "Ordinal outcome",
        "metadata": {
          "option_code": "ordinal"
        }
      },
      {
        "source": "A1",
        "target": "A17",
        "label": "Binary outcome",
        "metadata": {
          "option_code": "binary"
        }
      },
      {
        "source": "A1",
        "target": "A19",
        "label": "Nominal categorical outcome",
        "metadata": {
          "option_code": "nominal"
        }
      },
      {
        "source": "A1",
        "target": "A20",
        "label": "Count/rate outcome",
        "metadata": {
          "option_code": "count"
        }
      },
      {
        "source": "A1",
        "target": "A22",
        "label": "Time-to-event outcome",
        "metadata": {
          "option_code": "survival"
        }
      },
      {
        "source": "A1",
        "target": "A24",
        "label": "Multivariate outcome",
        "metadata": {
          "option_code": "multivariate"
        }
      },
      {
        "source": "A2",
        "target": "A3",
        "label": "1 group",
        "metadata": {
          "option_code": "1"
        }
      },
      {
        "source": "A2",
        "target": "A5",
        "label": "2 groups",
        "metadata": {
          "option_code": "2"
        }
      },
      {
        "source": "A2",
        "target": "A10",
        "label": "3+ groups/conditions",
        "metadata": {
          "option_code": "3plus"
        }
      },
      {
        "source": "A3",
        "target": "L_A3",
        "label": "Yes: mean vs known value",
        "metadata": {
          "option_code": "yes"
        }
      },
      {
        "source": "A3",
        "target": "A4",
        "label": "No: not (only) mean",
        "metadata": {
          "option_code": "no"
        }
      },
      {
        "source": "A4",
        "target": "L_A4_MED",
        "label": "Yes: median/typical shift",
        "metadata": {
          "option_code": "yes"
        }
      },
      {
        "source": "A4",
        "target": "L_A4_PERM",
        "label": "No: general shift/difference",
        "metadata": {
          "option_code": "no"
        }
      },
      {
        "source": "A5",
        "target": "A7",
        "label": "No: independent groups",
        "metadata": {
          "option_code": "independent"
        }
      },
      {
        "source": "A5",
        "target": "A9",
        "label": "Yes: paired/matched/repeated",
        "metadata": {
          "option_code": "paired"
        }
      },
      {
        "source": "A7",
        "target": "A8",
        "label": "Yes: approx normal/large n",
        "metadata": {
          "option_code": "normal"
        }
      },
      {
        "source": "A7",
        "target": "L_A7_NONPARAM",
        "label": "No: non-normal/ordinal-like",
        "metadata": {
          "option_code": "nonnormal"
        }
      },
      {
        "source": "A8",
        "target": "L_A8_EQ",
        "label": "Yes: equal variances",
        "metadata": {
          "option_code": "equalvar"
        }
      },
      {
        "source": "A8",
        "target": "L_A8_NEQ",
        "label": "No/unsure: unequal variances",
        "metadata": {
          "option_code": "unequalvar"
        }
      },
      {
        "source": "A9",
        "target": "L_A9_T",
        "label": "Yes: differences approx normal",
        "metadata": {
          "option_code": "normal"
        }
      },
      {
        "source": "A9",
        "target": "L_A9_NONPARAM",
        "label": "No: non-normal differences",
        "metadata": {
          "option_code": "nonnormal"
        }
      },
      {
        "source": "A10",
        "target": "A11B",
        "label": "Between-subjects groups",
        "metadata": {
          "option_code": "between"
        }
      },
      {
        "source": "A10",
        "target": "A11W",
        "label": "Within-subjects repeated measures",
        "metadata": {
          "option_code": "within"
        }
      },
      {
        "source": "A10",
        "target": "L_A10_MIXED",
        "label": "Mixed/hierarchical/clustered",
        "metadata": {
          "option_code": "mixed"
        }
      },
      {
        "source": "A11B",
        "target": "A13",
        "label": "Yes: approx normal residuals",
        "metadata": {
          "option_code": "normal"
        }
      },
      {
        "source": "A11B",
        "target": "L_A11B_NONPARAM",
        "label": "No: non-normal/heterogeneous",
        "metadata": {
          "option_code": "nonnormal"
        }
      },
      {
        "source": "A13",
        "target": "L_A13_ANOVA",
        "label": "Yes: equal variances",
        "metadata": {
          "option_code": "equalvar"
        }
      },
      {
        "source": "A13",
        "target": "L_A13_WELCH",
        "label": "No: unequal variances",
        "metadata": {
          "option_code": "unequalvar"
        }
      },
      {
        "source": "L_A13_ANOVA",
        "target": "A14",
        "label": "Need post-hoc comparisons",
        "metadata": {
          "option_code": "posthoc"
        }
      },
      {
        "source": "L_A13_WELCH",
        "target": "A14",
        "label": "Need post-hoc comparisons",
        "metadata": {
          "option_code": "posthoc"
        }
      },
      {
        "source": "L_A11B_NONPARAM",
        "target": "A14",
        "label": "Need post-hoc comparisons",
        "metadata": {
          "option_code": "posthoc"
        }
      },
      {
        "source": "A14",
        "target": "L_A14_PAIRWISE_EQ",
        "label": "All pairwise (if equal-variance ANOVA path)",
        "metadata": {
          "option_code": "pairwise_eq"
        }
      },
      {
        "source": "A14",
        "target": "L_A14_PAIRWISE_NEQ",
        "label": "All pairwise (if Welch/unequal variances)",
        "metadata": {
          "option_code": "pairwise_neq"
        }
      },
      {
        "source": "A14",
        "target": "L_A14_VS_CONTROL",
        "label": "Each vs control",
        "metadata": {
          "option_code": "vs_control"
        }
      },
      {
        "source": "A14",
        "target": "L_A14_CONTRASTS",
        "label": "Planned contrasts",
        "metadata": {
          "option_code": "contrasts"
        }
      },
      {
        "source": "A11W",
        "target": "L_A11W_RM",
        "label": "Yes: assumptions OK (or corrected)",
        "metadata": {
          "option_code": "parametric"
        }
      },
      {
        "source": "A11W",
        "target": "L_A11W_NONPARAM",
        "label": "No: use nonparametric/mixed",
        "metadata": {
          "option_code": "nonparametric"
        }
      },
      {
        "source": "A16",
        "target": "L_A16_1S",
        "label": "1-sample / one condition vs reference",
        "metadata": {
          "option_code": "1"
        }
      },
      {
        "source": "A16",
        "target": "L_A16_2IND",
        "label": "2 independent groups",
        "metadata": {
          "option_code": "2ind"
        }
      },
      {
        "source": "A16",
        "target": "L_A16_2PAIR",
        "label": "2 paired conditions",
        "metadata": {
          "option_code": "2pair"
        }
      },
      {
        "source": "A16",
        "target": "L_A16_KIND",
        "label": "k independent groups (k≥3)",
        "metadata": {
          "option_code": "kind"
        }
      },
      {
        "source": "A16",
        "target": "L_A16_KPAIR",
        "label": "k paired conditions (k≥3)",
        "metadata": {
          "option_code": "kpair"
        }
      },
      {
        "source": "A17",
        "target": "L_A17_1S_EXACT",
        "label": "1 group proportion vs value",
        "metadata": {
          "option_code": "1"
        }
      },
      {
        "source": "A17",
        "target": "L_A17_PAIRED",
        "label": "2 groups paired/matched",
        "metadata": {
          "option_code": "paired"
        }
      },
      {
        "source": "A17",
        "target": "A18",
        "label": "2 groups independent (2x2)",
        "metadata": {
          "option_code": "2x2"
        }
      },
      {
        "source": "A17",
        "target": "L_A17_KIND",
        "label": "k groups independent",
        "metadata": {
          "option_code": "k_ind"
        }
      },
      {
        "source": "A17",
        "target": "L_A17_KPAIR",
        "label": "k conditions paired",
        "metadata": {
          "option_code": "k_pair"
        }
      },
      {
        "source": "A18",
        "target": "L_A18_CHI",
        "label": "Expected counts large enough",
        "metadata": {
          "option_code": "large"
        }
      },
      {
        "source": "A18",
        "target": "L_A18_FISHER",
        "label": "Small expected counts",
        "metadata": {
          "option_code": "small"
        }
      },
      {
        "source": "A19",
        "target": "L_A19_GOF",
        "label": "Goodness-of-fit (1 variable)",
        "metadata": {
          "option_code": "gof"
        }
      },
      {
        "source": "A19",
        "target": "L_A19_IND",
        "label": "Association between two categorical vars",
        "metadata": {
          "option_code": "assoc"
        }
      },
      {
        "source": "A19",
        "target": "L_A19_PAIREDK",
        "label": "Paired/matched nominal with >2 categories",
        "metadata": {
          "option_code": "pairedk"
        }
      },
      {
        "source": "A20",
        "target": "L_A20_RATE",
        "label": "Yes: rates with exposure/offset",
        "metadata": {
          "option_code": "rate"
        }
      },
      {
        "source": "A20",
        "target": "A21",
        "label": "No (counts) or after choosing Poisson GLM: check dispersion",
        "metadata": {
          "option_code": "counts"
        }
      },
      {
        "source": "A21",
        "target": "L_A20_RATE",
        "label": "No: Poisson adequate",
        "metadata": {
          "option_code": "poisson_ok"
        }
      },
      {
        "source": "A21",
        "target": "L_A21_NB",
        "label": "Yes: overdispersed/zero-inflated",
        "metadata": {
          "option_code": "overdispersed"
        }
      },
      {
        "source": "A22",
        "target": "L_A22_LOGRANK",
        "label": "Yes: compare groups' survival curves",
        "metadata": {
          "option_code": "compare"
        }
      },
      {
        "source": "A22",
        "target": "A23",
        "label": "Need covariate-adjusted survival model",
        "metadata": {
          "option_code": "adjust"
        }
      },
      {
        "source": "A23",
        "target": "L_A23_COX",
        "label": "Semi-parametric Cox PH",
        "metadata": {
          "option_code": "cox"
        }
      },
      {
        "source": "A23",
        "target": "L_A23_AFT",
        "label": "Parametric survival regression",
        "metadata": {
          "option_code": "param"
        }
      },
      {
        "source": "A24",
        "target": "L_A24_HOT1",
        "label": "1 sample vs known vector mean",
        "metadata": {
          "option_code": "hot1"
        }
      },
      {
        "source": "A24",
        "target": "L_A24_HOT2",
        "label": "2 groups multivariate mean diff",
        "metadata": {
          "option_code": "hot2"
        }
      },
      {
        "source": "A24",
        "target": "L_A24_MANOVA",
        "label": "Multiple DVs with group factors",
        "metadata": {
          "option_code": "manova"
        }
      },
      {
        "source": "A24",
        "target": "L_A24_PERMANOVA",
        "label": "Distance-based nonparametric multivariate",
        "metadata": {
          "option_code": "permanova"
        }
      },
      {
        "source": "B1",
        "target": "B2",
        "label": "Continuous vs continuous",
        "metadata": {
          "option_code": "cont_cont"
        }
      },
      {
        "source": "B1",
        "target": "B4",
        "label": "Ordinal/rank involved",
        "metadata": {
          "option_code": "ordinal"
        }
      },
      {
        "source": "B1",
        "target": "B5",
        "label": "Binary vs continuous",
        "metadata": {
          "option_code": "bin_cont"
        }
      },
      {
        "source": "B1",
        "target": "B6",
        "label": "Nominal vs nominal",
        "metadata": {
          "option_code": "nom_nom"
        }
      },
      {
        "source": "B1",
        "target": "B7",
        "label": "Binary proportions across ordered groups",
        "metadata": {
          "option_code": "trend"
        }
      },
      {
        "source": "B2",
        "target": "L_B2_PEARSON",
        "label": "Yes: linear/normal",
        "metadata": {
          "option_code": "linear"
        }
      },
      {
        "source": "B2",
        "target": "B3",
        "label": "No: non-linear or non-normal",
        "metadata": {
          "option_code": "nonlinear"
        }
      },
      {
        "source": "B3",
        "target": "L_B3_RANK",
        "label": "Yes: monotonic",
        "metadata": {
          "option_code": "monotonic"
        }
      },
      {
        "source": "B3",
        "target": "L_B3_NONLIN",
        "label": "No/unknown: general dependence",
        "metadata": {
          "option_code": "general"
        }
      },
      {
        "source": "B4",
        "target": "L_B4_ORD",
        "label": "Use rank-based association",
        "metadata": {
          "option_code": "rank"
        }
      },
      {
        "source": "B5",
        "target": "L_B5_PBIS",
        "label": "Yes/No (still valid): use point-biserial or equivalent t-test",
        "metadata": {
          "option_code": "pbis"
        }
      },
      {
        "source": "B6",
        "target": "L_B6_CHI",
        "label": "Independent",
        "metadata": {
          "option_code": "independent"
        }
      },
      {
        "source": "B6",
        "target": "L_B6_PAIRED",
        "label": "Paired/matched",
        "metadata": {
          "option_code": "paired"
        }
      },
      {
        "source": "B7",
        "target": "L_B7_TREND",
        "label": "Yes: trend test",
        "metadata": {
          "option_code": "trend"
        }
      },
      {
        "source": "C1",
        "target": "C2",
        "label": "Continuous response",
        "metadata": {
          "option_code": "continuous"
        }
      },
      {
        "source": "C1",
        "target": "C3",
        "label": "Binary response",
        "metadata": {
          "option_code": "binary"
        }
      },
      {
        "source": "C1",
        "target": "C4",
        "label": "Count response",
        "metadata": {
          "option_code": "count"
        }
      },
      {
        "source": "C1",
        "target": "C5",
        "label": "Ordinal response",
        "metadata": {
          "option_code": "ordinal"
        }
      },
      {
        "source": "C1",
        "target": "C6",
        "label": "Nominal multiclass response",
        "metadata": {
          "option_code": "multiclass"
        }
      },
      {
        "source": "C1",
        "target": "C7",
        "label": "Time-to-event response",
        "metadata": {
          "option_code": "survival"
        }
      },
      {
        "source": "C1",
        "target": "C8",
        "label": "Clustered/longitudinal structure (any response)",
        "metadata": {
          "option_code": "clustered"
        }
      },
      {
        "source": "C2",
        "target": "L_C2_LM",
        "label": "Yes: linear model OK",
        "metadata": {
          "option_code": "ok"
        }
      },
      {
        "source": "C2",
        "target": "L_C2_ALT",
        "label": "No: robust/nonlinear alternatives",
        "metadata": {
          "option_code": "alt"
        }
      },
      {
        "source": "C3",
        "target": "L_C3_LOGIT",
        "label": "Logistic/probit",
        "metadata": {
          "option_code": "logit"
        }
      },
      {
        "source": "C4",
        "target": "L_C4_POIS",
        "label": "Poisson adequate",
        "metadata": {
          "option_code": "poisson"
        }
      },
      {
        "source": "C4",
        "target": "L_C4_NB",
        "label": "Overdispersed/zero-inflated",
        "metadata": {
          "option_code": "nb"
        }
      },
      {
        "source": "C5",
        "target": "L_C5_ORD",
        "label": "Ordinal regression",
        "metadata": {
          "option_code": "ord"
        }
      },
      {
        "source": "C6",
        "target": "L_C6_MULTI",
        "label": "Multinomial regression",
        "metadata": {
          "option_code": "multi"
        }
      },
      {
        "source": "C7",
        "target": "L_C7_SURV",
        "label": "Survival regression",
        "metadata": {
          "option_code": "surv"
        }
      },
      {
        "source": "C8",
        "target": "L_C8_MIXED",
        "label": "Subject-specific (random effects)",
        "metadata": {
          "option_code": "mixed"
        }
      },
      {
        "source": "C8",
        "target": "L_C8_GEE",
        "label": "Population-average (GEE)",
        "metadata": {
          "option_code": "gee"
        }
      },
      {
        "source": "D1",
        "target": "D2",
        "label": "Continuous measures",
        "metadata": {
          "option_code": "continuous"
        }
      },
      {
        "source": "D1",
        "target": "D3",
        "label": "Categorical labels",
        "metadata": {
          "option_code": "categorical"
        }
      },
      {
        "source": "D1",
        "target": "D4",
        "label": "Classifier performance comparison",
        "metadata": {
          "option_code": "classifier"
        }
      },
      {
        "source": "D2",
        "target": "L_D2_BA",
        "label": "Two methods: agreement/LOA",
        "metadata": {
          "option_code": "ba"
        }
      },
      {
        "source": "D2",
        "target": "L_D2_ICC",
        "label": "Rater reliability (continuous)",
        "metadata": {
          "option_code": "icc"
        }
      },
      {
        "source": "D3",
        "target": "L_D3_KAPPA2",
        "label": "Two raters",
        "metadata": {
          "option_code": "two"
        }
      },
      {
        "source": "D3",
        "target": "L_D3_KAPPAM",
        "label": "Many raters",
        "metadata": {
          "option_code": "many"
        }
      },
      {
        "source": "D4",
        "target": "L_D4_MCNEMAR",
        "label": "Paired accuracy/dichotomous disagreement",
        "metadata": {
          "option_code": "mcnemar"
        }
      },
      {
        "source": "D4",
        "target": "L_D4_DELONG",
        "label": "Compare ROC AUCs",
        "metadata": {
          "option_code": "delong"
        }
      },
      {
        "source": "E1",
        "target": "E2",
        "label": "Normality",
        "metadata": {
          "option_code": "normality"
        }
      },
      {
        "source": "E1",
        "target": "E3",
        "label": "Equal variances",
        "metadata": {
          "option_code": "variance"
        }
      },
      {
        "source": "E1",
        "target": "E4",
        "label": "Independence/autocorrelation",
        "metadata": {
          "option_code": "independence"
        }
      },
      {
        "source": "E1",
        "target": "E5",
        "label": "Goodness-of-fit / nested models",
        "metadata": {
          "option_code": "gof"
        }
      },
      {
        "source": "E1",
        "target": "E6",
        "label": "Outliers/influence",
        "metadata": {
          "option_code": "outliers"
        }
      },
      {
        "source": "E2",
        "target": "L_E2_NORM",
        "label": "Choose a normality test",
        "metadata": {
          "option_code": "tests"
        }
      },
      {
        "source": "E3",
        "target": "L_E3_VAR",
        "label": "Choose a variance test",
        "metadata": {
          "option_code": "tests"
        }
      },
      {
        "source": "E4",
        "target": "L_E4_RUNS",
        "label": "General randomness",
        "metadata": {
          "option_code": "runs"
        }
      },
      {
        "source": "E4",
        "target": "L_E4_DW",
        "label": "Regression residual autocorrelation",
        "metadata": {
          "option_code": "dw_bg"
        }
      },
      {
        "source": "E4",
        "target": "L_E4_LJUNG",
        "label": "Time series residual autocorrelation overall",
        "metadata": {
          "option_code": "ljung"
        }
      },
      {
        "source": "E5",
        "target": "L_E5_GOF",
        "label": "Choose GOF/nested test",
        "metadata": {
          "option_code": "gof"
        }
      },
      {
        "source": "E6",
        "target": "L_E6_OUT",
        "label": "Choose outlier/influence diagnostic",
        "metadata": {
          "option_code": "out"
        }
      },
      {
        "source": "F1",
        "target": "F2",
        "label": "Stationarity/unit root",
        "metadata": {
          "option_code": "stationarity"
        }
      },
      {
        "source": "F1",
        "target": "F3",
        "label": "Residual whiteness after ARMA/ARIMA",
        "metadata": {
          "option_code": "whiteness"
        }
      },
      {
        "source": "F1",
        "target": "F4",
        "label": "Predictive causality between series",
        "metadata": {
          "option_code": "granger"
        }
      },
      {
        "source": "F1",
        "target": "F5",
        "label": "Structural breaks",
        "metadata": {
          "option_code": "breaks"
        }
      },
      {
        "source": "F1",
        "target": "F6",
        "label": "Compare forecast accuracy",
        "metadata": {
          "option_code": "forecast"
        }
      },
      {
        "source": "F2",
        "target": "L_F2_UNITROOT",
        "label": "ADF/KPSS",
        "metadata": {
          "option_code": "adf_kpss"
        }
      },
      {
        "source": "F3",
        "target": "L_F3_LJUNG",
        "label": "Box–Ljung",
        "metadata": {
          "option_code": "ljung"
        }
      },
      {
        "source": "F4",
        "target": "L_F4_GRANGER",
        "label": "Granger causality",
        "metadata": {
          "option_code": "granger"
        }
      },
      {
        "source": "F5",
        "target": "L_F5_BREAK",
        "label": "Break tests",
        "metadata": {
          "option_code": "break"
        }
      },
      {
        "source": "F6",
        "target": "L_F6_DM",
        "label": "Diebold–Mariano",
        "metadata": {
          "option_code": "dm"
        }
      },
      {
        "source": "G1",
        "target": "L_G1_CONT",
        "label": "Continuous effect sizes",
        "metadata": {
          "option_code": "cont"
        }
      },
      {
        "source": "G1",
        "target": "L_G1_BIN",
        "label": "Binary effect sizes",
        "metadata": {
          "option_code": "bin"
        }
      },
      {
        "source": "G1",
        "target": "L_G1_HET",
        "label": "Heterogeneity",
        "metadata": {
          "option_code": "het"
        }
      },
      {
        "source": "G1",
        "target": "L_G1_BIAS",
        "label": "Publication bias",
        "metadata": {
          "option_code": "bias"
        }
      },
      {
        "source": "G1",
        "target": "L_G1_MULTTEST",
        "label": "Multiple testing control",
        "metadata": {
          "option_code": "mult"
        }
      },
      {
        "source": "H1",
        "target": "L_H1_CLASSIC",
        "label": "Yes: likelihood-based tests",
        "metadata": {
          "option_code": "likelihood_yes"
        }
      },
      {
        "source": "H1",
        "target": "H2",
        "label": "No/unclear: consider resampling or Bayesian",
        "metadata": {
          "option_code": "likelihood_no"
        }
      },
      {
        "source": "H2",
        "target": "L_H2_PERM",
        "label": "Yes: permutation/bootstrap appropriate",
        "metadata": {
          "option_code": "perm"
        }
      },
      {
        "source": "H2",
        "target": "H3",
        "label": "No: consider Bayesian/simulation",
        "metadata": {
          "option_code": "no_perm"
        }
      },
      {
        "source": "H3",
        "target": "L_H3_BF",
        "label": "Yes: Bayes factors/model comp",
        "metadata": {
          "option_code": "bf"
        }
      },
      {
        "source": "H3",
        "target": "L_H3_MCMC",
        "label": "No/also: Bayesian estimation & checks",
        "metadata": {
          "option_code": "mcmc"
        }
      }
    ]
  };

// ===== Elaborated explanations injected from stat_tests_flowchart_elaborated_examples.docx =====
const EXPLANATIONS = {
  "Q0": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'What is the primary aim of your research question?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine hourly ICU heart-rate data streamed from bedside monitors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Differences between groups/conditions, Association/correlation, Prediction/modeling lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "A": "General explanation: Selecting 'Differences between groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: A) Compare groups/conditions (differences) — what is the outcome (dependent variable) type?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Differences between groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "A) Differences between groups/conditions": "General explanation: Selecting 'Differences between groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: A) Compare groups/conditions (differences) — what is the outcome (dependent variable) type?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Differences between groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "A: A) Differences between groups/conditions": "General explanation: Selecting 'Differences between groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: A) Compare groups/conditions (differences) — what is the outcome (dependent variable) type?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Differences between groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "B": "General explanation: Selecting 'Association/correlation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: B) Association between variables — what are the variable types?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association/correlation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "B) Association/correlation": "General explanation: Selecting 'Association/correlation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: B) Association between variables — what are the variable types?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association/correlation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "B: B) Association/correlation": "General explanation: Selecting 'Association/correlation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: B) Association between variables — what are the variable types?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association/correlation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "C": "General explanation: Selecting 'Prediction/modeling' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: C) Prediction/Modeling — what is the response variable type?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Prediction/modeling' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "C) Prediction/modeling": "General explanation: Selecting 'Prediction/modeling' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: C) Prediction/Modeling — what is the response variable type?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Prediction/modeling' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "C: C) Prediction/modeling": "General explanation: Selecting 'Prediction/modeling' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: C) Prediction/Modeling — what is the response variable type?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Prediction/modeling' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "D": "General explanation: Selecting 'Agreement/reliability' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: D) Agreement / reliability — what is the measurement scale?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Agreement/reliability' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "D) Agreement/reliability": "General explanation: Selecting 'Agreement/reliability' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: D) Agreement / reliability — what is the measurement scale?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Agreement/reliability' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "D: D) Agreement/reliability": "General explanation: Selecting 'Agreement/reliability' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: D) Agreement / reliability — what is the measurement scale?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Agreement/reliability' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "E": "General explanation: Selecting 'Diagnostics/assumptions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: E) Diagnostics / assumptions — what do you need to check?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diagnostics/assumptions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "E) Diagnostics/assumptions": "General explanation: Selecting 'Diagnostics/assumptions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: E) Diagnostics / assumptions — what do you need to check?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diagnostics/assumptions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "E: E) Diagnostics/assumptions": "General explanation: Selecting 'Diagnostics/assumptions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: E) Diagnostics / assumptions — what do you need to check?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diagnostics/assumptions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "F": "General explanation: Selecting 'Time series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: F) Time series — what is the primary goal?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "F) Time series": "General explanation: Selecting 'Time series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: F) Time series — what is the primary goal?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "F: F) Time series": "General explanation: Selecting 'Time series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: F) Time series — what is the primary goal?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "G": "General explanation: Selecting 'Meta-analysis/evidence synthesis' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: G) Combine results across studies (meta-analysis) — what is the outcome metric?.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Meta-analysis/evidence synthesis' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "G) Meta-analysis/evidence synthesis": "General explanation: Selecting 'Meta-analysis/evidence synthesis' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: G) Combine results across studies (meta-analysis) — what is the outcome metric?.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Meta-analysis/evidence synthesis' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "G: G) Meta-analysis/evidence synthesis": "General explanation: Selecting 'Meta-analysis/evidence synthesis' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: G) Combine results across studies (meta-analysis) — what is the outcome metric?.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Meta-analysis/evidence synthesis' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "H": "General explanation: Selecting 'Advanced/custom inference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: H) Custom/advanced inference — do you have an explicit likelihood model?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Advanced/custom inference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "H) Advanced/custom inference": "General explanation: Selecting 'Advanced/custom inference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: H) Custom/advanced inference — do you have an explicit likelihood model?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Advanced/custom inference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "H: H) Advanced/custom inference": "General explanation: Selecting 'Advanced/custom inference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: H) Custom/advanced inference — do you have an explicit likelihood model?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Advanced/custom inference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'A) Compare groups/conditions (differences) — what is the outcome (dependent variable) type?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time to first device-related malfunction after implantation. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Continuous outcome, Ordinal outcome, Binary outcome lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "continuous": "General explanation: Selecting 'Continuous outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is continuous (interval/ratio): how many groups/conditions are you comparing?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Continuous outcome": "General explanation: Selecting 'Continuous outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is continuous (interval/ratio): how many groups/conditions are you comparing?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "continuous: Continuous outcome": "General explanation: Selecting 'Continuous outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is continuous (interval/ratio): how many groups/conditions are you comparing?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal": "General explanation: Selecting 'Ordinal outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is ordinal (Likert/ranks): how many groups/conditions and is pairing present?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Ordinal outcome": "General explanation: Selecting 'Ordinal outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is ordinal (Likert/ranks): how many groups/conditions and is pairing present?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal: Ordinal outcome": "General explanation: Selecting 'Ordinal outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is ordinal (Likert/ranks): how many groups/conditions and is pairing present?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "binary": "General explanation: Selecting 'Binary outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is binary (0/1): what is the design?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Binary outcome": "General explanation: Selecting 'Binary outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is binary (0/1): what is the design?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "binary: Binary outcome": "General explanation: Selecting 'Binary outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is binary (0/1): what is the design?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nominal": "General explanation: Selecting 'Nominal categorical outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is nominal categorical (≥2 unordered categories): what is the question?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal categorical outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Nominal categorical outcome": "General explanation: Selecting 'Nominal categorical outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. Choosing this option signals an unordered categorical endpoint (≥2 categories without a natural order). Downstream, you must specify the comparison structure (for example: independent groups vs paired or clustered observations, the number of groups/arms, and the estimand of interest such as a risk difference, risk ratio, or odds ratio), because those choices determine which modeling families and exact/approximate procedures are appropriate and how uncertainty should be quantified.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal categorical outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nominal: Nominal categorical outcome": "General explanation: Selecting 'Nominal categorical outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is nominal categorical (≥2 unordered categories): what is the question?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal categorical outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "count": "General explanation: Selecting 'Count/rate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is counts/rates: do you have an exposure/offset (time at risk, area, etc.)?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count/rate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Count/rate outcome": "General explanation: Selecting 'Count/rate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is counts/rates: do you have an exposure/offset (time at risk, area, etc.)?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count/rate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "count: Count/rate outcome": "General explanation: Selecting 'Count/rate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is counts/rates: do you have an exposure/offset (time at risk, area, etc.)?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count/rate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "survival": "General explanation: Selecting 'Time-to-event outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is time-to-event: comparing survival curves between groups?.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Time-to-event outcome": "General explanation: Selecting 'Time-to-event outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is time-to-event: comparing survival curves between groups?.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "survival: Time-to-event outcome": "General explanation: Selecting 'Time-to-event outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outcome is time-to-event: comparing survival curves between groups?.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "multivariate": "General explanation: Selecting 'Multivariate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Multivariate continuous outcome (vectors) or many correlated outcomes?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multivariate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Multivariate outcome": "General explanation: Selecting 'Multivariate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Multivariate continuous outcome (vectors) or many correlated outcomes?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multivariate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "multivariate: Multivariate outcome": "General explanation: Selecting 'Multivariate outcome' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Multivariate continuous outcome (vectors) or many correlated outcomes?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multivariate outcome' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is continuous (interval/ratio): how many groups/conditions are you comparing?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine blood glucose (mg/dL) measured by a CGM and by lab analyzer. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like 1 group, 2 groups, 3+ groups/conditions lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "1": "General explanation: Selecting '1 group' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: One group: are you testing a population mean against a known value?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1 group": "General explanation: Selecting '1 group' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: One group: are you testing a population mean against a known value?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1: 1 group": "General explanation: Selecting '1 group' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: One group: are you testing a population mean against a known value?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2": "General explanation: Selecting '2 groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two groups: are observations paired/repeated (same units measured twice or matched pairs)?.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 groups": "General explanation: Selecting '2 groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two groups: are observations paired/repeated (same units measured twice or matched pairs)?.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2: 2 groups": "General explanation: Selecting '2 groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two groups: are observations paired/repeated (same units measured twice or matched pairs)?.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "3plus": "General explanation: Selecting '3+ groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: ≥3 groups/conditions: what is the design structure?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '3+ groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "3+ groups/conditions": "General explanation: Selecting '3+ groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: ≥3 groups/conditions: what is the design structure?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '3+ groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "3plus: 3+ groups/conditions": "General explanation: Selecting '3+ groups/conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: ≥3 groups/conditions: what is the design structure?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '3+ groups/conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'One group: are you testing a population mean against a known value?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine oxygen saturation (%) difference between sensors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: mean vs known value, No: not (only) mean lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "yes": "General explanation: Selecting 'Yes: mean vs known value' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use a 1-sample mean test. Typical analyses here include: One-sample t-test (σ unknown); One-sample z-test (σ known). Practical note: Prefer t-test unless population σ is known and normality is plausible; for large n, t is robust.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: mean vs known value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: mean vs known value": "General explanation: Selecting 'Yes: mean vs known value' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use a 1-sample mean test. Typical analyses here include: One-sample t-test (σ unknown); One-sample z-test (σ known). Practical note: Prefer t-test unless population σ is known and normality is plausible; for large n, t is robust.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: mean vs known value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "yes: Yes: mean vs known value": "General explanation: Selecting 'Yes: mean vs known value' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use a 1-sample mean test. Typical analyses here include: One-sample t-test (σ unknown); One-sample z-test (σ known). Practical note: Prefer t-test unless population σ is known and normality is plausible; for large n, t is robust.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: mean vs known value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no": "General explanation: Selecting 'No: not (only) mean' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If not a mean: are you testing median/paired-symmetry/typical value shift?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: not (only) mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: not (only) mean": "General explanation: Selecting 'No: not (only) mean' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If not a mean: are you testing median/paired-symmetry/typical value shift?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: not (only) mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no: No: not (only) mean": "General explanation: Selecting 'No: not (only) mean' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If not a mean: are you testing median/paired-symmetry/typical value shift?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: not (only) mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'If not a mean: are you testing median/paired-symmetry/typical value shift?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine oxygen saturation (%) difference between sensors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: median/typical shift, No: general shift/difference lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "yes": "General explanation: Selecting 'Yes: median/typical shift' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric 1-sample location test. Typical analyses here include: Wilcoxon signed-rank (requires symmetric differences); Sign test. Practical note: Signed-rank has more power under symmetry; sign test is minimal-assumption.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: median/typical shift' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: median/typical shift": "General explanation: Selecting 'Yes: median/typical shift' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric 1-sample location test. Typical analyses here include: Wilcoxon signed-rank (requires symmetric differences); Sign test. Practical note: Signed-rank has more power under symmetry; sign test is minimal-assumption.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: median/typical shift' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "yes: Yes: median/typical shift": "General explanation: Selecting 'Yes: median/typical shift' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric 1-sample location test. Typical analyses here include: Wilcoxon signed-rank (requires symmetric differences); Sign test. Practical note: Signed-rank has more power under symmetry; sign test is minimal-assumption.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: median/typical shift' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no": "General explanation: Selecting 'No: general shift/difference' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Distribution-free alternative. Typical analyses here include: One-sample permutation/randomization test. Practical note: Use if you can justify exchangeability under the null.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: general shift/difference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: general shift/difference": "General explanation: Selecting 'No: general shift/difference' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Distribution-free alternative. Typical analyses here include: One-sample permutation/randomization test. Practical note: Use if you can justify exchangeability under the null.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: general shift/difference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no: No: general shift/difference": "General explanation: Selecting 'No: general shift/difference' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Distribution-free alternative. Typical analyses here include: One-sample permutation/randomization test. Practical note: Use if you can justify exchangeability under the null.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: general shift/difference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A5": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Two groups: are observations paired/repeated (same units measured twice or matched pairs)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a multicenter study measuring HbA1c change after a digital diabetes coaching program. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like No: independent groups, Yes: paired/matched/repeated lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "independent": "General explanation: Selecting 'No: independent groups' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Independent groups: is the outcome approximately normal within each group (or n large enough)?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: independent groups": "General explanation: Selecting 'No: independent groups' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Independent groups: is the outcome approximately normal within each group (or n large enough)?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "independent: No: independent groups": "General explanation: Selecting 'No: independent groups' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Independent groups: is the outcome approximately normal within each group (or n large enough)?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired": "General explanation: Selecting 'Yes: paired/matched/repeated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Paired groups: are the within-pair differences approximately normal (or n large enough)?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: paired/matched/repeated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: paired/matched/repeated": "General explanation: Selecting 'Yes: paired/matched/repeated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Paired groups: are the within-pair differences approximately normal (or n large enough)?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: paired/matched/repeated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired: Yes: paired/matched/repeated": "General explanation: Selecting 'Yes: paired/matched/repeated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Paired groups: are the within-pair differences approximately normal (or n large enough)?.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: paired/matched/repeated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A7": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Independent groups: is the outcome approximately normal within each group (or n large enough)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a 5-point nurse usability Likert score for a new bedside monitor UI. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: approx normal/large n, No: non-normal/ordinal-like lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "normal": "General explanation: Selecting 'Yes: approx normal/large n' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Independent groups: are variances approximately equal (homoscedasticity)?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal/large n' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: approx normal/large n": "General explanation: Selecting 'Yes: approx normal/large n' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Independent groups: are variances approximately equal (homoscedasticity)?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal/large n' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "normal: Yes: approx normal/large n": "General explanation: Selecting 'Yes: approx normal/large n' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Independent groups: are variances approximately equal (homoscedasticity)?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal/large n' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal": "General explanation: Selecting 'No: non-normal/ordinal-like' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric 2-group location test. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel (if stochastic dominance questionable); Permutation test for difference in means/medians. Practical note: Rank-sum targets stochastic ordering; Brunner–Munzel relaxes equal-shape assumption.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/ordinal-like' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: non-normal/ordinal-like": "General explanation: Selecting 'No: non-normal/ordinal-like' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric 2-group location test. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel (if stochastic dominance questionable); Permutation test for difference in means/medians. Practical note: Rank-sum targets stochastic ordering; Brunner–Munzel relaxes equal-shape assumption.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/ordinal-like' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal: No: non-normal/ordinal-like": "General explanation: Selecting 'No: non-normal/ordinal-like' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric 2-group location test. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel (if stochastic dominance questionable); Permutation test for difference in means/medians. Practical note: Rank-sum targets stochastic ordering; Brunner–Munzel relaxes equal-shape assumption.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/ordinal-like' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A8": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Independent groups: are variances approximately equal (homoscedasticity)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine device battery life (hours) under standardized discharge cycles. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: equal variances, No/unsure: unequal variances lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "equalvar": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric 2-group mean test (equal variances). Typical analyses here include: Student two-sample t-test (pooled variance).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: equal variances": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric 2-group mean test (equal variances). Typical analyses here include: Student two-sample t-test (pooled variance).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "equalvar: Yes: equal variances": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric 2-group mean test (equal variances). Typical analyses here include: Student two-sample t-test (pooled variance).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "unequalvar": "General explanation: Selecting 'No/unsure: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric 2-group mean test (unequal variances). Typical analyses here include: Welch two-sample t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unsure: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No/unsure: unequal variances": "General explanation: Selecting 'No/unsure: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric 2-group mean test (unequal variances). Typical analyses here include: Welch two-sample t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unsure: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "unequalvar: No/unsure: unequal variances": "General explanation: Selecting 'No/unsure: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric 2-group mean test (unequal variances). Typical analyses here include: Welch two-sample t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unsure: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A9": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Paired groups: are the within-pair differences approximately normal (or n large enough)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine device battery life (hours) under standardized discharge cycles. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: differences approx normal, No: non-normal differences lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "normal": "General explanation: Selecting 'Yes: differences approx normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric paired mean test. Typical analyses here include: Paired t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: differences approx normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: differences approx normal": "General explanation: Selecting 'Yes: differences approx normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric paired mean test. Typical analyses here include: Paired t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: differences approx normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "normal: Yes: differences approx normal": "General explanation: Selecting 'Yes: differences approx normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric paired mean test. Typical analyses here include: Paired t-test.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: differences approx normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal": "General explanation: Selecting 'No: non-normal differences' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric paired test. Typical analyses here include: Wilcoxon signed-rank; Sign test; Paired permutation/randomization test.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal differences' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: non-normal differences": "General explanation: Selecting 'No: non-normal differences' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric paired test. Typical analyses here include: Wilcoxon signed-rank; Sign test; Paired permutation/randomization test.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal differences' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal: No: non-normal differences": "General explanation: Selecting 'No: non-normal differences' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric paired test. Typical analyses here include: Wilcoxon signed-rank; Sign test; Paired permutation/randomization test.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal differences' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A10": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: '≥3 groups/conditions: what is the design structure?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a prospective study validating a wearable ECG patch against a 12-lead ECG reference. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Between-subjects groups, Within-subjects repeated measures, Mixed/hierarchical/clustered lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "between": "General explanation: Selecting 'Between-subjects groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Between-subjects (independent groups): are residuals approximately normal?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Between-subjects groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Between-subjects groups": "General explanation: Selecting 'Between-subjects groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Between-subjects (independent groups): are residuals approximately normal?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Between-subjects groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "between: Between-subjects groups": "General explanation: Selecting 'Between-subjects groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Between-subjects (independent groups): are residuals approximately normal?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Between-subjects groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "within": "General explanation: Selecting 'Within-subjects repeated measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Within-subjects (repeated measures): do parametric assumptions (normality, sphericity) hold?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Within-subjects repeated measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Within-subjects repeated measures": "General explanation: Selecting 'Within-subjects repeated measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Within-subjects (repeated measures): do parametric assumptions (normality, sphericity) hold?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Within-subjects repeated measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "within: Within-subjects repeated measures": "General explanation: Selecting 'Within-subjects repeated measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Within-subjects (repeated measures): do parametric assumptions (normality, sphericity) hold?.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Within-subjects repeated measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mixed": "General explanation: Selecting 'Mixed/hierarchical/clustered' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed designs / hierarchical data. Typical analyses here include: Linear mixed-effects model (LMM); Generalized linear mixed model (GLMM). Practical note: Test fixed effects via likelihood ratio / Wald / score tests; use appropriate covariance structure.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Mixed/hierarchical/clustered' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Mixed/hierarchical/clustered": "General explanation: Selecting 'Mixed/hierarchical/clustered' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed designs / hierarchical data. Typical analyses here include: Linear mixed-effects model (LMM); Generalized linear mixed model (GLMM). Practical note: Test fixed effects via likelihood ratio / Wald / score tests; use appropriate covariance structure.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Mixed/hierarchical/clustered' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mixed: Mixed/hierarchical/clustered": "General explanation: Selecting 'Mixed/hierarchical/clustered' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed designs / hierarchical data. Typical analyses here include: Linear mixed-effects model (LMM); Generalized linear mixed model (GLMM). Practical note: Test fixed effects via likelihood ratio / Wald / score tests; use appropriate covariance structure.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Mixed/hierarchical/clustered' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A11B": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Between-subjects (independent groups): are residuals approximately normal?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine systolic blood pressure (mmHg) measured by two devices. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: approx normal residuals, No: non-normal/heterogeneous lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "normal": "General explanation: Selecting 'Yes: approx normal residuals' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Between-subjects: are variances approximately equal across groups?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal residuals' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: approx normal residuals": "General explanation: Selecting 'Yes: approx normal residuals' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Between-subjects: are variances approximately equal across groups?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal residuals' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "normal: Yes: approx normal residuals": "General explanation: Selecting 'Yes: approx normal residuals' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to the next decision: Between-subjects: are variances approximately equal across groups?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: approx normal residuals' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal": "General explanation: Selecting 'No: non-normal/heterogeneous' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric omnibus test. Typical analyses here include: Kruskal–Wallis test; Permutation ANOVA (randomization); Robust ANOVA / trimmed-means ANOVA. Practical note: Kruskal–Wallis targets stochastic ordering; permutation ANOVA uses exchangeability.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/heterogeneous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: non-normal/heterogeneous": "General explanation: Selecting 'No: non-normal/heterogeneous' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric omnibus test. Typical analyses here include: Kruskal–Wallis test; Permutation ANOVA (randomization); Robust ANOVA / trimmed-means ANOVA. Practical note: Kruskal–Wallis targets stochastic ordering; permutation ANOVA uses exchangeability.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/heterogeneous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonnormal: No: non-normal/heterogeneous": "General explanation: Selecting 'No: non-normal/heterogeneous' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric omnibus test. Typical analyses here include: Kruskal–Wallis test; Permutation ANOVA (randomization); Robust ANOVA / trimmed-means ANOVA. Practical note: Kruskal–Wallis targets stochastic ordering; permutation ANOVA uses exchangeability.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-normal/heterogeneous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A11W": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Within-subjects (repeated measures): do parametric assumptions (normality, sphericity) hold?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine blood glucose (mg/dL) measured by a CGM and by lab analyzer. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: assumptions OK (or corrected), No: use nonparametric/mixed lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "parametric": "General explanation: Selecting 'Yes: assumptions OK (or corrected)' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric repeated-measures omnibus. Typical analyses here include: Repeated-measures ANOVA (with GG/HF correction if needed).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: assumptions OK (or corrected)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: assumptions OK (or corrected)": "General explanation: Selecting 'Yes: assumptions OK (or corrected)' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric repeated-measures omnibus. Typical analyses here include: Repeated-measures ANOVA (with GG/HF correction if needed).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: assumptions OK (or corrected)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "parametric: Yes: assumptions OK (or corrected)": "General explanation: Selecting 'Yes: assumptions OK (or corrected)' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric repeated-measures omnibus. Typical analyses here include: Repeated-measures ANOVA (with GG/HF correction if needed).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: assumptions OK (or corrected)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonparametric": "General explanation: Selecting 'No: use nonparametric/mixed' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric/robust repeated-measures. Typical analyses here include: Friedman test; Mixed-effects model (LMM). Practical note: Friedman is nonparametric for ranked repeated measures; LMM handles missingness/complex covariance.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: use nonparametric/mixed' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: use nonparametric/mixed": "General explanation: Selecting 'No: use nonparametric/mixed' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric/robust repeated-measures. Typical analyses here include: Friedman test; Mixed-effects model (LMM). Practical note: Friedman is nonparametric for ranked repeated measures; LMM handles missingness/complex covariance.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: use nonparametric/mixed' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonparametric: No: use nonparametric/mixed": "General explanation: Selecting 'No: use nonparametric/mixed' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Nonparametric/robust repeated-measures. Typical analyses here include: Friedman test; Mixed-effects model (LMM). Practical note: Friedman is nonparametric for ranked repeated measures; LMM handles missingness/complex covariance.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: use nonparametric/mixed' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A13": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Between-subjects: are variances approximately equal across groups?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine blood glucose (mg/dL) measured by a CGM and by lab analyzer. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: equal variances, No: unequal variances lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "equalvar": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric omnibus test. Typical analyses here include: One-way ANOVA.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: equal variances": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric omnibus test. Typical analyses here include: One-way ANOVA.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "equalvar: Yes: equal variances": "General explanation: Selecting 'Yes: equal variances' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Parametric omnibus test. Typical analyses here include: One-way ANOVA.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "unequalvar": "General explanation: Selecting 'No: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric omnibus test robust to unequal variances. Typical analyses here include: Welch ANOVA.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: unequal variances": "General explanation: Selecting 'No: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric omnibus test robust to unequal variances. Typical analyses here include: Welch ANOVA.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "unequalvar: No: unequal variances": "General explanation: Selecting 'No: unequal variances' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Parametric omnibus test robust to unequal variances. Typical analyses here include: Welch ANOVA.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: unequal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A14": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'If omnibus is significant and you need follow-up comparisons, what is your comparison plan?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time-to-result (minutes) for lab turnaround after workflow changes. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like All pairwise (if equal-variance ANOVA path), All pairwise (if Welch/unequal variances), Each vs control lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "pairwise_eq": "General explanation: Selecting 'All pairwise (if equal-variance ANOVA path)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (equal-variance parametric path). Typical analyses here include: Tukey HSD.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if equal-variance ANOVA path)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "All pairwise (if equal-variance ANOVA path)": "General explanation: Selecting 'All pairwise (if equal-variance ANOVA path)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (equal-variance parametric path). Typical analyses here include: Tukey HSD.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if equal-variance ANOVA path)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pairwise_eq: All pairwise (if equal-variance ANOVA path)": "General explanation: Selecting 'All pairwise (if equal-variance ANOVA path)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (equal-variance parametric path). Typical analyses here include: Tukey HSD.\n\nExample: Suppose you are working on device battery life (hours) under standardized discharge cycles. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if equal-variance ANOVA path)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pairwise_neq": "General explanation: Selecting 'All pairwise (if Welch/unequal variances)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (unequal-variance parametric path). Typical analyses here include: Games–Howell.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if Welch/unequal variances)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "All pairwise (if Welch/unequal variances)": "General explanation: Selecting 'All pairwise (if Welch/unequal variances)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (unequal-variance parametric path). Typical analyses here include: Games–Howell.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if Welch/unequal variances)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pairwise_neq: All pairwise (if Welch/unequal variances)": "General explanation: Selecting 'All pairwise (if Welch/unequal variances)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to All pairwise (unequal-variance parametric path). Typical analyses here include: Games–Howell.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'All pairwise (if Welch/unequal variances)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "vs_control": "General explanation: Selecting 'Each vs control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Each group vs control. Typical analyses here include: Dunnett test.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Each vs control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Each vs control": "General explanation: Selecting 'Each vs control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Each group vs control. Typical analyses here include: Dunnett test.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Each vs control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "vs_control: Each vs control": "General explanation: Selecting 'Each vs control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Each group vs control. Typical analyses here include: Dunnett test.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Each vs control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "contrasts": "General explanation: Selecting 'Planned contrasts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Planned contrasts / linear hypotheses. Typical analyses here include: t contrasts within ANOVA/GLM framework; Linear hypothesis tests with multiplicity control. Practical note: Use multiplicity control (e.g., Holm/BH) if many contrasts.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Planned contrasts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Planned contrasts": "General explanation: Selecting 'Planned contrasts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Planned contrasts / linear hypotheses. Typical analyses here include: t contrasts within ANOVA/GLM framework; Linear hypothesis tests with multiplicity control. Practical note: Use multiplicity control (e.g., Holm/BH) if many contrasts.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Planned contrasts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "contrasts: Planned contrasts": "General explanation: Selecting 'Planned contrasts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Planned contrasts / linear hypotheses. Typical analyses here include: t contrasts within ANOVA/GLM framework; Linear hypothesis tests with multiplicity control. Practical note: Use multiplicity control (e.g., Holm/BH) if many contrasts.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Planned contrasts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A16": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is ordinal (Likert/ranks): how many groups/conditions and is pairing present?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine pain score categories (none/mild/moderate/severe) after physiotherapy. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like 1-sample / one condition vs reference, 2 independent groups, 2 paired conditions lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "1": "General explanation: Selecting '1-sample / one condition vs reference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 1-sample ordinal location. Typical analyses here include: Sign test; Wilcoxon signed-rank.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1-sample / one condition vs reference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1-sample / one condition vs reference": "General explanation: Selecting '1-sample / one condition vs reference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 1-sample ordinal location. Typical analyses here include: Sign test; Wilcoxon signed-rank.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1-sample / one condition vs reference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1: 1-sample / one condition vs reference": "General explanation: Selecting '1-sample / one condition vs reference' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 1-sample ordinal location. Typical analyses here include: Sign test; Wilcoxon signed-rank.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1-sample / one condition vs reference' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2ind": "General explanation: Selecting '2 independent groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 independent ordinal groups. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 independent groups": "General explanation: Selecting '2 independent groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 independent ordinal groups. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2ind: 2 independent groups": "General explanation: Selecting '2 independent groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 independent ordinal groups. Typical analyses here include: Mann–Whitney U / Wilcoxon rank-sum; Brunner–Munzel.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 independent groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2pair": "General explanation: Selecting '2 paired conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 paired ordinal conditions. Typical analyses here include: Wilcoxon signed-rank; Sign test.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 paired conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 paired conditions": "General explanation: Selecting '2 paired conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 paired ordinal conditions. Typical analyses here include: Wilcoxon signed-rank; Sign test.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 paired conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2pair: 2 paired conditions": "General explanation: Selecting '2 paired conditions' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to 2 paired ordinal conditions. Typical analyses here include: Wilcoxon signed-rank; Sign test.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 paired conditions' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "kind": "General explanation: Selecting 'k independent groups (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent ordinal groups. Typical analyses here include: Kruskal–Wallis.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k independent groups (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k independent groups (k≥3)": "General explanation: Selecting 'k independent groups (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent ordinal groups. Typical analyses here include: Kruskal–Wallis.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k independent groups (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "kind: k independent groups (k≥3)": "General explanation: Selecting 'k independent groups (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent ordinal groups. Typical analyses here include: Kruskal–Wallis.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k independent groups (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "kpair": "General explanation: Selecting 'k paired conditions (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k paired ordinal conditions. Typical analyses here include: Friedman test.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k paired conditions (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k paired conditions (k≥3)": "General explanation: Selecting 'k paired conditions (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k paired ordinal conditions. Typical analyses here include: Friedman test.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k paired conditions (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "kpair: k paired conditions (k≥3)": "General explanation: Selecting 'k paired conditions (k≥3)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k paired ordinal conditions. Typical analyses here include: Friedman test.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k paired conditions (k≥3)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A17": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is binary (0/1): what is the design?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine presence/absence of atrial fibrillation detected by an algorithm. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like 1 group proportion vs value, 2 groups paired/matched, 2 groups independent (2x2) lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "1": "General explanation: Selecting '1 group proportion vs value' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample binary proportion. Typical analyses here include: Exact binomial test; One-sample proportion z-test (large n).\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group proportion vs value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1 group proportion vs value": "General explanation: Selecting '1 group proportion vs value' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample binary proportion. Typical analyses here include: Exact binomial test; One-sample proportion z-test (large n).\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group proportion vs value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1: 1 group proportion vs value": "General explanation: Selecting '1 group proportion vs value' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample binary proportion. Typical analyses here include: Exact binomial test; One-sample proportion z-test (large n).\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 group proportion vs value' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired": "General explanation: Selecting '2 groups paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired binary (before/after; matched). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 groups paired/matched": "General explanation: Selecting '2 groups paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired binary (before/after; matched). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired: 2 groups paired/matched": "General explanation: Selecting '2 groups paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired binary (before/after; matched). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2x2": "General explanation: Selecting '2 groups independent (2x2)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independent 2x2: are expected cell counts sufficiently large?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups independent (2x2)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 groups independent (2x2)": "General explanation: Selecting '2 groups independent (2x2)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independent 2x2: are expected cell counts sufficiently large?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups independent (2x2)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2x2: 2 groups independent (2x2)": "General explanation: Selecting '2 groups independent (2x2)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independent 2x2: are expected cell counts sufficiently large?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups independent (2x2)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k_ind": "General explanation: Selecting 'k groups independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent proportions (≥2 groups). Typical analyses here include: Chi-square test of homogeneity/independence.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k groups independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k groups independent": "General explanation: Selecting 'k groups independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent proportions (≥2 groups). Typical analyses here include: Chi-square test of homogeneity/independence.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k groups independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k_ind: k groups independent": "General explanation: Selecting 'k groups independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k independent proportions (≥2 groups). Typical analyses here include: Chi-square test of homogeneity/independence.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k groups independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k_pair": "General explanation: Selecting 'k conditions paired' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k related proportions (repeated measures). Typical analyses here include: Cochran’s Q test.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k conditions paired' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k conditions paired": "General explanation: Selecting 'k conditions paired' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k related proportions (repeated measures). Typical analyses here include: Cochran’s Q test.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k conditions paired' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "k_pair: k conditions paired": "General explanation: Selecting 'k conditions paired' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to k related proportions (repeated measures). Typical analyses here include: Cochran’s Q test.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'k conditions paired' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A18": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Independent 2x2: are expected cell counts sufficiently large?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine number of ventilator alarms per patient-day. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Expected counts large enough, Small expected counts lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "large": "General explanation: Selecting 'Expected counts large enough' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 large-sample association/difference in proportions. Typical analyses here include: Pearson chi-square test; Two-proportion z-test. Practical note: Equivalent in 2x2 under standard conditions.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Expected counts large enough' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Expected counts large enough": "General explanation: Selecting 'Expected counts large enough' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 large-sample association/difference in proportions. Typical analyses here include: Pearson chi-square test; Two-proportion z-test. Practical note: Equivalent in 2x2 under standard conditions.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Expected counts large enough' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "large: Expected counts large enough": "General explanation: Selecting 'Expected counts large enough' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 large-sample association/difference in proportions. Typical analyses here include: Pearson chi-square test; Two-proportion z-test. Practical note: Equivalent in 2x2 under standard conditions.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Expected counts large enough' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "small": "General explanation: Selecting 'Small expected counts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 small-sample exact. Typical analyses here include: Fisher exact test; Exact unconditional tests. Practical note: Prefer exact when expected counts are small.\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Small expected counts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Small expected counts": "General explanation: Selecting 'Small expected counts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 small-sample exact. Typical analyses here include: Fisher exact test; Exact unconditional tests. Practical note: Prefer exact when expected counts are small.\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Small expected counts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "small: Small expected counts": "General explanation: Selecting 'Small expected counts' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Independent 2x2 small-sample exact. Typical analyses here include: Fisher exact test; Exact unconditional tests. Practical note: Prefer exact when expected counts are small.\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Small expected counts' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A19": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is nominal categorical (≥2 unordered categories): what is the question?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a post-market surveillance analysis of infusion pump alarm burden in an ICU. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Goodness-of-fit (1 variable), Association between two categorical vars, Paired/matched nominal with >2 categories lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "gof": "General explanation: Selecting 'Goodness-of-fit (1 variable)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Does observed distribution match a specified distribution?. Typical analyses here include: Chi-square goodness-of-fit (multinomial); Exact multinomial tests (small n).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit (1 variable)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Goodness-of-fit (1 variable)": "General explanation: Selecting 'Goodness-of-fit (1 variable)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Does observed distribution match a specified distribution?. Typical analyses here include: Chi-square goodness-of-fit (multinomial); Exact multinomial tests (small n).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit (1 variable)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gof: Goodness-of-fit (1 variable)": "General explanation: Selecting 'Goodness-of-fit (1 variable)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Does observed distribution match a specified distribution?. Typical analyses here include: Chi-square goodness-of-fit (multinomial); Exact multinomial tests (small n).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit (1 variable)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "assoc": "General explanation: Selecting 'Association between two categorical vars' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Are two categorical variables associated?. Typical analyses here include: Chi-square test of independence; Fisher–Freeman–Halton exact (RxC small n).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association between two categorical vars' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Association between two categorical vars": "General explanation: Selecting 'Association between two categorical vars' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Are two categorical variables associated?. Typical analyses here include: Chi-square test of independence; Fisher–Freeman–Halton exact (RxC small n).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association between two categorical vars' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "assoc: Association between two categorical vars": "General explanation: Selecting 'Association between two categorical vars' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Are two categorical variables associated?. Typical analyses here include: Chi-square test of independence; Fisher–Freeman–Halton exact (RxC small n).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Association between two categorical vars' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pairedk": "General explanation: Selecting 'Paired/matched nominal with >2 categories' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal with >2 categories. Typical analyses here include: Stuart–Maxwell test (marginal homogeneity); Bowker test (symmetry).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched nominal with >2 categories' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Paired/matched nominal with >2 categories": "General explanation: Selecting 'Paired/matched nominal with >2 categories' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal with >2 categories. Typical analyses here include: Stuart–Maxwell test (marginal homogeneity); Bowker test (symmetry).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched nominal with >2 categories' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pairedk: Paired/matched nominal with >2 categories": "General explanation: Selecting 'Paired/matched nominal with >2 categories' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal with >2 categories. Typical analyses here include: Stuart–Maxwell test (marginal homogeneity); Bowker test (symmetry).\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched nominal with >2 categories' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A20": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is counts/rates: do you have an exposure/offset (time at risk, area, etc.)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine falls per 1,000 patient-days on two wards. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: rates with exposure/offset, No (counts) or after choosing Poisson GLM: check dispersion lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "rate": "General explanation: Selecting 'Yes: rates with exposure/offset' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: rates with exposure/offset' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: rates with exposure/offset": "General explanation: Selecting 'Yes: rates with exposure/offset' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: rates with exposure/offset' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "rate: Yes: rates with exposure/offset": "General explanation: Selecting 'Yes: rates with exposure/offset' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: rates with exposure/offset' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "counts": "General explanation: Selecting 'No (counts) or after choosing Poisson GLM: check dispersion' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are counts overdispersed or zero-inflated relative to Poisson?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No (counts) or after choosing Poisson GLM: check dispersion' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No (counts) or after choosing Poisson GLM: check dispersion": "General explanation: Selecting 'No (counts) or after choosing Poisson GLM: check dispersion' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are counts overdispersed or zero-inflated relative to Poisson?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No (counts) or after choosing Poisson GLM: check dispersion' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "counts: No (counts) or after choosing Poisson GLM: check dispersion": "General explanation: Selecting 'No (counts) or after choosing Poisson GLM: check dispersion' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are counts overdispersed or zero-inflated relative to Poisson?.\n\nExample: Suppose you are working on falls per 1,000 patient-days on two wards. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No (counts) or after choosing Poisson GLM: check dispersion' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A21": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Are counts overdispersed or zero-inflated relative to Poisson?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine falls per 1,000 patient-days on two wards. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like No: Poisson adequate, Yes: overdispersed/zero-inflated lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "poisson_ok": "General explanation: Selecting 'No: Poisson adequate' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: Poisson adequate": "General explanation: Selecting 'No: Poisson adequate' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "poisson_ok: No: Poisson adequate": "General explanation: Selecting 'No: Poisson adequate' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Rate comparison / incidence. Typical analyses here include: Poisson rate test / exact rate ratio test; Poisson regression with offset.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "overdispersed": "General explanation: Selecting 'Yes: overdispersed/zero-inflated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use overdispersion-aware models. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated Poisson/NB.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: overdispersed/zero-inflated": "General explanation: Selecting 'Yes: overdispersed/zero-inflated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use overdispersion-aware models. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated Poisson/NB.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "overdispersed: Yes: overdispersed/zero-inflated": "General explanation: Selecting 'Yes: overdispersed/zero-inflated' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Use overdispersion-aware models. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated Poisson/NB.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A22": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outcome is time-to-event: comparing survival curves between groups?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time to catheter occlusion for two catheter materials. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: compare groups' survival curves, Need covariate-adjusted survival model lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "compare": "General explanation: Selecting 'Yes: compare groups' survival curves' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric survival curve comparison. Typical analyses here include: Log-rank test. Practical note: Compares survival distributions across ≥2 groups.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: compare groups' survival curves' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: compare groups' survival curves": "General explanation: Selecting 'Yes: compare groups' survival curves' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric survival curve comparison. Typical analyses here include: Log-rank test. Practical note: Compares survival distributions across ≥2 groups.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: compare groups' survival curves' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "compare: Yes: compare groups' survival curves": "General explanation: Selecting 'Yes: compare groups' survival curves' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Nonparametric survival curve comparison. Typical analyses here include: Log-rank test. Practical note: Compares survival distributions across ≥2 groups.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: compare groups' survival curves' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "adjust": "General explanation: Selecting 'Need covariate-adjusted survival model' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Need covariate adjustment or time-varying predictors?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Need covariate-adjusted survival model' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Need covariate-adjusted survival model": "General explanation: Selecting 'Need covariate-adjusted survival model' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Need covariate adjustment or time-varying predictors?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Need covariate-adjusted survival model' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "adjust: Need covariate-adjusted survival model": "General explanation: Selecting 'Need covariate-adjusted survival model' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Need covariate adjustment or time-varying predictors?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Need covariate-adjusted survival model' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A23": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Need covariate adjustment or time-varying predictors?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time to recurrent hospitalization after starting remote monitoring. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Semi-parametric Cox PH, Parametric survival regression lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "cox": "General explanation: Selecting 'Semi-parametric Cox PH' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Semi-parametric survival regression. Typical analyses here include: Cox proportional hazards model. Practical note: Test predictors via likelihood ratio / Wald / score tests.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Semi-parametric Cox PH' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Semi-parametric Cox PH": "General explanation: Selecting 'Semi-parametric Cox PH' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Semi-parametric survival regression. Typical analyses here include: Cox proportional hazards model. Practical note: Test predictors via likelihood ratio / Wald / score tests.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Semi-parametric Cox PH' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "cox: Semi-parametric Cox PH": "General explanation: Selecting 'Semi-parametric Cox PH' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Semi-parametric survival regression. Typical analyses here include: Cox proportional hazards model. Practical note: Test predictors via likelihood ratio / Wald / score tests.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Semi-parametric Cox PH' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "param": "General explanation: Selecting 'Parametric survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Parametric survival regression. Typical analyses here include: Accelerated failure time (AFT) models; Parametric PH models (Weibull, log-logistic, etc.).\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Parametric survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Parametric survival regression": "General explanation: Selecting 'Parametric survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Parametric survival regression. Typical analyses here include: Accelerated failure time (AFT) models; Parametric PH models (Weibull, log-logistic, etc.).\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Parametric survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "param: Parametric survival regression": "General explanation: Selecting 'Parametric survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Parametric survival regression. Typical analyses here include: Accelerated failure time (AFT) models; Parametric PH models (Weibull, log-logistic, etc.).\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Parametric survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "A24": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Multivariate continuous outcome (vectors) or many correlated outcomes?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time-to-result (minutes) for lab turnaround after workflow changes. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like 1 sample vs known vector mean, 2 groups multivariate mean diff, Multiple DVs with group factors lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "hot1": "General explanation: Selecting '1 sample vs known vector mean' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample multivariate mean. Typical analyses here include: Hotelling’s T² (one-sample).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 sample vs known vector mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "1 sample vs known vector mean": "General explanation: Selecting '1 sample vs known vector mean' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample multivariate mean. Typical analyses here include: Hotelling’s T² (one-sample).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 sample vs known vector mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "hot1: 1 sample vs known vector mean": "General explanation: Selecting '1 sample vs known vector mean' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to One-sample multivariate mean. Typical analyses here include: Hotelling’s T² (one-sample).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '1 sample vs known vector mean' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "hot2": "General explanation: Selecting '2 groups multivariate mean diff' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two-sample multivariate mean. Typical analyses here include: Hotelling’s T² (two-sample).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups multivariate mean diff' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "2 groups multivariate mean diff": "General explanation: Selecting '2 groups multivariate mean diff' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two-sample multivariate mean. Typical analyses here include: Hotelling’s T² (two-sample).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups multivariate mean diff' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "hot2: 2 groups multivariate mean diff": "General explanation: Selecting '2 groups multivariate mean diff' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two-sample multivariate mean. Typical analyses here include: Hotelling’s T² (two-sample).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that '2 groups multivariate mean diff' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "manova": "General explanation: Selecting 'Multiple DVs with group factors' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple outcomes with group factor(s). Typical analyses here include: MANOVA.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple DVs with group factors' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Multiple DVs with group factors": "General explanation: Selecting 'Multiple DVs with group factors' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple outcomes with group factor(s). Typical analyses here include: MANOVA.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple DVs with group factors' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "manova: Multiple DVs with group factors": "General explanation: Selecting 'Multiple DVs with group factors' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple outcomes with group factor(s). Typical analyses here include: MANOVA.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple DVs with group factors' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "permanova": "General explanation: Selecting 'Distance-based nonparametric multivariate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Nonparametric multivariate distance-based. Typical analyses here include: PERMANOVA (permutation MANOVA).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Distance-based nonparametric multivariate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Distance-based nonparametric multivariate": "General explanation: Selecting 'Distance-based nonparametric multivariate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Nonparametric multivariate distance-based. Typical analyses here include: PERMANOVA (permutation MANOVA).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Distance-based nonparametric multivariate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "permanova: Distance-based nonparametric multivariate": "General explanation: Selecting 'Distance-based nonparametric multivariate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Nonparametric multivariate distance-based. Typical analyses here include: PERMANOVA (permutation MANOVA).\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Distance-based nonparametric multivariate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'B) Association between variables — what are the variable types?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a 5-point nurse usability Likert score for a new bedside monitor UI. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Continuous vs continuous, Ordinal/rank involved, Binary vs continuous lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "cont_cont": "General explanation: Selecting 'Continuous vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two continuous variables: is relationship approximately linear and variables approximately normal?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Continuous vs continuous": "General explanation: Selecting 'Continuous vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two continuous variables: is relationship approximately linear and variables approximately normal?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "cont_cont: Continuous vs continuous": "General explanation: Selecting 'Continuous vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Two continuous variables: is relationship approximately linear and variables approximately normal?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal": "General explanation: Selecting 'Ordinal/rank involved' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal/Rank variables: paired ranks or independent?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal/rank involved' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Ordinal/rank involved": "General explanation: Selecting 'Ordinal/rank involved' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal/Rank variables: paired ranks or independent?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal/rank involved' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal: Ordinal/rank involved": "General explanation: Selecting 'Ordinal/rank involved' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal/Rank variables: paired ranks or independent?.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal/rank involved' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bin_cont": "General explanation: Selecting 'Binary vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary vs continuous: is the continuous approximately normal within binary groups?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Binary vs continuous": "General explanation: Selecting 'Binary vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary vs continuous: is the continuous approximately normal within binary groups?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bin_cont: Binary vs continuous": "General explanation: Selecting 'Binary vs continuous' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary vs continuous: is the continuous approximately normal within binary groups?.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary vs continuous' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nom_nom": "General explanation: Selecting 'Nominal vs nominal' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal vs nominal: independent or paired?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal vs nominal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Nominal vs nominal": "General explanation: Selecting 'Nominal vs nominal' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal vs nominal: independent or paired?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal vs nominal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nom_nom: Nominal vs nominal": "General explanation: Selecting 'Nominal vs nominal' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal vs nominal: independent or paired?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal vs nominal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "trend": "General explanation: Selecting 'Binary proportions across ordered groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary proportions across ordered groups: testing for trend?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary proportions across ordered groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Binary proportions across ordered groups": "General explanation: Selecting 'Binary proportions across ordered groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary proportions across ordered groups: testing for trend?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary proportions across ordered groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "trend: Binary proportions across ordered groups": "General explanation: Selecting 'Binary proportions across ordered groups' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary proportions across ordered groups: testing for trend?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary proportions across ordered groups' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Two continuous variables: is relationship approximately linear and variables approximately normal?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine oxygen saturation (%) difference between sensors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: linear/normal, No: non-linear or non-normal lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "linear": "General explanation: Selecting 'Yes: linear/normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Linear association. Typical analyses here include: Pearson correlation; Simple linear regression t-test on slope.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear/normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: linear/normal": "General explanation: Selecting 'Yes: linear/normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Linear association. Typical analyses here include: Pearson correlation; Simple linear regression t-test on slope.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear/normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "linear: Yes: linear/normal": "General explanation: Selecting 'Yes: linear/normal' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Linear association. Typical analyses here include: Pearson correlation; Simple linear regression t-test on slope.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear/normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonlinear": "General explanation: Selecting 'No: non-linear or non-normal' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If non-normal/nonlinear: is association monotonic?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-linear or non-normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: non-linear or non-normal": "General explanation: Selecting 'No: non-linear or non-normal' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If non-normal/nonlinear: is association monotonic?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-linear or non-normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nonlinear: No: non-linear or non-normal": "General explanation: Selecting 'No: non-linear or non-normal' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: If non-normal/nonlinear: is association monotonic?.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: non-linear or non-normal' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'If non-normal/nonlinear: is association monotonic?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine systolic blood pressure (mmHg) measured by two devices. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: monotonic, No/unknown: general dependence lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "monotonic": "General explanation: Selecting 'Yes: monotonic' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Monotonic association. Typical analyses here include: Spearman rank correlation; Kendall’s tau.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: monotonic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: monotonic": "General explanation: Selecting 'Yes: monotonic' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Monotonic association. Typical analyses here include: Spearman rank correlation; Kendall’s tau.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: monotonic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "monotonic: Yes: monotonic": "General explanation: Selecting 'Yes: monotonic' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Monotonic association. Typical analyses here include: Spearman rank correlation; Kendall’s tau.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: monotonic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "general": "General explanation: Selecting 'No/unknown: general dependence' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to General dependence. Typical analyses here include: Distance correlation (often via permutation); Mutual information tests (resampling).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unknown: general dependence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No/unknown: general dependence": "General explanation: Selecting 'No/unknown: general dependence' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to General dependence. Typical analyses here include: Distance correlation (often via permutation); Mutual information tests (resampling).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unknown: general dependence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "general: No/unknown: general dependence": "General explanation: Selecting 'No/unknown: general dependence' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to General dependence. Typical analyses here include: Distance correlation (often via permutation); Mutual information tests (resampling).\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unknown: general dependence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Ordinal/Rank variables: paired ranks or independent?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a 5-point nurse usability Likert score for a new bedside monitor UI. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Use rank-based association lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "rank": "General explanation: Selecting 'Use rank-based association' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rank-based association. Typical analyses here include: Spearman; Kendall. Practical note: For ordered categorical with ties, consider polychoric/polyserial correlation models.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Use rank-based association' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Use rank-based association": "General explanation: Selecting 'Use rank-based association' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rank-based association. Typical analyses here include: Spearman; Kendall. Practical note: For ordered categorical with ties, consider polychoric/polyserial correlation models.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Use rank-based association' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "rank: Use rank-based association": "General explanation: Selecting 'Use rank-based association' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rank-based association. Typical analyses here include: Spearman; Kendall. Practical note: For ordered categorical with ties, consider polychoric/polyserial correlation models.\n\nExample: Suppose you are working on a 5-point nurse usability Likert score for a new bedside monitor UI. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Use rank-based association' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B5": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Binary vs continuous: is the continuous approximately normal within binary groups?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine presence/absence of atrial fibrillation detected by an algorithm. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes/No (still valid): use point-biserial or equivalent t-test lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "pbis": "General explanation: Selecting 'Yes/No (still valid): use point-biserial or equivalent t-test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Binary-continuous association. Typical analyses here include: Point-biserial correlation; Two-sample t-test equivalence. Practical note: Point-biserial is Pearson with 0/1 coding.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes/No (still valid): use point-biserial or equivalent t-test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes/No (still valid): use point-biserial or equivalent t-test": "General explanation: Selecting 'Yes/No (still valid): use point-biserial or equivalent t-test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Binary-continuous association. Typical analyses here include: Point-biserial correlation; Two-sample t-test equivalence. Practical note: Point-biserial is Pearson with 0/1 coding.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes/No (still valid): use point-biserial or equivalent t-test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "pbis: Yes/No (still valid): use point-biserial or equivalent t-test": "General explanation: Selecting 'Yes/No (still valid): use point-biserial or equivalent t-test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Binary-continuous association. Typical analyses here include: Point-biserial correlation; Two-sample t-test equivalence. Practical note: Point-biserial is Pearson with 0/1 coding.\n\nExample: Suppose you are working on adverse event occurrence (yes/no) after a new implant procedure. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes/No (still valid): use point-biserial or equivalent t-test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B6": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Nominal vs nominal: independent or paired?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a multicenter study measuring HbA1c change after a digital diabetes coaching program. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Independent, Paired/matched lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "independent": "General explanation: Selecting 'Independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Association of two categorical variables. Typical analyses here include: Chi-square test of independence; Fisher exact (small counts).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Independent": "General explanation: Selecting 'Independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Association of two categorical variables. Typical analyses here include: Chi-square test of independence; Fisher exact (small counts).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "independent: Independent": "General explanation: Selecting 'Independent' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Association of two categorical variables. Typical analyses here include: Chi-square test of independence; Fisher exact (small counts).\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independent' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired": "General explanation: Selecting 'Paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal association. Typical analyses here include: McNemar (2x2); Stuart–Maxwell/Bowker (>2 categories).\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Paired/matched": "General explanation: Selecting 'Paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal association. Typical analyses here include: McNemar (2x2); Stuart–Maxwell/Bowker (>2 categories).\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "paired: Paired/matched": "General explanation: Selecting 'Paired/matched' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired nominal association. Typical analyses here include: McNemar (2x2); Stuart–Maxwell/Bowker (>2 categories).\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired/matched' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "B7": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Binary proportions across ordered groups: testing for trend?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine adverse event occurrence (yes/no) after a new implant procedure. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: trend test lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "trend": "General explanation: Selecting 'Yes: trend test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Trend in proportions. Typical analyses here include: Cochran–Armitage trend test. Practical note: For ordered exposure/dose response.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: trend test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: trend test": "General explanation: Selecting 'Yes: trend test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Trend in proportions. Typical analyses here include: Cochran–Armitage trend test. Practical note: For ordered exposure/dose response.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: trend test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "trend: Yes: trend test": "General explanation: Selecting 'Yes: trend test' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Trend in proportions. Typical analyses here include: Cochran–Armitage trend test. Practical note: For ordered exposure/dose response.\n\nExample: Suppose you are working on presence/absence of atrial fibrillation detected by an algorithm. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: trend test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'C) Prediction/Modeling — what is the response variable type?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time to catheter occlusion for two catheter materials. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Continuous response, Binary response, Count response lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "continuous": "General explanation: Selecting 'Continuous response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous response: linear model assumptions reasonable?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Continuous response": "General explanation: Selecting 'Continuous response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous response: linear model assumptions reasonable?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "continuous: Continuous response": "General explanation: Selecting 'Continuous response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous response: linear model assumptions reasonable?.\n\nExample: Suppose you are working on systolic blood pressure (mmHg) measured by two devices. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "binary": "General explanation: Selecting 'Binary response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary response: modeling probability of success?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Binary response": "General explanation: Selecting 'Binary response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary response: modeling probability of success?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "binary: Binary response": "General explanation: Selecting 'Binary response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Binary response: modeling probability of success?.\n\nExample: Suppose you are working on test positive/negative compared with gold standard. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "count": "General explanation: Selecting 'Count response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Count response: Poisson-like?.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Count response": "General explanation: Selecting 'Count response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Count response: Poisson-like?.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "count: Count response": "General explanation: Selecting 'Count response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Count response: Poisson-like?.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Count response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal": "General explanation: Selecting 'Ordinal response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal response: ordered categories?.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Ordinal response": "General explanation: Selecting 'Ordinal response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal response: ordered categories?.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ordinal: Ordinal response": "General explanation: Selecting 'Ordinal response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Ordinal response: ordered categories?.\n\nExample: Suppose you are working on pain score categories (none/mild/moderate/severe) after physiotherapy. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "multiclass": "General explanation: Selecting 'Nominal multiclass response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal multiclass response: unordered categories?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal multiclass response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Nominal multiclass response": "General explanation: Selecting 'Nominal multiclass response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal multiclass response: unordered categories?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal multiclass response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "multiclass: Nominal multiclass response": "General explanation: Selecting 'Nominal multiclass response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Nominal multiclass response: unordered categories?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Nominal multiclass response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "survival": "General explanation: Selecting 'Time-to-event response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Time-to-event response: censored outcomes?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Time-to-event response": "General explanation: Selecting 'Time-to-event response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Time-to-event response: censored outcomes?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "survival: Time-to-event response": "General explanation: Selecting 'Time-to-event response' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Time-to-event response: censored outcomes?.\n\nExample: Suppose you are working on time to catheter occlusion for two catheter materials. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time-to-event response' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "clustered": "General explanation: Selecting 'Clustered/longitudinal structure (any response)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Clustered/longitudinal data: repeated measures or multi-level structure?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Clustered/longitudinal structure (any response)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Clustered/longitudinal structure (any response)": "General explanation: Selecting 'Clustered/longitudinal structure (any response)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Clustered/longitudinal data: repeated measures or multi-level structure?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Clustered/longitudinal structure (any response)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "clustered: Clustered/longitudinal structure (any response)": "General explanation: Selecting 'Clustered/longitudinal structure (any response)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Clustered/longitudinal data: repeated measures or multi-level structure?.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Clustered/longitudinal structure (any response)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Continuous response: linear model assumptions reasonable?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time-to-result (minutes) for lab turnaround after workflow changes. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: linear model OK, No: robust/nonlinear alternatives lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "ok": "General explanation: Selecting 'Yes: linear model OK' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Continuous regression/GLM. Typical analyses here include: Linear regression (OLS); ANCOVA (continuous + categorical predictors); Multiple regression. Practical note: Use robust SEs if heteroskedasticity.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear model OK' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: linear model OK": "General explanation: Selecting 'Yes: linear model OK' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Continuous regression/GLM. Typical analyses here include: Linear regression (OLS); ANCOVA (continuous + categorical predictors); Multiple regression. Practical note: Use robust SEs if heteroskedasticity.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear model OK' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ok: Yes: linear model OK": "General explanation: Selecting 'Yes: linear model OK' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Continuous regression/GLM. Typical analyses here include: Linear regression (OLS); ANCOVA (continuous + categorical predictors); Multiple regression. Practical note: Use robust SEs if heteroskedasticity.\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: linear model OK' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "alt": "General explanation: Selecting 'No: robust/nonlinear alternatives' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to If assumptions violated or relationship nonlinear. Typical analyses here include: Robust regression (Huber/M-estimators); Quantile regression; Generalized additive models (GAM).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: robust/nonlinear alternatives' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: robust/nonlinear alternatives": "General explanation: Selecting 'No: robust/nonlinear alternatives' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to If assumptions violated or relationship nonlinear. Typical analyses here include: Robust regression (Huber/M-estimators); Quantile regression; Generalized additive models (GAM).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: robust/nonlinear alternatives' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "alt: No: robust/nonlinear alternatives": "General explanation: Selecting 'No: robust/nonlinear alternatives' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to If assumptions violated or relationship nonlinear. Typical analyses here include: Robust regression (Huber/M-estimators); Quantile regression; Generalized additive models (GAM).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: robust/nonlinear alternatives' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Binary response: modeling probability of success?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine adverse event occurrence (yes/no) after a new implant procedure. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Logistic/probit lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "logit": "General explanation: Selecting 'Logistic/probit' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary regression. Typical analyses here include: Logistic regression; Probit regression.\n\nExample: Suppose you are working on readmission within 30 days (yes/no) after discharge. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Logistic/probit' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Logistic/probit": "General explanation: Selecting 'Logistic/probit' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary regression. Typical analyses here include: Logistic regression; Probit regression.\n\nExample: Suppose you are working on readmission within 30 days (yes/no) after discharge. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Logistic/probit' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "logit: Logistic/probit": "General explanation: Selecting 'Logistic/probit' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary regression. Typical analyses here include: Logistic regression; Probit regression.\n\nExample: Suppose you are working on readmission within 30 days (yes/no) after discharge. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Logistic/probit' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Count response: Poisson-like?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine falls per 1,000 patient-days on two wards. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Poisson adequate, Overdispersed/zero-inflated lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "poisson": "General explanation: Selecting 'Poisson adequate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Count regression. Typical analyses here include: Poisson regression (with offset if needed).\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Poisson adequate": "General explanation: Selecting 'Poisson adequate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Count regression. Typical analyses here include: Poisson regression (with offset if needed).\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "poisson: Poisson adequate": "General explanation: Selecting 'Poisson adequate' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Count regression. Typical analyses here include: Poisson regression (with offset if needed).\n\nExample: Suppose you are working on number of ventilator alarms per patient-day. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Poisson adequate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nb": "General explanation: Selecting 'Overdispersed/zero-inflated' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overdispersion/zero inflation. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated models.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Overdispersed/zero-inflated": "General explanation: Selecting 'Overdispersed/zero-inflated' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overdispersion/zero inflation. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated models.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "nb: Overdispersed/zero-inflated": "General explanation: Selecting 'Overdispersed/zero-inflated' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overdispersion/zero inflation. Typical analyses here include: Negative binomial regression; Quasi-Poisson; Zero-inflated models.\n\nExample: Suppose you are working on number of hypoglycemia episodes per month in CGM users. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Overdispersed/zero-inflated' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C5": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Ordinal response: ordered categories?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine pain score categories (none/mild/moderate/severe) after physiotherapy. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Ordinal regression lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "ord": "General explanation: Selecting 'Ordinal regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Ordinal regression. Typical analyses here include: Ordinal logistic (proportional odds); Ordinal probit.\n\nExample: Suppose you are working on NYHA functional class (I-IV) recorded at follow-up visits. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Ordinal regression": "General explanation: Selecting 'Ordinal regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Ordinal regression. Typical analyses here include: Ordinal logistic (proportional odds); Ordinal probit.\n\nExample: Suppose you are working on NYHA functional class (I-IV) recorded at follow-up visits. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ord: Ordinal regression": "General explanation: Selecting 'Ordinal regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Ordinal regression. Typical analyses here include: Ordinal logistic (proportional odds); Ordinal probit.\n\nExample: Suppose you are working on NYHA functional class (I-IV) recorded at follow-up visits. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Ordinal regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C6": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Nominal multiclass response: unordered categories?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a multicenter study measuring HbA1c change after a digital diabetes coaching program. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Multinomial regression lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "multi": "General explanation: Selecting 'Multinomial regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multinomial regression. Typical analyses here include: Multinomial logistic regression.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multinomial regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Multinomial regression": "General explanation: Selecting 'Multinomial regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multinomial regression. Typical analyses here include: Multinomial logistic regression.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multinomial regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "multi: Multinomial regression": "General explanation: Selecting 'Multinomial regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multinomial regression. Typical analyses here include: Multinomial logistic regression.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multinomial regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C7": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Time-to-event response: censored outcomes?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine time to first device-related malfunction after implantation. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Survival regression lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "surv": "General explanation: Selecting 'Survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Survival modeling. Typical analyses here include: Cox proportional hazards; AFT models. Practical note: Use partial likelihood inference; check PH assumptions.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Survival regression": "General explanation: Selecting 'Survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Survival modeling. Typical analyses here include: Cox proportional hazards; AFT models. Practical note: Use partial likelihood inference; check PH assumptions.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "surv: Survival regression": "General explanation: Selecting 'Survival regression' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Survival modeling. Typical analyses here include: Cox proportional hazards; AFT models. Practical note: Use partial likelihood inference; check PH assumptions.\n\nExample: Suppose you are working on time to first device-related malfunction after implantation. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Survival regression' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "C8": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Clustered/longitudinal data: repeated measures or multi-level structure?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a multicenter study measuring HbA1c change after a digital diabetes coaching program. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Subject-specific (random effects), Population-average (GEE) lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "mixed": "General explanation: Selecting 'Subject-specific (random effects)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed model / random effects. Typical analyses here include: LMM/GLMM. Practical note: Random intercepts/slopes; subject-specific inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Subject-specific (random effects)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Subject-specific (random effects)": "General explanation: Selecting 'Subject-specific (random effects)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed model / random effects. Typical analyses here include: LMM/GLMM. Practical note: Random intercepts/slopes; subject-specific inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Subject-specific (random effects)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mixed: Subject-specific (random effects)": "General explanation: Selecting 'Subject-specific (random effects)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Mixed model / random effects. Typical analyses here include: LMM/GLMM. Practical note: Random intercepts/slopes; subject-specific inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Subject-specific (random effects)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gee": "General explanation: Selecting 'Population-average (GEE)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Marginal model / correlated outcomes. Typical analyses here include: Generalized Estimating Equations (GEE). Practical note: Population-average inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Population-average (GEE)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Population-average (GEE)": "General explanation: Selecting 'Population-average (GEE)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Marginal model / correlated outcomes. Typical analyses here include: Generalized Estimating Equations (GEE). Practical note: Population-average inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Population-average (GEE)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gee: Population-average (GEE)": "General explanation: Selecting 'Population-average (GEE)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Marginal model / correlated outcomes. Typical analyses here include: Generalized Estimating Equations (GEE). Practical note: Population-average inference.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Population-average (GEE)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "D1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'D) Agreement / reliability — what is the measurement scale?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Continuous measures, Categorical labels, Classifier performance comparison lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "continuous": "General explanation: Selecting 'Continuous measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous measurements: agreement between two methods/raters?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Continuous measures": "General explanation: Selecting 'Continuous measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous measurements: agreement between two methods/raters?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "continuous: Continuous measures": "General explanation: Selecting 'Continuous measures' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Continuous measurements: agreement between two methods/raters?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous measures' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "categorical": "General explanation: Selecting 'Categorical labels' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Categorical labels: two raters or many raters?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Categorical labels' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Categorical labels": "General explanation: Selecting 'Categorical labels' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Categorical labels: two raters or many raters?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Categorical labels' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "categorical: Categorical labels": "General explanation: Selecting 'Categorical labels' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Categorical labels: two raters or many raters?.\n\nExample: Suppose you are working on comparing lab analyzer A vs analyzer B for potassium measurements. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Categorical labels' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "classifier": "General explanation: Selecting 'Classifier performance comparison' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare two classifiers on paired data?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Classifier performance comparison' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Classifier performance comparison": "General explanation: Selecting 'Classifier performance comparison' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare two classifiers on paired data?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Classifier performance comparison' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "classifier: Classifier performance comparison": "General explanation: Selecting 'Classifier performance comparison' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare two classifiers on paired data?.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Classifier performance comparison' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "D2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Continuous measurements: agreement between two methods/raters?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine comparing lab analyzer A vs analyzer B for potassium measurements. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Two methods: agreement/LOA, Rater reliability (continuous) lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "ba": "General explanation: Selecting 'Two methods: agreement/LOA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Method comparison. Typical analyses here include: Bland–Altman analysis (limits of agreement).\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two methods: agreement/LOA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Two methods: agreement/LOA": "General explanation: Selecting 'Two methods: agreement/LOA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Method comparison. Typical analyses here include: Bland–Altman analysis (limits of agreement).\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two methods: agreement/LOA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ba: Two methods: agreement/LOA": "General explanation: Selecting 'Two methods: agreement/LOA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Method comparison. Typical analyses here include: Bland–Altman analysis (limits of agreement).\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two methods: agreement/LOA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "icc": "General explanation: Selecting 'Rater reliability (continuous)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rater reliability (continuous). Typical analyses here include: Intraclass correlation coefficient (ICC). Practical note: Choose ICC form per design (Shrout & Fleiss).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Rater reliability (continuous)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Rater reliability (continuous)": "General explanation: Selecting 'Rater reliability (continuous)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rater reliability (continuous). Typical analyses here include: Intraclass correlation coefficient (ICC). Practical note: Choose ICC form per design (Shrout & Fleiss).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Rater reliability (continuous)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "icc: Rater reliability (continuous)": "General explanation: Selecting 'Rater reliability (continuous)' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Rater reliability (continuous). Typical analyses here include: Intraclass correlation coefficient (ICC). Practical note: Choose ICC form per design (Shrout & Fleiss).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Rater reliability (continuous)' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "D3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Categorical labels: two raters or many raters?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine assessing agreement between two pulse oximeters during motion. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Two raters, Many raters lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "two": "General explanation: Selecting 'Two raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two raters categorical agreement. Typical analyses here include: Cohen’s kappa (weighted kappa if ordinal).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Two raters": "General explanation: Selecting 'Two raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two raters categorical agreement. Typical analyses here include: Cohen’s kappa (weighted kappa if ordinal).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "two: Two raters": "General explanation: Selecting 'Two raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Two raters categorical agreement. Typical analyses here include: Cohen’s kappa (weighted kappa if ordinal).\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Two raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "many": "General explanation: Selecting 'Many raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Many raters categorical agreement. Typical analyses here include: Fleiss’ kappa; Krippendorff’s alpha.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Many raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Many raters": "General explanation: Selecting 'Many raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Many raters categorical agreement. Typical analyses here include: Fleiss’ kappa; Krippendorff’s alpha.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Many raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "many: Many raters": "General explanation: Selecting 'Many raters' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Many raters categorical agreement. Typical analyses here include: Fleiss’ kappa; Krippendorff’s alpha.\n\nExample: Suppose you are working on comparing a cuffless blood pressure algorithm to a calibrated sphygmomanometer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Many raters' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "D4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Compare two classifiers on paired data?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine assessing agreement between two pulse oximeters during motion. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Paired accuracy/dichotomous disagreement, Compare ROC AUCs lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "mcnemar": "General explanation: Selecting 'Paired accuracy/dichotomous disagreement' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired accuracy differences (binary correct/incorrect). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired accuracy/dichotomous disagreement' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Paired accuracy/dichotomous disagreement": "General explanation: Selecting 'Paired accuracy/dichotomous disagreement' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired accuracy differences (binary correct/incorrect). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired accuracy/dichotomous disagreement' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mcnemar: Paired accuracy/dichotomous disagreement": "General explanation: Selecting 'Paired accuracy/dichotomous disagreement' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Paired accuracy differences (binary correct/incorrect). Typical analyses here include: McNemar test.\n\nExample: Suppose you are working on assessing agreement between two pulse oximeters during motion. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Paired accuracy/dichotomous disagreement' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "delong": "General explanation: Selecting 'Compare ROC AUCs' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Compare ROC AUCs (correlated curves). Typical analyses here include: DeLong test for AUC differences.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare ROC AUCs' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Compare ROC AUCs": "General explanation: Selecting 'Compare ROC AUCs' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Compare ROC AUCs (correlated curves). Typical analyses here include: DeLong test for AUC differences.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare ROC AUCs' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "delong: Compare ROC AUCs": "General explanation: Selecting 'Compare ROC AUCs' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Compare ROC AUCs (correlated curves). Typical analyses here include: DeLong test for AUC differences.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare ROC AUCs' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'E) Diagnostics / assumptions — what do you need to check?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine minute-by-minute glucose readings from continuous glucose monitors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Normality, Equal variances, Independence/autocorrelation lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "normality": "General explanation: Selecting 'Normality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Normality check for residuals/sample?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Normality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Normality": "General explanation: Selecting 'Normality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Normality check for residuals/sample?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Normality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "normality: Normality": "General explanation: Selecting 'Normality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Normality check for residuals/sample?.\n\nExample: Suppose you are working on oxygen saturation (%) difference between sensors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Normality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "variance": "General explanation: Selecting 'Equal variances' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Equal variance / homoscedasticity check?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Equal variances": "General explanation: Selecting 'Equal variances' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Equal variance / homoscedasticity check?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "variance: Equal variances": "General explanation: Selecting 'Equal variances' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Equal variance / homoscedasticity check?.\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Equal variances' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "independence": "General explanation: Selecting 'Independence/autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independence/autocorrelation in residuals?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independence/autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Independence/autocorrelation": "General explanation: Selecting 'Independence/autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independence/autocorrelation in residuals?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independence/autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "independence: Independence/autocorrelation": "General explanation: Selecting 'Independence/autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Independence/autocorrelation in residuals?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Independence/autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gof": "General explanation: Selecting 'Goodness-of-fit / nested models' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Goodness-of-fit / nested model comparison?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit / nested models' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Goodness-of-fit / nested models": "General explanation: Selecting 'Goodness-of-fit / nested models' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Goodness-of-fit / nested model comparison?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit / nested models' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gof: Goodness-of-fit / nested models": "General explanation: Selecting 'Goodness-of-fit / nested models' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Goodness-of-fit / nested model comparison?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Goodness-of-fit / nested models' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "outliers": "General explanation: Selecting 'Outliers/influence' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outliers/influence in regression?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Outliers/influence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Outliers/influence": "General explanation: Selecting 'Outliers/influence' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outliers/influence in regression?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Outliers/influence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "outliers: Outliers/influence": "General explanation: Selecting 'Outliers/influence' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Outliers/influence in regression?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Outliers/influence' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Normality check for residuals/sample?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine oxygen saturation (%) difference between sensors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Choose a normality test lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "tests": "General explanation: Selecting 'Choose a normality test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Normality tests. Typical analyses here include: Shapiro–Wilk; Anderson–Darling; Kolmogorov–Smirnov (with estimated params caution).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a normality test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Choose a normality test": "General explanation: Selecting 'Choose a normality test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Normality tests. Typical analyses here include: Shapiro–Wilk; Anderson–Darling; Kolmogorov–Smirnov (with estimated params caution).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a normality test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "tests: Choose a normality test": "General explanation: Selecting 'Choose a normality test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Normality tests. Typical analyses here include: Shapiro–Wilk; Anderson–Darling; Kolmogorov–Smirnov (with estimated params caution).\n\nExample: Suppose you are working on time-to-result (minutes) for lab turnaround after workflow changes. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a normality test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Equal variance / homoscedasticity check?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine systolic blood pressure (mmHg) measured by two devices. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Choose a variance test lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "tests": "General explanation: Selecting 'Choose a variance test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Variance equality tests. Typical analyses here include: Levene / Brown–Forsythe; Bartlett (normality-sensitive).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a variance test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Choose a variance test": "General explanation: Selecting 'Choose a variance test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Variance equality tests. Typical analyses here include: Levene / Brown–Forsythe; Bartlett (normality-sensitive).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a variance test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "tests: Choose a variance test": "General explanation: Selecting 'Choose a variance test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Variance equality tests. Typical analyses here include: Levene / Brown–Forsythe; Bartlett (normality-sensitive).\n\nExample: Suppose you are working on blood glucose (mg/dL) measured by a CGM and by lab analyzer. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose a variance test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Independence/autocorrelation in residuals?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine minute-by-minute glucose readings from continuous glucose monitors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like General randomness, Regression residual autocorrelation, Time series residual autocorrelation overall lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "runs": "General explanation: Selecting 'General randomness' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Randomness/independence. Typical analyses here include: Runs test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'General randomness' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "General randomness": "General explanation: Selecting 'General randomness' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Randomness/independence. Typical analyses here include: Runs test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'General randomness' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "runs: General randomness": "General explanation: Selecting 'General randomness' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Randomness/independence. Typical analyses here include: Runs test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'General randomness' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "dw_bg": "General explanation: Selecting 'Regression residual autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Serial correlation in regression residuals. Typical analyses here include: Durbin–Watson; Breusch–Godfrey.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Regression residual autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Regression residual autocorrelation": "General explanation: Selecting 'Regression residual autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Serial correlation in regression residuals. Typical analyses here include: Durbin–Watson; Breusch–Godfrey.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Regression residual autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "dw_bg: Regression residual autocorrelation": "General explanation: Selecting 'Regression residual autocorrelation' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Serial correlation in regression residuals. Typical analyses here include: Durbin–Watson; Breusch–Godfrey.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Regression residual autocorrelation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ljung": "General explanation: Selecting 'Time series residual autocorrelation overall' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overall autocorrelation across lags. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series residual autocorrelation overall' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Time series residual autocorrelation overall": "General explanation: Selecting 'Time series residual autocorrelation overall' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overall autocorrelation across lags. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series residual autocorrelation overall' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ljung: Time series residual autocorrelation overall": "General explanation: Selecting 'Time series residual autocorrelation overall' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Overall autocorrelation across lags. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Time series residual autocorrelation overall' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E5": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Goodness-of-fit / nested model comparison?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a radiology AI triage tool aiming to reduce time-to-report for critical findings. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Choose GOF/nested test lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "gof": "General explanation: Selecting 'Choose GOF/nested test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Distribution/model GOF. Typical analyses here include: Chi-square GOF (categorical); Continuous GOF tests (AD/KS); Likelihood ratio test for nested models.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose GOF/nested test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Choose GOF/nested test": "General explanation: Selecting 'Choose GOF/nested test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Distribution/model GOF. Typical analyses here include: Chi-square GOF (categorical); Continuous GOF tests (AD/KS); Likelihood ratio test for nested models.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose GOF/nested test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "gof: Choose GOF/nested test": "General explanation: Selecting 'Choose GOF/nested test' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Distribution/model GOF. Typical analyses here include: Chi-square GOF (categorical); Continuous GOF tests (AD/KS); Likelihood ratio test for nested models.\n\nExample: Suppose you are working on a post-market surveillance analysis of infusion pump alarm burden in an ICU. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose GOF/nested test' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "E6": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Outliers/influence in regression?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Choose outlier/influence diagnostic lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "out": "General explanation: Selecting 'Choose outlier/influence diagnostic' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Outlier/influence diagnostics. Typical analyses here include: Grubbs (single outlier, normal); Generalized ESD (Rosner); Cook’s distance / leverage. Practical note: Use influence diagnostics rather than deleting by default.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose outlier/influence diagnostic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Choose outlier/influence diagnostic": "General explanation: Selecting 'Choose outlier/influence diagnostic' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Outlier/influence diagnostics. Typical analyses here include: Grubbs (single outlier, normal); Generalized ESD (Rosner); Cook’s distance / leverage. Practical note: Use influence diagnostics rather than deleting by default.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose outlier/influence diagnostic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "out: Choose outlier/influence diagnostic": "General explanation: Selecting 'Choose outlier/influence diagnostic' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Outlier/influence diagnostics. Typical analyses here include: Grubbs (single outlier, normal); Generalized ESD (Rosner); Cook’s distance / leverage. Practical note: Use influence diagnostics rather than deleting by default.\n\nExample: Suppose you are working on a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Choose outlier/influence diagnostic' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'F) Time series — what is the primary goal?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine daily counts of ED visits after a public health intervention. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Stationarity/unit root, Residual whiteness after ARMA/ARIMA, Predictive causality between series lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "stationarity": "General explanation: Selecting 'Stationarity/unit root' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Stationarity / unit root question?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Stationarity/unit root' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Stationarity/unit root": "General explanation: Selecting 'Stationarity/unit root' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Stationarity / unit root question?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Stationarity/unit root' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "stationarity: Stationarity/unit root": "General explanation: Selecting 'Stationarity/unit root' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Stationarity / unit root question?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Stationarity/unit root' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "whiteness": "General explanation: Selecting 'Residual whiteness after ARMA/ARIMA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Model diagnostics: are ARIMA/ARMA residuals white noise?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Residual whiteness after ARMA/ARIMA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Residual whiteness after ARMA/ARIMA": "General explanation: Selecting 'Residual whiteness after ARMA/ARIMA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Model diagnostics: are ARIMA/ARMA residuals white noise?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Residual whiteness after ARMA/ARIMA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "whiteness: Residual whiteness after ARMA/ARIMA": "General explanation: Selecting 'Residual whiteness after ARMA/ARIMA' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Model diagnostics: are ARIMA/ARMA residuals white noise?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Residual whiteness after ARMA/ARIMA' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "granger": "General explanation: Selecting 'Predictive causality between series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Causality/predictive precedence between series?.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Predictive causality between series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Predictive causality between series": "General explanation: Selecting 'Predictive causality between series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Causality/predictive precedence between series?.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Predictive causality between series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "granger: Predictive causality between series": "General explanation: Selecting 'Predictive causality between series' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Causality/predictive precedence between series?.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Predictive causality between series' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "breaks": "General explanation: Selecting 'Structural breaks' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Structural break / regime change?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Structural breaks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Structural breaks": "General explanation: Selecting 'Structural breaks' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Structural break / regime change?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Structural breaks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "breaks: Structural breaks": "General explanation: Selecting 'Structural breaks' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Structural break / regime change?.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Structural breaks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "forecast": "General explanation: Selecting 'Compare forecast accuracy' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare forecast accuracy between two models?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare forecast accuracy' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Compare forecast accuracy": "General explanation: Selecting 'Compare forecast accuracy' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare forecast accuracy between two models?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare forecast accuracy' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "forecast: Compare forecast accuracy": "General explanation: Selecting 'Compare forecast accuracy' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to the next decision: Compare forecast accuracy between two models?.\n\nExample: Suppose you are working on daily counts of ED visits after a public health intervention. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Compare forecast accuracy' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Stationarity / unit root question?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine daily counts of ED visits after a public health intervention. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like ADF/KPSS lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "adf_kpss": "General explanation: Selecting 'ADF/KPSS' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Stationarity / unit root tests. Typical analyses here include: ADF (Augmented Dickey–Fuller); KPSS. Practical note: ADF tests unit root null; KPSS tests stationarity null (complementary).\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'ADF/KPSS' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ADF/KPSS": "General explanation: Selecting 'ADF/KPSS' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Stationarity / unit root tests. Typical analyses here include: ADF (Augmented Dickey–Fuller); KPSS. Practical note: ADF tests unit root null; KPSS tests stationarity null (complementary).\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'ADF/KPSS' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "adf_kpss: ADF/KPSS": "General explanation: Selecting 'ADF/KPSS' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Stationarity / unit root tests. Typical analyses here include: ADF (Augmented Dickey–Fuller); KPSS. Practical note: ADF tests unit root null; KPSS tests stationarity null (complementary).\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'ADF/KPSS' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Model diagnostics: are ARIMA/ARMA residuals white noise?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine daily counts of ED visits after a public health intervention. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Box–Ljung lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "ljung": "General explanation: Selecting 'Box–Ljung' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Time series residual autocorrelation. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Box–Ljung' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Box–Ljung": "General explanation: Selecting 'Box–Ljung' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Time series residual autocorrelation. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Box–Ljung' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "ljung: Box–Ljung": "General explanation: Selecting 'Box–Ljung' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Time series residual autocorrelation. Typical analyses here include: Box–Ljung test.\n\nExample: Suppose you are working on minute-by-minute glucose readings from continuous glucose monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Box–Ljung' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F4": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Causality/predictive precedence between series?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a clinical evaluation of a smartphone-based blood pressure app compared with a cuff device. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Granger causality lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "granger": "General explanation: Selecting 'Granger causality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Predictive causality. Typical analyses here include: Granger causality tests.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Granger causality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Granger causality": "General explanation: Selecting 'Granger causality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Predictive causality. Typical analyses here include: Granger causality tests.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Granger causality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "granger: Granger causality": "General explanation: Selecting 'Granger causality' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Predictive causality. Typical analyses here include: Granger causality tests.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Granger causality' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F5": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Structural break / regime change?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a prospective study validating a wearable ECG patch against a 12-lead ECG reference. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Break tests lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "break": "General explanation: Selecting 'Break tests' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Break tests. Typical analyses here include: Chow test; CUSUM/CUSUMSQ; Bai–Perron multiple break tests.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Break tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Break tests": "General explanation: Selecting 'Break tests' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Break tests. Typical analyses here include: Chow test; CUSUM/CUSUMSQ; Bai–Perron multiple break tests.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Break tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "break: Break tests": "General explanation: Selecting 'Break tests' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Break tests. Typical analyses here include: Chow test; CUSUM/CUSUMSQ; Bai–Perron multiple break tests.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Break tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "F6": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Compare forecast accuracy between two models?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine minute-by-minute glucose readings from continuous glucose monitors. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Diebold–Mariano lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "dm": "General explanation: Selecting 'Diebold–Mariano' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Forecast comparison. Typical analyses here include: Diebold–Mariano test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diebold–Mariano' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Diebold–Mariano": "General explanation: Selecting 'Diebold–Mariano' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Forecast comparison. Typical analyses here include: Diebold–Mariano test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diebold–Mariano' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "dm: Diebold–Mariano": "General explanation: Selecting 'Diebold–Mariano' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Forecast comparison. Typical analyses here include: Diebold–Mariano test.\n\nExample: Suppose you are working on hourly ICU heart-rate data streamed from bedside monitors. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Diebold–Mariano' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "G1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'G) Combine results across studies (meta-analysis) — what is the outcome metric?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine pooling effect sizes from multiple hospital studies of the same sepsis prediction model. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Continuous effect sizes, Binary effect sizes, Heterogeneity lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "cont": "General explanation: Selecting 'Continuous effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Continuous effects (e.g., mean difference, SMD). Typical analyses here include: Inverse-variance fixed-effect meta-analysis; Random-effects meta-analysis (e.g., DerSimonian–Laird, REML).\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Continuous effect sizes": "General explanation: Selecting 'Continuous effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Continuous effects (e.g., mean difference, SMD). Typical analyses here include: Inverse-variance fixed-effect meta-analysis; Random-effects meta-analysis (e.g., DerSimonian–Laird, REML).\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "cont: Continuous effect sizes": "General explanation: Selecting 'Continuous effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Continuous effects (e.g., mean difference, SMD). Typical analyses here include: Inverse-variance fixed-effect meta-analysis; Random-effects meta-analysis (e.g., DerSimonian–Laird, REML).\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Continuous effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bin": "General explanation: Selecting 'Binary effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary effects (risk ratio/odds ratio). Typical analyses here include: Mantel–Haenszel fixed-effect pooling; Inverse-variance pooling on log scale.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Binary effect sizes": "General explanation: Selecting 'Binary effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary effects (risk ratio/odds ratio). Typical analyses here include: Mantel–Haenszel fixed-effect pooling; Inverse-variance pooling on log scale.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bin: Binary effect sizes": "General explanation: Selecting 'Binary effect sizes' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Binary effects (risk ratio/odds ratio). Typical analyses here include: Mantel–Haenszel fixed-effect pooling; Inverse-variance pooling on log scale.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Binary effect sizes' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "het": "General explanation: Selecting 'Heterogeneity' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess heterogeneity. Typical analyses here include: Cochran’s Q; I². Practical note: Use Q to test, I² to quantify heterogeneity.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Heterogeneity' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Heterogeneity": "General explanation: Selecting 'Heterogeneity' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess heterogeneity. Typical analyses here include: Cochran’s Q; I². Practical note: Use Q to test, I² to quantify heterogeneity.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Heterogeneity' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "het: Heterogeneity": "General explanation: Selecting 'Heterogeneity' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess heterogeneity. Typical analyses here include: Cochran’s Q; I². Practical note: Use Q to test, I² to quantify heterogeneity.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Heterogeneity' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bias": "General explanation: Selecting 'Publication bias' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess small-study/publication bias. Typical analyses here include: Egger regression test; Funnel plot asymmetry tests.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Publication bias' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Publication bias": "General explanation: Selecting 'Publication bias' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess small-study/publication bias. Typical analyses here include: Egger regression test; Funnel plot asymmetry tests.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Publication bias' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bias: Publication bias": "General explanation: Selecting 'Publication bias' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Assess small-study/publication bias. Typical analyses here include: Egger regression test; Funnel plot asymmetry tests.\n\nExample: Suppose you are working on pooling effect sizes from multiple hospital studies of the same sepsis prediction model. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Publication bias' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mult": "General explanation: Selecting 'Multiple testing control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple comparisons across many endpoints/studies. Typical analyses here include: Benjamini–Hochberg FDR control; Holm/Hochberg familywise control.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple testing control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Multiple testing control": "General explanation: Selecting 'Multiple testing control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple comparisons across many endpoints/studies. Typical analyses here include: Benjamini–Hochberg FDR control; Holm/Hochberg familywise control.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple testing control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mult: Multiple testing control": "General explanation: Selecting 'Multiple testing control' is a classification of your data and the claim you want to make. In healthcare and medtech, that classification is practical: it determines whether your endpoint is treated as continuous, ordinal, categorical, a count/rate, or time-to-event; whether observations are independent or repeated; and whether the most meaningful output is a mean difference, odds ratio, rate ratio, hazard ratio, or an agreement metric. Choosing the right option aligns the analysis with clinical meaning and with how the data were generated. In this flow, choosing this option routes you to Multiple comparisons across many endpoints/studies. Typical analyses here include: Benjamini–Hochberg FDR control; Holm/Hochberg familywise control.\n\nExample: Suppose you are working on combining sensitivity/specificity across trials of a rapid antigen test. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Multiple testing control' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "H1": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'H) Custom/advanced inference — do you have an explicit likelihood model?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a radiology AI triage tool aiming to reduce time-to-report for critical findings. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: likelihood-based tests, No/unclear: consider resampling or Bayesian lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "likelihood_yes": "General explanation: Selecting 'Yes: likelihood-based tests' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Likelihood-based tests. Typical analyses here include: Likelihood ratio test (LRT); Wald test; Score/Lagrange multiplier test. Practical note: Often asymptotically equivalent; choose based on robustness and computation.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: likelihood-based tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: likelihood-based tests": "General explanation: Selecting 'Yes: likelihood-based tests' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Likelihood-based tests. Typical analyses here include: Likelihood ratio test (LRT); Wald test; Score/Lagrange multiplier test. Practical note: Often asymptotically equivalent; choose based on robustness and computation.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: likelihood-based tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "likelihood_yes: Yes: likelihood-based tests": "General explanation: Selecting 'Yes: likelihood-based tests' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Likelihood-based tests. Typical analyses here include: Likelihood ratio test (LRT); Wald test; Score/Lagrange multiplier test. Practical note: Often asymptotically equivalent; choose based on robustness and computation.\n\nExample: Suppose you are working on a multicenter study measuring HbA1c change after a digital diabetes coaching program. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: likelihood-based tests' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "likelihood_no": "General explanation: Selecting 'No/unclear: consider resampling or Bayesian' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Can you justify exchangeability/randomization under the null (e.g., via design)?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unclear: consider resampling or Bayesian' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No/unclear: consider resampling or Bayesian": "General explanation: Selecting 'No/unclear: consider resampling or Bayesian' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Can you justify exchangeability/randomization under the null (e.g., via design)?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unclear: consider resampling or Bayesian' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "likelihood_no: No/unclear: consider resampling or Bayesian": "General explanation: Selecting 'No/unclear: consider resampling or Bayesian' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Can you justify exchangeability/randomization under the null (e.g., via design)?.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/unclear: consider resampling or Bayesian' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "H2": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Can you justify exchangeability/randomization under the null (e.g., via design)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a prospective study validating a wearable ECG patch against a 12-lead ECG reference. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: permutation/bootstrap appropriate, No: consider Bayesian/simulation lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "perm": "General explanation: Selecting 'Yes: permutation/bootstrap appropriate' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Resampling-based inference. Typical analyses here include: Permutation tests; Randomization tests; Bootstrap CIs/p-values. Practical note: Permutation requires valid null exchangeability; bootstrap targets sampling distribution.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: permutation/bootstrap appropriate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: permutation/bootstrap appropriate": "General explanation: Selecting 'Yes: permutation/bootstrap appropriate' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Resampling-based inference. Typical analyses here include: Permutation tests; Randomization tests; Bootstrap CIs/p-values. Practical note: Permutation requires valid null exchangeability; bootstrap targets sampling distribution.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: permutation/bootstrap appropriate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "perm: Yes: permutation/bootstrap appropriate": "General explanation: Selecting 'Yes: permutation/bootstrap appropriate' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Resampling-based inference. Typical analyses here include: Permutation tests; Randomization tests; Bootstrap CIs/p-values. Practical note: Permutation requires valid null exchangeability; bootstrap targets sampling distribution.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: permutation/bootstrap appropriate' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no_perm": "General explanation: Selecting 'No: consider Bayesian/simulation' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are Bayesian methods acceptable for your question (priors, posterior inference)?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: consider Bayesian/simulation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "No: consider Bayesian/simulation": "General explanation: Selecting 'No: consider Bayesian/simulation' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are Bayesian methods acceptable for your question (priors, posterior inference)?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: consider Bayesian/simulation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "no_perm: No: consider Bayesian/simulation": "General explanation: Selecting 'No: consider Bayesian/simulation' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to the next decision: Are Bayesian methods acceptable for your question (priors, posterior inference)?.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No: consider Bayesian/simulation' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline."
    }
  },
  "H3": {
    "question": "General explanation: This question is a decision point in the flowchart for selecting an appropriate statistical analysis in medtech and healthcare research. It asks: 'Are Bayesian methods acceptable for your question (priors, posterior inference)?'. The goal is to ensure your analysis matches your study design, outcome scale, and the clinical interpretation you need. In regulated settings (device validation, clinical performance evaluation, real-world evidence), these decisions should be made before looking at results because they affect error rates, confidence intervals, and how defensible your claims are. If you answer this question incorrectly, you can end up with effect sizes or p-values that do not reflect how clinicians think about benefit, harm, equivalence, or safety signals.\n\nExample: Imagine a post-market surveillance analysis of infusion pump alarm burden in an ICU. Before you run any test, you must answer this decision question. Depending on your answer, the flow takes you toward different families of methods (for example, options like Yes: Bayes factors/model comp, No/also: Bayesian estimation & checks lead to different model assumptions, effect measures, and visualizations). Making the decision explicit helps you justify the final method in a clinical evaluation report or statistical analysis plan.",
    "options": {
      "bf": "General explanation: Selecting 'Yes: Bayes factors/model comp' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Bayesian model comparison. Typical analyses here include: Bayes factors; Posterior model probabilities.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: Bayes factors/model comp' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "Yes: Bayes factors/model comp": "General explanation: Selecting 'Yes: Bayes factors/model comp' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Bayesian model comparison. Typical analyses here include: Bayes factors; Posterior model probabilities.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: Bayes factors/model comp' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "bf: Yes: Bayes factors/model comp": "General explanation: Selecting 'Yes: Bayes factors/model comp' means the assumption or condition described is reasonably satisfied for your dataset. In healthcare analytics, this often implies your endpoint and residual behavior are stable enough to use methods that rely on that assumption (commonly parametric tests or models with interpretable effect estimates). You cannot prove the assumption, but you can justify it with study design (randomization, blocking, matching), adequate sample size, and diagnostics (plots, residual checks). This choice often yields more power and cleaner confidence intervals, but it still requires you to report uncertainty, check sensitivity, and confirm that the clinical conclusion does not hinge on a fragile modeling choice. In this flow, choosing this option routes you to Bayesian model comparison. Typical analyses here include: Bayes factors; Posterior model probabilities.\n\nExample: Suppose you are working on a radiology AI triage tool aiming to reduce time-to-report for critical findings. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'Yes: Bayes factors/model comp' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline.",
      "mcmc": "General explanation: Selecting 'No/also: Bayesian estimation & checks' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Bayesian estimation. Typical analyses here include: MCMC/VI posterior inference; Posterior predictive checks.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/also: Bayesian estimation & checks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline. References and further reading Below are the sources embedded in the flowchart metadata, plus a few widely used healthcare/biostatistics references commonly cited for survival analysis and agreement metrics. [JGF_SPEC] JSON Graph Format Specification — https://jsongraphformat.info/ [JGF_GITHUB] json-graph-specification (GitHub) — https://github.com/jsongraph/json-graph-specification [NIST_EHANDBOOK] NIST/SEMATECH e-Handbook of Statistical Methods — https://www.itl.nist.gov/div898/handbook/ [UCLA_WHATSTAT] UCLA OARC: What statistical analysis should I use? — https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/ [NYU_CHOOSE_TEST] NYU Quantitative Analysis Guide: Choosing a Statistical Test — https://guides.nyu.edu/quant/choose_test_1DV [NIST_NORMALITY] NIST: Anderson-Darling and Shapiro-Wilk tests — https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm [NIST_BOXLJUNG] NIST: Box-Ljung Test — https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm [LOGRANK_PMC] The logrank test (Bland, 2004) - PMC — https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/ [BLAND_ALTMAN_PUBMED] Bland & Altman (1986) agreement paper - PubMed — https://pubmed.ncbi.nlm.nih.gov/2868172/ [DELONG_PUBMED] DeLong et al. (1988) correlated ROC AUC comparison - PubMed — https://pubmed.ncbi.nlm.nih.gov/3203132/ [ICC_PUBMED] Shrout & Fleiss (1979) intraclass correlations - PubMed — https://pubmed.ncbi.nlm.nih.gov/18839484/ [EGGER_BMJ] Egger et al. (1997) publication bias test - BMJ — https://www.bmj.com/content/315/7109/629 [BH_FDR] Benjamini & Hochberg (1995) FDR procedure - JRSS B — https://academic.oup.com/jrsssb/article/57/1/289/7035855 [GRANGER_1969] Granger (1969) causality paper (PDF) — https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf [DIEBOLD_MARIANO_1995] Diebold & Mariano (1995) Comparing Predictive Accuracy — https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599 [KPSS_1992] Kwiatkowski et al. (1992) stationarity test paper — https://www.sciencedirect.com/science/article/pii/030440769290104Y [COCHRANS_Q_NCSS] NCSS: Cochran’s Q Test (binary matched sets) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf [MCNEMAR_NCSS] NCSS/PASS: McNemar test (paired proportions) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf [COCHRANE_HANDBOOK] Cochrane Handbook (Chapter 10: meta-analysis) — https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10 [MANTEL_HAENSZEL_WHO] IARC/WHO text describing Mantel–Haenszel method (PDF) — https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf [COCHRAN_Q_PUBMED] Hoaglin (2016) paper on Q and heterogeneity - PubMed — https://pubmed.ncbi.nlm.nih.gov/26303773/ [CA_TREND_SCIDIR] Neuhäuser (1999) Cochran–Armitage trend test note — https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917 Additional sources used for examples and background: - StatPearls Survival Analysis (NCBI Bookshelf) — https://www.ncbi.nlm.nih.gov/sites/books/NBK560604/ - Interrater reliability: the kappa statistic (Biochemia Medica) — https://biochemia-medica.com/en/journal/22/3/10.11613/BM.2012.031/fullArticle - Methods to Analyze Time-to-Event Data: The Cox Regression Analysis (PubMed) — https://pubmed.ncbi.nlm.nih.gov/34887996/ - Stata Manual: icc — Intraclass correlation coefficients (PDF) — https://www.stata.com/manuals/ricc.pdf - Granger causality overview (Wikipedia) — https://en.wikipedia.org/wiki/Granger_causality",
      "No/also: Bayesian estimation & checks": "General explanation: Selecting 'No/also: Bayesian estimation & checks' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Bayesian estimation. Typical analyses here include: MCMC/VI posterior inference; Posterior predictive checks.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/also: Bayesian estimation & checks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline. References and further reading Below are the sources embedded in the flowchart metadata, plus a few widely used healthcare/biostatistics references commonly cited for survival analysis and agreement metrics. [JGF_SPEC] JSON Graph Format Specification — https://jsongraphformat.info/ [JGF_GITHUB] json-graph-specification (GitHub) — https://github.com/jsongraph/json-graph-specification [NIST_EHANDBOOK] NIST/SEMATECH e-Handbook of Statistical Methods — https://www.itl.nist.gov/div898/handbook/ [UCLA_WHATSTAT] UCLA OARC: What statistical analysis should I use? — https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/ [NYU_CHOOSE_TEST] NYU Quantitative Analysis Guide: Choosing a Statistical Test — https://guides.nyu.edu/quant/choose_test_1DV [NIST_NORMALITY] NIST: Anderson-Darling and Shapiro-Wilk tests — https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm [NIST_BOXLJUNG] NIST: Box-Ljung Test — https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm [LOGRANK_PMC] The logrank test (Bland, 2004) - PMC — https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/ [BLAND_ALTMAN_PUBMED] Bland & Altman (1986) agreement paper - PubMed — https://pubmed.ncbi.nlm.nih.gov/2868172/ [DELONG_PUBMED] DeLong et al. (1988) correlated ROC AUC comparison - PubMed — https://pubmed.ncbi.nlm.nih.gov/3203132/ [ICC_PUBMED] Shrout & Fleiss (1979) intraclass correlations - PubMed — https://pubmed.ncbi.nlm.nih.gov/18839484/ [EGGER_BMJ] Egger et al. (1997) publication bias test - BMJ — https://www.bmj.com/content/315/7109/629 [BH_FDR] Benjamini & Hochberg (1995) FDR procedure - JRSS B — https://academic.oup.com/jrsssb/article/57/1/289/7035855 [GRANGER_1969] Granger (1969) causality paper (PDF) — https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf [DIEBOLD_MARIANO_1995] Diebold & Mariano (1995) Comparing Predictive Accuracy — https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599 [KPSS_1992] Kwiatkowski et al. (1992) stationarity test paper — https://www.sciencedirect.com/science/article/pii/030440769290104Y [COCHRANS_Q_NCSS] NCSS: Cochran’s Q Test (binary matched sets) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf [MCNEMAR_NCSS] NCSS/PASS: McNemar test (paired proportions) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf [COCHRANE_HANDBOOK] Cochrane Handbook (Chapter 10: meta-analysis) — https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10 [MANTEL_HAENSZEL_WHO] IARC/WHO text describing Mantel–Haenszel method (PDF) — https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf [COCHRAN_Q_PUBMED] Hoaglin (2016) paper on Q and heterogeneity - PubMed — https://pubmed.ncbi.nlm.nih.gov/26303773/ [CA_TREND_SCIDIR] Neuhäuser (1999) Cochran–Armitage trend test note — https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917 Additional sources used for examples and background: - StatPearls Survival Analysis (NCBI Bookshelf) — https://www.ncbi.nlm.nih.gov/sites/books/NBK560604/ - Interrater reliability: the kappa statistic (Biochemia Medica) — https://biochemia-medica.com/en/journal/22/3/10.11613/BM.2012.031/fullArticle - Methods to Analyze Time-to-Event Data: The Cox Regression Analysis (PubMed) — https://pubmed.ncbi.nlm.nih.gov/34887996/ - Stata Manual: icc — Intraclass correlation coefficients (PDF) — https://www.stata.com/manuals/ricc.pdf - Granger causality overview (Wikipedia) — https://en.wikipedia.org/wiki/Granger_causality",
      "mcmc: No/also: Bayesian estimation & checks": "General explanation: Selecting 'No/also: Bayesian estimation & checks' means the assumption or condition is not credible, or you prefer not to rely on it. In medtech studies this is common: outcomes can be skewed (length of stay), bounded (SpO2), heavy-tailed (measurement error spikes), or sample sizes can be small (pilot feasibility). This option pushes you toward robust or distribution-free methods that protect validity when classical assumptions fail. You may trade some power for stronger guarantees on Type I error and a result that is harder to dismiss in peer review or regulatory review, especially when data quality varies across sites, devices, or operators. In this flow, choosing this option routes you to Bayesian estimation. Typical analyses here include: MCMC/VI posterior inference; Posterior predictive checks.\n\nExample: Suppose you are working on a prospective study validating a wearable ECG patch against a 12-lead ECG reference. You inspect how the endpoint is recorded and how subjects are sampled, and you decide that 'No/also: Bayesian estimation & checks' best describes your situation. That decision changes what you compute and report: the flow either asks the next clarifying question or points you directly to a recommended method family. In a medtech report, you would document why this option applies (measurement scale, pairing or independence, and the clinical interpretation), then implement the downstream method and include sensitivity analyses if the decision is borderline. References and further reading Below are the sources embedded in the flowchart metadata, plus a few widely used healthcare/biostatistics references commonly cited for survival analysis and agreement metrics. [JGF_SPEC] JSON Graph Format Specification — https://jsongraphformat.info/ [JGF_GITHUB] json-graph-specification (GitHub) — https://github.com/jsongraph/json-graph-specification [NIST_EHANDBOOK] NIST/SEMATECH e-Handbook of Statistical Methods — https://www.itl.nist.gov/div898/handbook/ [UCLA_WHATSTAT] UCLA OARC: What statistical analysis should I use? — https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/ [NYU_CHOOSE_TEST] NYU Quantitative Analysis Guide: Choosing a Statistical Test — https://guides.nyu.edu/quant/choose_test_1DV [NIST_NORMALITY] NIST: Anderson-Darling and Shapiro-Wilk tests — https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm [NIST_BOXLJUNG] NIST: Box-Ljung Test — https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm [LOGRANK_PMC] The logrank test (Bland, 2004) - PMC — https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/ [BLAND_ALTMAN_PUBMED] Bland & Altman (1986) agreement paper - PubMed — https://pubmed.ncbi.nlm.nih.gov/2868172/ [DELONG_PUBMED] DeLong et al. (1988) correlated ROC AUC comparison - PubMed — https://pubmed.ncbi.nlm.nih.gov/3203132/ [ICC_PUBMED] Shrout & Fleiss (1979) intraclass correlations - PubMed — https://pubmed.ncbi.nlm.nih.gov/18839484/ [EGGER_BMJ] Egger et al. (1997) publication bias test - BMJ — https://www.bmj.com/content/315/7109/629 [BH_FDR] Benjamini & Hochberg (1995) FDR procedure - JRSS B — https://academic.oup.com/jrsssb/article/57/1/289/7035855 [GRANGER_1969] Granger (1969) causality paper (PDF) — https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf [DIEBOLD_MARIANO_1995] Diebold & Mariano (1995) Comparing Predictive Accuracy — https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599 [KPSS_1992] Kwiatkowski et al. (1992) stationarity test paper — https://www.sciencedirect.com/science/article/pii/030440769290104Y [COCHRANS_Q_NCSS] NCSS: Cochran’s Q Test (binary matched sets) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf [MCNEMAR_NCSS] NCSS/PASS: McNemar test (paired proportions) (PDF) — https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf [COCHRANE_HANDBOOK] Cochrane Handbook (Chapter 10: meta-analysis) — https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10 [MANTEL_HAENSZEL_WHO] IARC/WHO text describing Mantel–Haenszel method (PDF) — https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf [COCHRAN_Q_PUBMED] Hoaglin (2016) paper on Q and heterogeneity - PubMed — https://pubmed.ncbi.nlm.nih.gov/26303773/ [CA_TREND_SCIDIR] Neuhäuser (1999) Cochran–Armitage trend test note — https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917 Additional sources used for examples and background: - StatPearls Survival Analysis (NCBI Bookshelf) — https://www.ncbi.nlm.nih.gov/sites/books/NBK560604/ - Interrater reliability: the kappa statistic (Biochemia Medica) — https://biochemia-medica.com/en/journal/22/3/10.11613/BM.2012.031/fullArticle - Methods to Analyze Time-to-Event Data: The Cox Regression Analysis (PubMed) — https://pubmed.ncbi.nlm.nih.gov/34887996/ - Stata Manual: icc — Intraclass correlation coefficients (PDF) — https://www.stata.com/manuals/ricc.pdf - Granger causality overview (Wikipedia) — https://en.wikipedia.org/wiki/Granger_causality"
    }
  }
};
// ===== Test explanations injected from Leaf_Node_Stat_Tests_Explanations.docx =====
const TEST_EXPLANATIONS = {
  "ADF (Augmented Dickey–Fuller)": "General explanation: ADF (Augmented Dickey–Fuller) is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use ADF (Augmented Dickey–Fuller) within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "AFT models": "General explanation: AFT models is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply AFT models to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "ANCOVA (continuous + categorical predictors)": "General explanation: ANCOVA (continuous + categorical predictors) extends group comparison by adjusting for continuous covariates, which is common in healthcare when baseline risk varies across patients. For example, when comparing an intervention vs control arm for a blood pressure endpoint, baseline blood pressure strongly predicts follow-up, and adjusting for it can reduce noise and improve precision. ANCOVA models the outcome as a function of treatment group plus covariates (and sometimes interactions), producing an adjusted treatment effect. Key options include which covariates are pre-specified (to avoid p-hacking), whether you include interaction terms (e.g., treatment-by-baseline), and whether model assumptions like linearity and equal slopes across groups are reasonable. In medtech, ANCOVA is especially useful when trials are small and you want to preserve power, but it requires careful handling of missing baseline data and a clear definition of the estimand consistent with the clinical question.\n\nExample: Example (medtech/healthcare): A randomized trial evaluates a connected inhaler coaching app for asthma control. The primary endpoint is a continuous symptom score at 12 weeks. Because baseline symptom score strongly predicts follow-up, you fit ANCOVA (continuous + categorical predictors) with treatment group plus baseline score and season as covariates. The adjusted treatment effect is a 2.1-point improvement with a 95% CI that excludes the minimum clinically important difference. You also check treatment-by-baseline interaction to confirm the effect is not limited to severe patients. Finally, you report adjusted means by group (least-squares means) to make results understandable to clinicians.",
  "Accelerated failure time (AFT) models": "General explanation: Accelerated failure time (AFT) models is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Accelerated failure time (AFT) models to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Anderson–Darling": "General explanation: Anderson–Darling is typically used to assess whether a sample behaves like it came from a target distribution, most commonly to check normality before applying parametric methods. In healthcare studies, distribution checks matter because endpoints like biomarker concentrations, length of stay, and device error often have skewness or heavy tails. Normality or goodness-of-fit tests provide an objective signal, but they are sensitive to sample size: with large clinical datasets they can flag tiny, clinically irrelevant deviations, while with small pilot studies they may have low power. Practical options include choosing a test appropriate for the sample size and the type of deviation you care about (tail behavior vs central fit), using Q-Q plots alongside p-values, and considering transformations (log, Box-Cox) or robust/nonparametric alternatives when normality is implausible. The result should be interpreted in the context of the downstream method’s robustness, not as a standalone gatekeeper.\n\nExample: Example (medtech/healthcare): A lab team compares two assay protocols and plans to use a pooled-variance t-test on log-transformed concentrations. Before finalizing, they check whether the transformed concentrations are approximately normal within each protocol group. They run Anderson–Darling and inspect Q-Q plots. The test suggests mild tail deviations in a small subgroup of hemolyzed samples. Rather than deleting data, they document a sensitivity analysis: a robust comparison (e.g., Welch or nonparametric) and a bootstrap CI for the mean difference. If conclusions agree across approaches, they proceed with the pre-specified parametric analysis and note that the normality check did not materially change interpretation.",
  "Bai–Perron multiple break tests": "General explanation: Bai–Perron multiple break tests is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Bai–Perron multiple break tests within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Bartlett (normality-sensitive)": "General explanation: Bartlett (normality-sensitive) is used to evaluate whether different groups have similar variability, which is an important assumption for several classical parametric comparisons. In medtech, variance differences are common: a glucose sensor might be much noisier in hypoglycemia than euglycemia, or measurement error may increase with body motion. Variance-homogeneity tests help you decide whether a pooled-variance t-test/ANOVA is appropriate or whether you should switch to Welch-type methods or model heteroskedasticity explicitly. Options include which variance test to use: Bartlett is powerful under normality but can be overly sensitive when data are skewed; Levene and Brown–Forsythe are more robust because they operate on absolute deviations from a center (mean or median). In regulated analyses, the key is not merely to report the p-value, but to show how the decision affected your primary inference and to document a pre-specified fallback analysis.\n\nExample: Example (medtech/healthcare): You compare measurement error of a glucose sensor across three temperature ranges: cold, room temperature, and warm. Before running a classical ANOVA on mean error, you test whether the error variance is similar across temperature groups using Bartlett (normality-sensitive). The test indicates heteroskedasticity, with higher variance in the warm condition (likely due to sweat and motion artifacts). You then use Welch ANOVA and a heteroskedasticity-robust regression as sensitivity analyses. In your validation report, you explain that unequal variances were expected physiologically and that the conclusion about mean error differences is robust to the variance structure. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Bayes factors": "General explanation: Bayes factors represents Bayesian evidence quantification or inference. In medtech, Bayesian methods are often used when prior information exists (earlier device versions, published studies) or when sample sizes are limited and you want probabilistic statements that align with decision-making. Bayes factors compare how well competing hypotheses predict the observed data, providing a graded measure of evidence rather than a binary p-value threshold. Posterior inference methods such as MCMC (Markov chain Monte Carlo) or variational inference (VI) compute distributions over parameters—useful for uncertainty quantification in risk models or calibration curves. Key options include specifying priors (weakly informative vs informative), choosing computational settings (chains, warmup, convergence diagnostics), and defining decision thresholds that make sense clinically. Bayesian outputs should be communicated in clinically interpretable terms (e.g., probability a sensitivity improvement exceeds a minimum clinically important difference).\n\nExample: Example (medtech/healthcare): A startup has prior data from an earlier glucose-sensor version and runs a small confirmatory study for a new coating. They use Bayes factors to quantify evidence that mean absolute relative difference improved by at least 1 percentage point. A weakly informative prior encodes plausible improvement ranges based on historical studies, and posterior inference yields the probability that the improvement exceeds the clinically meaningful threshold. They report posterior intervals, run convergence diagnostics if using MCMC, and perform sensitivity analyses with different priors. The Bayesian summary helps internal decision-making (ship vs iterate) while still translating results into familiar metrics for external stakeholders.",
  "Benjamini–Hochberg FDR control": "General explanation: Benjamini–Hochberg FDR control is a multiplicity-control procedure used when you run many hypothesis tests—common in medtech when evaluating multiple endpoints, multiple sensor channels, many subgroups, or large biomarker panels. Without correction, the chance of at least one false positive can become large. Familywise error rate methods (like Holm/Hochberg) aim to keep the probability of any false discovery below a target, which is conservative but attractive for high-stakes claims. False discovery rate (FDR) control (like Benjamini–Hochberg) instead controls the expected proportion of false positives among the discoveries, which can improve power in exploratory settings. Options include the error rate you want to control (FWER vs FDR), whether tests are independent or positively dependent (which affects guarantees), and whether you pre-specify a hierarchy of endpoints. In healthcare documentation, it is important to align the correction choice with the claim type: confirmatory vs exploratory.\n\nExample: Example (medtech/healthcare): A multi-sensor study evaluates 25 candidate digital biomarkers for predicting exacerbations in asthma. To avoid declaring success due to chance, the team applies Benjamini–Hochberg FDR control across the 25 p-values. They predefine that only biomarkers passing the adjusted threshold will be treated as ‘confirmed’ and moved to validation, while others remain exploratory. They also report adjusted q-values and the expected number of false discoveries, so clinicians understand the trade-off between sensitivity to true signals and protection from false positives. This supports disciplined feature selection before building a regulated model that would influence patient care. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Bland–Altman analysis (limits of agreement)": "General explanation: Bland–Altman analysis (limits of agreement) is a method-comparison approach that focuses on agreement, not association. In healthcare device validation, you often have a new measurement method (e.g., a wearable sensor or new lab assay) and a reference method (e.g., a benchtop analyzer). Even if the correlation is high, the new method can be biased or have clinically unacceptable error. Bland–Altman analysis plots the pairwise difference (new minus reference) against the pairwise mean and summarizes (1) the average bias and (2) limits of agreement that reflect how far individual measurements may deviate. Important choices include whether you need a proportional bias check, whether you should analyze repeated measures per patient (which changes the variance calculation), and whether you use absolute or percentage differences depending on the clinical scale. It is most useful when you care about replaceability or interchangeability of measurements.\n\nExample: Example (medtech/healthcare): You are validating a new cuffless blood-pressure wearable against a reference oscillometric cuff in 60 patients. Each patient has three paired measurements at rest and three after a short walk. You compute differences (wearable minus cuff) and plot them against the mean of the two methods. The bias is +2 mmHg, but the 95% limits of agreement are wide (e.g., -14 to +18 mmHg), showing that individual readings can deviate enough to change a hypertension stage. You also notice the differences increase with higher pressures, suggesting proportional bias; you therefore evaluate a calibration update and rerun the analysis. You summarize results per ISO-style performance criteria and explain whether the wearable can replace the cuff for clinical decision-making or only for trend monitoring.",
  "Bootstrap CIs/p-values": "General explanation: Bootstrap CIs/p-values relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Bootstrap CIs/p-values to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Bowker test (symmetry)": "General explanation: Bowker test (symmetry) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Bowker test (symmetry) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Box–Ljung test": "General explanation: Box–Ljung test is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Box–Ljung test within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Breusch–Godfrey": "General explanation: Breusch–Godfrey is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Breusch–Godfrey within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Brunner–Munzel": "General explanation: Brunner–Munzel is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Brunner–Munzel to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "Brunner–Munzel (if stochastic dominance questionable)": "General explanation: Brunner–Munzel (if stochastic dominance questionable) is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Brunner–Munzel (if stochastic dominance questionable) to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "CUSUM/CUSUMSQ": "General explanation: CUSUM/CUSUMSQ is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use CUSUM/CUSUMSQ within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Chi-square GOF (categorical)": "General explanation: Chi-square GOF (categorical) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Chi-square GOF (categorical) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Chi-square goodness-of-fit (multinomial)": "General explanation: Chi-square goodness-of-fit (multinomial) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Chi-square goodness-of-fit (multinomial) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Chi-square test of homogeneity/independence": "General explanation: Chi-square test of homogeneity/independence applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Chi-square test of homogeneity/independence to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Chi-square test of independence": "General explanation: Chi-square test of independence applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Chi-square test of independence to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Chow test": "General explanation: Chow test is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Chow test within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Cochran–Armitage trend test": "General explanation: Cochran–Armitage trend test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Cochran–Armitage trend test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Cochran’s Q": "General explanation: Cochran’s Q applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Cochran’s Q to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Cochran’s Q test": "General explanation: Cochran’s Q test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Cochran’s Q test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Cohen’s kappa (weighted kappa if ordinal)": "General explanation: Cohen’s kappa (weighted kappa if ordinal) is used to quantify agreement between human or algorithmic raters when the outcome is categorical (for example, radiology categories, ECG rhythm labels, or adverse event severity grades). Raw percent agreement can be misleading when one category is very common, because raters can “agree” by chance. Chance-corrected coefficients like kappa and related reliability statistics adjust for expected agreement under independent rating. Options usually include choosing unweighted vs weighted versions (weighted is common for ordinal clinical scales where a 1-step disagreement is less severe than a 3-step disagreement), deciding how to handle missing/uncertain labels, and reporting category-specific agreement to diagnose where disagreements cluster. In medtech evidence packages, these metrics support claims about label quality, reader training, or algorithm consistency, and they are often paired with confidence intervals and a clear mapping of categories to clinical actions.\n\nExample: Example (medtech/healthcare): An AI tool flags chest X-rays as ‘normal’, ‘possible pneumonia’, or ‘pneumonia’. Two radiologists label 500 images, and the model outputs the same three-category label. You compute Cohen’s kappa (weighted kappa if ordinal) to quantify agreement among the two readers (and optionally between each reader and the model). Because categories are ordinal, you use a weighted version so that confusing ‘possible pneumonia’ with ‘pneumonia’ is penalized less than confusing ‘normal’ with ‘pneumonia’. You report the overall coefficient with a confidence interval and a confusion matrix to show where disagreements occur. The analysis informs whether additional reader training is needed or whether the model should be evaluated with adjudicated consensus labels before clinical deployment.",
  "Continuous GOF tests (AD/KS)": "General explanation: Continuous GOF tests (AD/KS) is typically used to assess whether a sample behaves like it came from a target distribution, most commonly to check normality before applying parametric methods. In healthcare studies, distribution checks matter because endpoints like biomarker concentrations, length of stay, and device error often have skewness or heavy tails. Normality or goodness-of-fit tests provide an objective signal, but they are sensitive to sample size: with large clinical datasets they can flag tiny, clinically irrelevant deviations, while with small pilot studies they may have low power. Practical options include choosing a test appropriate for the sample size and the type of deviation you care about (tail behavior vs central fit), using Q-Q plots alongside p-values, and considering transformations (log, Box-Cox) or robust/nonparametric alternatives when normality is implausible. The result should be interpreted in the context of the downstream method’s robustness, not as a standalone gatekeeper.\n\nExample: Example (medtech/healthcare): A lab team compares two assay protocols and plans to use a pooled-variance t-test on log-transformed concentrations. Before finalizing, they check whether the transformed concentrations are approximately normal within each protocol group. They run Continuous GOF tests (AD/KS) and inspect Q-Q plots. The test suggests mild tail deviations in a small subgroup of hemolyzed samples. Rather than deleting data, they document a sensitivity analysis: a robust comparison (e.g., Welch or nonparametric) and a bootstrap CI for the mean difference. If conclusions agree across approaches, they proceed with the pre-specified parametric analysis and note that the normality check did not materially change interpretation.",
  "Cook’s distance / leverage": "General explanation: Cook’s distance / leverage is used to detect or manage unusual observations that may reflect data quality issues, rare clinical states, or true but extreme patient responses. In medtech pipelines, outliers can come from sensor dropouts, mislabeled timestamps, or rare physiologic events. Classical outlier tests like Grubbs or generalized ESD assume approximate normality and are designed for single or multiple extreme values, while influence diagnostics like Cook’s distance assess how much a point shifts regression estimates. Options include deciding whether you are screening for one outlier or many, setting a pre-specified alpha to control false flagging, and defining what happens after detection: exclusion, winsorization, robust modeling, or root-cause investigation. In regulated analyses, the safest posture is transparency: report analyses with and without flagged points and justify any exclusions with clinical or technical rationale.\n\nExample: Example (medtech/healthcare): A home spirometry device reports forced expiratory volume (FEV1) daily for COPD patients. A few days show implausibly high readings after firmware updates, likely due to calibration reset. You apply Cook’s distance / leverage as part of a pre-specified data quality pipeline to flag candidate outliers, then investigate their root cause using device logs and patient notes. Rather than automatically deleting points, you label them as ‘suspect’, rerun the primary analysis with and without them, and report both results. If the inference changes, you document why and whether the affected data should be excluded per protocol. This approach balances statistical detection with clinical and engineering plausibility.",
  "Cox proportional hazards": "General explanation: Cox proportional hazards is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Cox proportional hazards to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Cox proportional hazards model": "General explanation: Cox proportional hazards model is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Cox proportional hazards model to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "DeLong test for AUC differences": "General explanation: DeLong test for AUC differences is used when you want to compare the discriminative performance of two diagnostic or triage models that were evaluated on the same patients, so their ROC curves (and AUCs) are statistically correlated. In medtech, this comes up when you update a risk score inside a device or clinical decision support tool and want to show that the new version truly improves (or at least does not worsen) AUC. The core idea is to estimate the variance of the difference between AUCs while accounting for within-patient pairing, rather than pretending the two AUCs came from independent samples. Practical options include deciding whether you are doing a two-sided test or a one-sided non-inferiority style comparison, and whether you also report confidence intervals for the AUC difference. A key limitation is that AUC alone can hide clinically important trade-offs, so teams often pair this with threshold-based metrics (sensitivity, specificity) and calibration checks.\n\nExample: Example (medtech/healthcare): A company updates its emergency-department sepsis risk model used in a clinical decision support module. Both the old and new models are run on the same retrospective cohort of 3,000 ED encounters with adjudicated sepsis outcomes. Because the predictions are paired within each patient encounter, the AUCs are correlated. The team uses the DeLong test for AUC differences to test whether the new AUC is higher than the old AUC. They report the estimated AUC difference with a confidence interval and confirm that the improvement persists in key subgroups (older adults, renal impairment). If the test is not significant, they may still proceed if the new model improves calibration or reduces false alarms at the operational threshold, documenting that the AUC change is small and within uncertainty.",
  "Diebold–Mariano test": "General explanation: Diebold–Mariano test is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Diebold–Mariano test within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Distance correlation (often via permutation)": "General explanation: Distance correlation (often via permutation) relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Distance correlation (often via permutation) to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Dunnett test": "General explanation: Dunnett test is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Dunnett test, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Durbin–Watson": "General explanation: Durbin–Watson is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Durbin–Watson within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Egger regression test": "General explanation: Egger regression test is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Egger regression test (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "Exact binomial test": "General explanation: Exact binomial test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Exact binomial test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Exact multinomial tests (small n)": "General explanation: Exact multinomial tests (small n) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Exact multinomial tests (small n) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Exact unconditional tests": "General explanation: Exact unconditional tests applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Exact unconditional tests to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Fisher exact (small counts)": "General explanation: Fisher exact (small counts) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Fisher exact (small counts) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Fisher exact test": "General explanation: Fisher exact test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Fisher exact test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Fisher–Freeman–Halton exact (RxC small n)": "General explanation: Fisher–Freeman–Halton exact (RxC small n) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Fisher–Freeman–Halton exact (RxC small n) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Fleiss’ kappa": "General explanation: Fleiss’ kappa is used to quantify agreement between human or algorithmic raters when the outcome is categorical (for example, radiology categories, ECG rhythm labels, or adverse event severity grades). Raw percent agreement can be misleading when one category is very common, because raters can “agree” by chance. Chance-corrected coefficients like kappa and related reliability statistics adjust for expected agreement under independent rating. Options usually include choosing unweighted vs weighted versions (weighted is common for ordinal clinical scales where a 1-step disagreement is less severe than a 3-step disagreement), deciding how to handle missing/uncertain labels, and reporting category-specific agreement to diagnose where disagreements cluster. In medtech evidence packages, these metrics support claims about label quality, reader training, or algorithm consistency, and they are often paired with confidence intervals and a clear mapping of categories to clinical actions.\n\nExample: Example (medtech/healthcare): An AI tool flags chest X-rays as ‘normal’, ‘possible pneumonia’, or ‘pneumonia’. Two radiologists label 500 images, and the model outputs the same three-category label. You compute Fleiss’ kappa to quantify agreement among the two readers (and optionally between each reader and the model). Because categories are ordinal, you use a weighted version so that confusing ‘possible pneumonia’ with ‘pneumonia’ is penalized less than confusing ‘normal’ with ‘pneumonia’. You report the overall coefficient with a confidence interval and a confusion matrix to show where disagreements occur. The analysis informs whether additional reader training is needed or whether the model should be evaluated with adjudicated consensus labels before clinical deployment.",
  "Friedman test": "General explanation: Friedman test is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Friedman test to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "Funnel plot asymmetry tests": "General explanation: Funnel plot asymmetry tests is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Funnel plot asymmetry tests (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "Games–Howell": "General explanation: Games–Howell is a post-hoc multiple-comparison procedure used after an omnibus ANOVA-type analysis indicates that not all group means are equal. In a medtech context, imagine you tested four sensor firmware versions and saw an overall difference in mean absolute error; a post-hoc method tells you which pairs differ while controlling the familywise error rate. Tukey HSD is typically used when variances are roughly equal and sample sizes are balanced, while Games–Howell is a popular choice when variances or sample sizes differ. Options include which pairs to compare (all pairs vs a subset), whether to use adjusted confidence intervals, and whether you pre-specify clinically meaningful contrasts (e.g., new device vs current standard). These procedures help avoid overclaiming improvements when many comparisons are possible, which is especially important in regulated evidence where multiplicity control is scrutinized.\n\nExample: Example (medtech/healthcare): A hospital tests four alarm-threshold settings for a pulse oximeter to reduce nuisance alarms while maintaining safety. An ANOVA on alarm counts per patient-day suggests differences among thresholds. To find which settings differ, you apply Games–Howell as a post-hoc procedure. You obtain adjusted confidence intervals for each pair of thresholds and identify that threshold C significantly reduces alarms compared with A and B, while C and D are not meaningfully different. You pair the statistical result with a clinical safety review of missed desaturation events, ensuring the chosen threshold is not just statistically different but clinically acceptable.",
  "Generalized ESD (Rosner)": "General explanation: Generalized ESD (Rosner) is used to detect or manage unusual observations that may reflect data quality issues, rare clinical states, or true but extreme patient responses. In medtech pipelines, outliers can come from sensor dropouts, mislabeled timestamps, or rare physiologic events. Classical outlier tests like Grubbs or generalized ESD assume approximate normality and are designed for single or multiple extreme values, while influence diagnostics like Cook’s distance assess how much a point shifts regression estimates. Options include deciding whether you are screening for one outlier or many, setting a pre-specified alpha to control false flagging, and defining what happens after detection: exclusion, winsorization, robust modeling, or root-cause investigation. In regulated analyses, the safest posture is transparency: report analyses with and without flagged points and justify any exclusions with clinical or technical rationale.\n\nExample: Example (medtech/healthcare): A home spirometry device reports forced expiratory volume (FEV1) daily for COPD patients. A few days show implausibly high readings after firmware updates, likely due to calibration reset. You apply Generalized ESD (Rosner) as part of a pre-specified data quality pipeline to flag candidate outliers, then investigate their root cause using device logs and patient notes. Rather than automatically deleting points, you label them as ‘suspect’, rerun the primary analysis with and without them, and report both results. If the inference changes, you document why and whether the affected data should be excluded per protocol. This approach balances statistical detection with clinical and engineering plausibility.",
  "Generalized Estimating Equations (GEE)": "General explanation: Generalized Estimating Equations (GEE) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Generalized Estimating Equations (GEE) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Generalized additive models (GAM)": "General explanation: Generalized additive models (GAM) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Generalized additive models (GAM) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Generalized linear mixed model (GLMM)": "General explanation: Generalized linear mixed model (GLMM) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Generalized linear mixed model (GLMM) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Granger causality tests": "General explanation: Granger causality tests is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use Granger causality tests within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Grubbs (single outlier, normal)": "General explanation: Grubbs (single outlier, normal) is used to detect or manage unusual observations that may reflect data quality issues, rare clinical states, or true but extreme patient responses. In medtech pipelines, outliers can come from sensor dropouts, mislabeled timestamps, or rare physiologic events. Classical outlier tests like Grubbs or generalized ESD assume approximate normality and are designed for single or multiple extreme values, while influence diagnostics like Cook’s distance assess how much a point shifts regression estimates. Options include deciding whether you are screening for one outlier or many, setting a pre-specified alpha to control false flagging, and defining what happens after detection: exclusion, winsorization, robust modeling, or root-cause investigation. In regulated analyses, the safest posture is transparency: report analyses with and without flagged points and justify any exclusions with clinical or technical rationale.\n\nExample: Example (medtech/healthcare): A home spirometry device reports forced expiratory volume (FEV1) daily for COPD patients. A few days show implausibly high readings after firmware updates, likely due to calibration reset. You apply Grubbs (single outlier, normal) as part of a pre-specified data quality pipeline to flag candidate outliers, then investigate their root cause using device logs and patient notes. Rather than automatically deleting points, you label them as ‘suspect’, rerun the primary analysis with and without them, and report both results. If the inference changes, you document why and whether the affected data should be excluded per protocol. This approach balances statistical detection with clinical and engineering plausibility.",
  "Holm/Hochberg familywise control": "General explanation: Holm/Hochberg familywise control is a multiplicity-control procedure used when you run many hypothesis tests—common in medtech when evaluating multiple endpoints, multiple sensor channels, many subgroups, or large biomarker panels. Without correction, the chance of at least one false positive can become large. Familywise error rate methods (like Holm/Hochberg) aim to keep the probability of any false discovery below a target, which is conservative but attractive for high-stakes claims. False discovery rate (FDR) control (like Benjamini–Hochberg) instead controls the expected proportion of false positives among the discoveries, which can improve power in exploratory settings. Options include the error rate you want to control (FWER vs FDR), whether tests are independent or positively dependent (which affects guarantees), and whether you pre-specify a hierarchy of endpoints. In healthcare documentation, it is important to align the correction choice with the claim type: confirmatory vs exploratory.\n\nExample: Example (medtech/healthcare): A multi-sensor study evaluates 25 candidate digital biomarkers for predicting exacerbations in asthma. To avoid declaring success due to chance, the team applies Holm/Hochberg familywise control across the 25 p-values. They predefine that only biomarkers passing the adjusted threshold will be treated as ‘confirmed’ and moved to validation, while others remain exploratory. They also report adjusted q-values and the expected number of false discoveries, so clinicians understand the trade-off between sensitivity to true signals and protection from false positives. This supports disciplined feature selection before building a regulated model that would influence patient care. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Hotelling’s T² (one-sample)": "General explanation: Hotelling’s T² (one-sample) is a multivariate extension of mean comparison, used when you have multiple correlated continuous outcomes and you want a single joint test. In healthcare, this arises when a device affects several physiologic measurements simultaneously (e.g., systolic BP, diastolic BP, and heart rate) or when an imaging algorithm produces a vector of quantitative features. Hotelling’s T-squared is the classic two-group multivariate analog of the t-test, while MANOVA extends ANOVA to multiple outcomes across multiple groups. Options include which multivariate test statistic you report (Wilks’ lambda, Pillai’s trace, etc.), how you address covariance assumptions, and how you follow up a significant omnibus result with clinically interpretable univariate comparisons while controlling multiplicity. Multivariate tests can be more powerful than separate tests when outcomes are correlated, but they can be unstable if the sample size is small relative to the number of outcomes.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Hotelling’s T² (one-sample), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Hotelling’s T² (two-sample)": "General explanation: Hotelling’s T² (two-sample) is a multivariate extension of mean comparison, used when you have multiple correlated continuous outcomes and you want a single joint test. In healthcare, this arises when a device affects several physiologic measurements simultaneously (e.g., systolic BP, diastolic BP, and heart rate) or when an imaging algorithm produces a vector of quantitative features. Hotelling’s T-squared is the classic two-group multivariate analog of the t-test, while MANOVA extends ANOVA to multiple outcomes across multiple groups. Options include which multivariate test statistic you report (Wilks’ lambda, Pillai’s trace, etc.), how you address covariance assumptions, and how you follow up a significant omnibus result with clinically interpretable univariate comparisons while controlling multiplicity. Multivariate tests can be more powerful than separate tests when outcomes are correlated, but they can be unstable if the sample size is small relative to the number of outcomes.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Hotelling’s T² (two-sample), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Intraclass correlation coefficient (ICC)": "General explanation: Intraclass correlation coefficient (ICC) measures reliability for continuous outcomes when multiple raters or repeated measurements are involved—common in imaging biomarkers, hemodynamic measurements, or lab quantification where multiple operators or instruments are used. Unlike simple correlation, ICC partitions variance into between-subject and within-subject components to capture whether subjects can be distinguished consistently despite measurement noise. A major “option” is choosing the ICC model that matches your design: one-way vs two-way, random vs fixed raters, and whether you care about absolute agreement or consistency. These choices change interpretation and the numerical value, so medtech reports should state them explicitly. ICC is often paired with repeatability or reproducibility analyses, and with plots (e.g., Bland–Altman) that show the size of disagreements in clinically meaningful units.\n\nExample: Example (medtech/healthcare): Three sonographers measure left-ventricular ejection fraction (LVEF) on the same 40 echo studies using a semi-automated tool. You compute Intraclass correlation coefficient (ICC) under a two-way random-effects, absolute-agreement model because you want to generalize to other qualified sonographers and you care about numeric agreement, not just ranking. An ICC of 0.92 suggests high reliability, but you also look at the within-subject standard deviation to translate this into expected absolute error in percentage points. You then test whether reliability differs between low-quality and high-quality images, which helps decide if the tool needs additional image-quality checks. This supports a medtech claim about measurement reproducibility across operators and sites.",
  "Inverse-variance fixed-effect meta-analysis": "General explanation: Inverse-variance fixed-effect meta-analysis is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Inverse-variance fixed-effect meta-analysis (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "Inverse-variance pooling on log scale": "General explanation: Inverse-variance pooling on log scale is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Inverse-variance pooling on log scale (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "I²": "General explanation: I² is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use I² (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "KPSS": "General explanation: KPSS is used when observations are ordered in time and may be autocorrelated—common in medtech for continuous monitoring signals (ECG, SpO2, glucose), hospital operations time series, or post-market surveillance counts. Time-series tests help answer questions like: is the series stationary, does a model’s residuals look like white noise, did a structural change occur after a software update, or does one signal help predict another. Options include selecting lag lengths, deciding whether to include trends or seasonality, and choosing robust variants when data are irregular or missing. In regulated settings, time-series diagnostics matter because autocorrelation can inflate false positives if you treat correlated measurements as independent. A sound workflow pairs formal tests with plots (autocorrelation function, residual diagnostics) and with sensitivity analyses that confirm conclusions are not artifacts of arbitrary lag choices.\n\nExample: Example (medtech/healthcare): A wearable ECG patch produces a daily count of atrial fibrillation (AF) episodes per user. The analytics team wants to evaluate whether an algorithm update in April introduced a step-change in detected episode counts. They use KPSS within a time-series workflow: first checking stationarity, then fitting a model with seasonality (weekday effects), and finally testing for structural change. They also examine residual autocorrelation; if residuals are not white noise, they refine the model so standard errors are valid. The conclusion is documented as part of software change control, showing whether observed shifts are true clinical signal or artifacts of the update.",
  "Kendall": "General explanation: Kendall assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Kendall to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Kendall’s tau": "General explanation: Kendall’s tau assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Kendall’s tau to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Kolmogorov–Smirnov (with estimated params caution)": "General explanation: Kolmogorov–Smirnov (with estimated params caution) is typically used to assess whether a sample behaves like it came from a target distribution, most commonly to check normality before applying parametric methods. In healthcare studies, distribution checks matter because endpoints like biomarker concentrations, length of stay, and device error often have skewness or heavy tails. Normality or goodness-of-fit tests provide an objective signal, but they are sensitive to sample size: with large clinical datasets they can flag tiny, clinically irrelevant deviations, while with small pilot studies they may have low power. Practical options include choosing a test appropriate for the sample size and the type of deviation you care about (tail behavior vs central fit), using Q-Q plots alongside p-values, and considering transformations (log, Box-Cox) or robust/nonparametric alternatives when normality is implausible. The result should be interpreted in the context of the downstream method’s robustness, not as a standalone gatekeeper.\n\nExample: Example (medtech/healthcare): A lab team compares two assay protocols and plans to use a pooled-variance t-test on log-transformed concentrations. Before finalizing, they check whether the transformed concentrations are approximately normal within each protocol group. They run Kolmogorov–Smirnov (with estimated params caution) and inspect Q-Q plots. The test suggests mild tail deviations in a small subgroup of hemolyzed samples. Rather than deleting data, they document a sensitivity analysis: a robust comparison (e.g., Welch or nonparametric) and a bootstrap CI for the mean difference. If conclusions agree across approaches, they proceed with the pre-specified parametric analysis and note that the normality check did not materially change interpretation.",
  "Krippendorff’s alpha": "General explanation: Krippendorff’s alpha is used to quantify agreement between human or algorithmic raters when the outcome is categorical (for example, radiology categories, ECG rhythm labels, or adverse event severity grades). Raw percent agreement can be misleading when one category is very common, because raters can “agree” by chance. Chance-corrected coefficients like kappa and related reliability statistics adjust for expected agreement under independent rating. Options usually include choosing unweighted vs weighted versions (weighted is common for ordinal clinical scales where a 1-step disagreement is less severe than a 3-step disagreement), deciding how to handle missing/uncertain labels, and reporting category-specific agreement to diagnose where disagreements cluster. In medtech evidence packages, these metrics support claims about label quality, reader training, or algorithm consistency, and they are often paired with confidence intervals and a clear mapping of categories to clinical actions.\n\nExample: Example (medtech/healthcare): An AI tool flags chest X-rays as ‘normal’, ‘possible pneumonia’, or ‘pneumonia’. Two radiologists label 500 images, and the model outputs the same three-category label. You compute Krippendorff’s alpha to quantify agreement among the two readers (and optionally between each reader and the model). Because categories are ordinal, you use a weighted version so that confusing ‘possible pneumonia’ with ‘pneumonia’ is penalized less than confusing ‘normal’ with ‘pneumonia’. You report the overall coefficient with a confidence interval and a confusion matrix to show where disagreements occur. The analysis informs whether additional reader training is needed or whether the model should be evaluated with adjudicated consensus labels before clinical deployment.",
  "Kruskal–Wallis": "General explanation: Kruskal–Wallis is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Kruskal–Wallis to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "Kruskal–Wallis test": "General explanation: Kruskal–Wallis test is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Kruskal–Wallis test to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "LMM/GLMM": "General explanation: LMM/GLMM belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits LMM/GLMM (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Levene / Brown–Forsythe": "General explanation: Levene / Brown–Forsythe is used to evaluate whether different groups have similar variability, which is an important assumption for several classical parametric comparisons. In medtech, variance differences are common: a glucose sensor might be much noisier in hypoglycemia than euglycemia, or measurement error may increase with body motion. Variance-homogeneity tests help you decide whether a pooled-variance t-test/ANOVA is appropriate or whether you should switch to Welch-type methods or model heteroskedasticity explicitly. Options include which variance test to use: Bartlett is powerful under normality but can be overly sensitive when data are skewed; Levene and Brown–Forsythe are more robust because they operate on absolute deviations from a center (mean or median). In regulated analyses, the key is not merely to report the p-value, but to show how the decision affected your primary inference and to document a pre-specified fallback analysis.\n\nExample: Example (medtech/healthcare): You compare measurement error of a glucose sensor across three temperature ranges: cold, room temperature, and warm. Before running a classical ANOVA on mean error, you test whether the error variance is similar across temperature groups using Levene / Brown–Forsythe. The test indicates heteroskedasticity, with higher variance in the warm condition (likely due to sweat and motion artifacts). You then use Welch ANOVA and a heteroskedasticity-robust regression as sensitivity analyses. In your validation report, you explain that unequal variances were expected physiologically and that the conclusion about mean error differences is robust to the variance structure. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Likelihood ratio test (LRT)": "General explanation: Likelihood ratio test (LRT) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Likelihood ratio test (LRT) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Likelihood ratio test for nested models": "General explanation: Likelihood ratio test for nested models belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Likelihood ratio test for nested models (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Linear hypothesis tests with multiplicity control": "General explanation: Linear hypothesis tests with multiplicity control is a multiplicity-control procedure used when you run many hypothesis tests—common in medtech when evaluating multiple endpoints, multiple sensor channels, many subgroups, or large biomarker panels. Without correction, the chance of at least one false positive can become large. Familywise error rate methods (like Holm/Hochberg) aim to keep the probability of any false discovery below a target, which is conservative but attractive for high-stakes claims. False discovery rate (FDR) control (like Benjamini–Hochberg) instead controls the expected proportion of false positives among the discoveries, which can improve power in exploratory settings. Options include the error rate you want to control (FWER vs FDR), whether tests are independent or positively dependent (which affects guarantees), and whether you pre-specify a hierarchy of endpoints. In healthcare documentation, it is important to align the correction choice with the claim type: confirmatory vs exploratory.\n\nExample: Example (medtech/healthcare): A multi-sensor study evaluates 25 candidate digital biomarkers for predicting exacerbations in asthma. To avoid declaring success due to chance, the team applies Linear hypothesis tests with multiplicity control across the 25 p-values. They predefine that only biomarkers passing the adjusted threshold will be treated as ‘confirmed’ and moved to validation, while others remain exploratory. They also report adjusted q-values and the expected number of false discoveries, so clinicians understand the trade-off between sensitivity to true signals and protection from false positives. This supports disciplined feature selection before building a regulated model that would influence patient care. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Linear mixed-effects model (LMM)": "General explanation: Linear mixed-effects model (LMM) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Linear mixed-effects model (LMM) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Linear regression (OLS)": "General explanation: Linear regression (OLS) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Linear regression (OLS) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Log-rank test": "General explanation: Log-rank test is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Log-rank test to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Logistic regression": "General explanation: Logistic regression belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Logistic regression (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "MANOVA": "General explanation: MANOVA is a multivariate extension of mean comparison, used when you have multiple correlated continuous outcomes and you want a single joint test. In healthcare, this arises when a device affects several physiologic measurements simultaneously (e.g., systolic BP, diastolic BP, and heart rate) or when an imaging algorithm produces a vector of quantitative features. Hotelling’s T-squared is the classic two-group multivariate analog of the t-test, while MANOVA extends ANOVA to multiple outcomes across multiple groups. Options include which multivariate test statistic you report (Wilks’ lambda, Pillai’s trace, etc.), how you address covariance assumptions, and how you follow up a significant omnibus result with clinically interpretable univariate comparisons while controlling multiplicity. Multivariate tests can be more powerful than separate tests when outcomes are correlated, but they can be unstable if the sample size is small relative to the number of outcomes.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply MANOVA to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "MCMC/VI posterior inference": "General explanation: MCMC/VI posterior inference represents Bayesian evidence quantification or inference. In medtech, Bayesian methods are often used when prior information exists (earlier device versions, published studies) or when sample sizes are limited and you want probabilistic statements that align with decision-making. Bayes factors compare how well competing hypotheses predict the observed data, providing a graded measure of evidence rather than a binary p-value threshold. Posterior inference methods such as MCMC (Markov chain Monte Carlo) or variational inference (VI) compute distributions over parameters—useful for uncertainty quantification in risk models or calibration curves. Key options include specifying priors (weakly informative vs informative), choosing computational settings (chains, warmup, convergence diagnostics), and defining decision thresholds that make sense clinically. Bayesian outputs should be communicated in clinically interpretable terms (e.g., probability a sensitivity improvement exceeds a minimum clinically important difference).\n\nExample: Example (medtech/healthcare): A startup has prior data from an earlier glucose-sensor version and runs a small confirmatory study for a new coating. They use MCMC/VI posterior inference to quantify evidence that mean absolute relative difference improved by at least 1 percentage point. A weakly informative prior encodes plausible improvement ranges based on historical studies, and posterior inference yields the probability that the improvement exceeds the clinically meaningful threshold. They report posterior intervals, run convergence diagnostics if using MCMC, and perform sensitivity analyses with different priors. The Bayesian summary helps internal decision-making (ship vs iterate) while still translating results into familiar metrics for external stakeholders.",
  "Mann–Whitney U / Wilcoxon rank-sum": "General explanation: The Mann–Whitney U test (also called the Wilcoxon rank-sum test) is a nonparametric method for comparing two independent groups when the outcome is ordinal or continuous and you don’t want to rely on normality assumptions. Instead of comparing means, it ranks all observations from both groups together and asks whether values from one group tend to receive systematically higher (or lower) ranks than the other. It’s often described as testing for a difference in central tendency, and it can be interpreted as a median comparison only when the two group distributions have a similar shape/spread. Many analysts also report an effect size interpretation like the probability that a randomly chosen patient/device measurement from Group A exceeds one from Group B (a stochastic superiority / probability-of-superiority view). The test assumes independent observations; if data are paired (pre/post in the same patient), you’d use a paired alternative. Importantly, despite the rank idea, Mann–Whitney is not appropriate for censored time-to-event outcomes; censoring requires survival methods (e.g., log-rank, Cox) because standard rank-sum cannot handle censored observations.\n\nExample: Example (medtech/healthcare): A medtech team compares two independent cohorts of post-op patients who receive Catheter A vs Catheter B and records pain score (0–10) one hour after insertion. The pain scores are clearly non-normal (many 0–2s, with a long tail of higher scores), and clinicians care about whether one catheter generally produces lower patient discomfort, not necessarily about mean differences. The team uses the Mann–Whitney U test to compare the distributions. They report the median and IQR for each catheter and add an effect-size interpretation: for example, there is a ~65% probability that a randomly selected patient from Catheter B reports a lower pain score than a randomly selected patient from Catheter A, which is often more intuitive for clinical stakeholders than a rank statistic. They also verify design validity: patients appear only once (independent), and the groups were enrolled under the same protocol. If they needed covariate adjustment (age, procedure type), they would switch to an appropriate regression model rather than trying to adjust Mann–Whitney directly.",
  "Mantel–Haenszel fixed-effect pooling": "General explanation: Mantel–Haenszel fixed-effect pooling is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Mantel–Haenszel fixed-effect pooling (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "McNemar (2x2)": "General explanation: McNemar (2x2) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply McNemar (2x2) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "McNemar test": "General explanation: McNemar test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply McNemar test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Mixed-effects model (LMM)": "General explanation: Mixed-effects model (LMM) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Mixed-effects model (LMM) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Multinomial logistic regression": "General explanation: Multinomial logistic regression applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Multinomial logistic regression to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Multiple regression": "General explanation: Multiple regression belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Multiple regression (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Mutual information tests (resampling)": "General explanation: Mutual information tests (resampling) assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Mutual information tests (resampling) to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Negative binomial regression": "General explanation: Negative binomial regression is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Negative binomial regression with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "One-sample permutation/randomization test": "General explanation: One-sample permutation/randomization test relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses One-sample permutation/randomization test to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "One-sample proportion z-test (large n)": "General explanation: One-sample proportion z-test (large n) is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using One-sample proportion z-test (large n), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "One-sample t-test (σ unknown)": "General explanation: One-sample t-test (σ unknown) is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using One-sample t-test (σ unknown), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. A practical tip is to pair the test with an effect size and a confidence interval so the result is interpretable for clinicians and product stakeholders, not only statistically significant.",
  "One-sample z-test (σ known)": "General explanation: One-sample z-test (σ known) is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using One-sample z-test (σ known), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. A practical tip is to pair the test with an effect size and a confidence interval so the result is interpretable for clinicians and product stakeholders, not only statistically significant.",
  "One-way ANOVA": "General explanation: One-way ANOVA compares mean outcomes across three or more groups (or conditions), which is common in medtech when evaluating multiple device configurations, dose levels, or clinical sites. The central idea is to partition variability into between-group and within-group components and test whether group means differ more than expected by random variation. Options include choosing classical ANOVA (assuming equal variances) versus Welch ANOVA (allowing unequal variances), deciding whether the design is one-way, two-way, or repeated measures, and specifying post-hoc comparisons (e.g., Tukey HSD or Games–Howell) if the omnibus test is significant. In healthcare reporting, it is important to pair ANOVA results with effect sizes (mean differences) and confidence intervals, because clinical relevance depends on magnitude. Also consider whether patient-level clustering or repeated measurements require mixed models rather than simple ANOVA.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply One-way ANOVA to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Ordinal logistic (proportional odds)": "General explanation: Ordinal logistic (proportional odds) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Ordinal logistic (proportional odds) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Ordinal probit": "General explanation: Ordinal probit belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Ordinal probit (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "PERMANOVA (permutation MANOVA)": "General explanation: PERMANOVA (permutation MANOVA) relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses PERMANOVA (permutation MANOVA) to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Paired permutation/randomization test": "General explanation: Paired permutation/randomization test relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Paired permutation/randomization test to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Paired t-test": "General explanation: Paired t-test is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Paired t-test, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. A practical tip is to pair the test with an effect size and a confidence interval so the result is interpretable for clinicians and product stakeholders, not only statistically significant.",
  "Parametric PH models (Weibull, log-logistic, etc.)": "General explanation: Parametric PH models (Weibull, log-logistic, etc.) is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Parametric PH models (Weibull, log-logistic, etc.) to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Pearson chi-square test": "General explanation: Pearson chi-square test assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Pearson chi-square test to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Pearson correlation": "General explanation: Pearson correlation assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Pearson correlation to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Permutation ANOVA (randomization)": "General explanation: Permutation ANOVA (randomization) relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Permutation ANOVA (randomization) to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Permutation test for difference in means/medians": "General explanation: Permutation test for difference in means/medians relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Permutation test for difference in means/medians to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Permutation tests": "General explanation: Permutation tests relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Permutation tests to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Point-biserial correlation": "General explanation: Point-biserial correlation assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Point-biserial correlation to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Poisson rate test / exact rate ratio test": "General explanation: Poisson rate test / exact rate ratio test applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Poisson rate test / exact rate ratio test to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Poisson regression (with offset if needed)": "General explanation: Poisson regression (with offset if needed) is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Poisson regression (with offset if needed) with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "Poisson regression with offset": "General explanation: Poisson regression with offset is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Poisson regression with offset with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "Posterior model probabilities": "General explanation: Posterior model probabilities represents Bayesian evidence quantification or inference. In medtech, Bayesian methods are often used when prior information exists (earlier device versions, published studies) or when sample sizes are limited and you want probabilistic statements that align with decision-making. Bayes factors compare how well competing hypotheses predict the observed data, providing a graded measure of evidence rather than a binary p-value threshold. Posterior inference methods such as MCMC (Markov chain Monte Carlo) or variational inference (VI) compute distributions over parameters—useful for uncertainty quantification in risk models or calibration curves. Key options include specifying priors (weakly informative vs informative), choosing computational settings (chains, warmup, convergence diagnostics), and defining decision thresholds that make sense clinically. Bayesian outputs should be communicated in clinically interpretable terms (e.g., probability a sensitivity improvement exceeds a minimum clinically important difference).\n\nExample: Example (medtech/healthcare): A startup has prior data from an earlier glucose-sensor version and runs a small confirmatory study for a new coating. They use Posterior model probabilities to quantify evidence that mean absolute relative difference improved by at least 1 percentage point. A weakly informative prior encodes plausible improvement ranges based on historical studies, and posterior inference yields the probability that the improvement exceeds the clinically meaningful threshold. They report posterior intervals, run convergence diagnostics if using MCMC, and perform sensitivity analyses with different priors. The Bayesian summary helps internal decision-making (ship vs iterate) while still translating results into familiar metrics for external stakeholders.",
  "Posterior predictive checks": "General explanation: Posterior predictive checks represents Bayesian evidence quantification or inference. In medtech, Bayesian methods are often used when prior information exists (earlier device versions, published studies) or when sample sizes are limited and you want probabilistic statements that align with decision-making. Bayes factors compare how well competing hypotheses predict the observed data, providing a graded measure of evidence rather than a binary p-value threshold. Posterior inference methods such as MCMC (Markov chain Monte Carlo) or variational inference (VI) compute distributions over parameters—useful for uncertainty quantification in risk models or calibration curves. Key options include specifying priors (weakly informative vs informative), choosing computational settings (chains, warmup, convergence diagnostics), and defining decision thresholds that make sense clinically. Bayesian outputs should be communicated in clinically interpretable terms (e.g., probability a sensitivity improvement exceeds a minimum clinically important difference).\n\nExample: Example (medtech/healthcare): A startup has prior data from an earlier glucose-sensor version and runs a small confirmatory study for a new coating. They use Posterior predictive checks to quantify evidence that mean absolute relative difference improved by at least 1 percentage point. A weakly informative prior encodes plausible improvement ranges based on historical studies, and posterior inference yields the probability that the improvement exceeds the clinically meaningful threshold. They report posterior intervals, run convergence diagnostics if using MCMC, and perform sensitivity analyses with different priors. The Bayesian summary helps internal decision-making (ship vs iterate) while still translating results into familiar metrics for external stakeholders.",
  "Probit regression": "General explanation: Probit regression belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Probit regression (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Quantile regression": "General explanation: Quantile regression belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Quantile regression (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Quasi-Poisson": "General explanation: Quasi-Poisson is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Quasi-Poisson with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "Random-effects meta-analysis (e.g., DerSimonian–Laird, REML)": "General explanation: Random-effects meta-analysis (e.g., DerSimonian–Laird, REML) is used when synthesizing evidence across studies, which is common in healthcare systematic reviews and in medtech literature benchmarking. Meta-analysis combines effect estimates using weights (often inverse-variance) to produce a pooled estimate and quantify uncertainty. Options include fixed-effect models (assuming one true effect) versus random-effects models (allowing heterogeneity across studies), and choosing an effect scale such as log risk ratio, log odds ratio, or standardized mean difference. Diagnostics include heterogeneity measures like I-squared and tests like Cochran’s Q, while bias checks include funnel plots and regression-based asymmetry tests (e.g., Egger). In medtech, meta-analytic methods also appear when aggregating performance metrics across sites or when pooling real-world evidence sources; careful attention to heterogeneity and study quality is essential to avoid misleading pooled conclusions.\n\nExample: Example (medtech/healthcare): A team conducting a systematic review of wearable AF detection aggregates sensitivity and specificity across 12 studies. They use Random-effects meta-analysis (e.g., DerSimonian–Laird, REML) (as appropriate) to pool effect estimates and to quantify heterogeneity across study designs and populations. If heterogeneity is high, they explore subgroup differences (e.g., inpatient vs ambulatory) and consider random-effects models. They inspect funnel plots and run asymmetry tests to assess publication bias, noting that small negative studies may be underreported. The final write-up emphasizes that pooled performance depends on population spectrum and study quality, which is critical when translating literature evidence to product claims.",
  "Randomization tests": "General explanation: Randomization tests relies on resampling (bootstrap, permutation, or randomization) to quantify uncertainty or to compute p-values without heavy parametric assumptions. In healthcare, resampling is valuable when sample sizes are modest, when analytic formulas are complex (e.g., AUC differences, cluster designs), or when the outcome distribution is skewed. Bootstrap confidence intervals repeatedly resample patients (or clusters) to approximate the sampling distribution, while permutation/randomization tests shuffle labels under the null to obtain an exact or near-exact reference distribution. Key options include the resampling unit (patient vs visit vs site), the number of resamples, and the type of bootstrap interval (percentile, BCa, studentized). For medtech, a crucial detail is preserving the study design during resampling—for example, keeping paired measurements together or resampling clusters rather than individual observations—to avoid overstating precision.\n\nExample: Example (medtech/healthcare): A small prospective study evaluates whether a new AI triage tool reduces clinician response time. Because only 35 cases are available and response times are skewed, the team uses Randomization tests to quantify uncertainty without relying on normality. They bootstrap at the patient level (keeping repeated measurements together) to generate a confidence interval for the median difference, and they run a permutation test by shuffling the ‘AI on/off’ label within matched shifts. They predefine 10,000 resamples, verify stability of the interval, and report both the resampling-based p-value and the effect size. This makes the evidence more defensible when classic asymptotic approximations would be fragile.",
  "Repeated-measures ANOVA (with GG/HF correction if needed)": "General explanation: Repeated-measures ANOVA (with GG/HF correction if needed) compares mean outcomes across three or more groups (or conditions), which is common in medtech when evaluating multiple device configurations, dose levels, or clinical sites. The central idea is to partition variability into between-group and within-group components and test whether group means differ more than expected by random variation. Options include choosing classical ANOVA (assuming equal variances) versus Welch ANOVA (allowing unequal variances), deciding whether the design is one-way, two-way, or repeated measures, and specifying post-hoc comparisons (e.g., Tukey HSD or Games–Howell) if the omnibus test is significant. In healthcare reporting, it is important to pair ANOVA results with effect sizes (mean differences) and confidence intervals, because clinical relevance depends on magnitude. Also consider whether patient-level clustering or repeated measurements require mixed models rather than simple ANOVA.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply Repeated-measures ANOVA (with GG/HF correction if needed) to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Robust ANOVA / trimmed-means ANOVA": "General explanation: Robust ANOVA / trimmed-means ANOVA compares mean outcomes across three or more groups (or conditions), which is common in medtech when evaluating multiple device configurations, dose levels, or clinical sites. The central idea is to partition variability into between-group and within-group components and test whether group means differ more than expected by random variation. Options include choosing classical ANOVA (assuming equal variances) versus Welch ANOVA (allowing unequal variances), deciding whether the design is one-way, two-way, or repeated measures, and specifying post-hoc comparisons (e.g., Tukey HSD or Games–Howell) if the omnibus test is significant. In healthcare reporting, it is important to pair ANOVA results with effect sizes (mean differences) and confidence intervals, because clinical relevance depends on magnitude. Also consider whether patient-level clustering or repeated measurements require mixed models rather than simple ANOVA.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply Robust ANOVA / trimmed-means ANOVA to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Robust regression (Huber/M-estimators)": "General explanation: Robust regression (Huber/M-estimators) belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Robust regression (Huber/M-estimators) (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Runs test": "General explanation: Runs test evaluates whether a sequence appears random versus exhibiting structure, which can be useful for checking assumptions in signal processing and quality control. In medtech, you might use such a test on residuals from a model applied to ECG segments or on sequences of device error signs to see whether errors cluster over time. A runs test, for example, looks at the number and length of consecutive runs of similar values (e.g., above vs below a threshold). Too few runs suggests positive autocorrelation or drift; too many can suggest alternation or overcorrection. Options include how you dichotomize the sequence (median split, sign of residuals, threshold around zero) and whether you analyze blocks per patient to avoid mixing independent and dependent segments. This kind of check is rarely the final analysis, but it can highlight model misspecification or process instability that would invalidate standard inference.\n\nExample: Example (medtech/healthcare): After fitting a heart-rate artifact removal filter, engineers examine the sign of residual errors over time for each patient. They run Runs test on the residual sign sequence to see whether errors cluster (suggesting drift) or alternate (suggesting overcorrection). They perform the test per patient-day to avoid mixing independent sequences, then correlate failures with device temperature logs. The finding—too few runs during long workouts—leads to a firmware fix that stabilizes the sensor under heat, improving downstream clinical metrics and ensuring the model assumptions used in later analyses are more defensible. A practical tip is to pair the test with an effect size and a confidence interval so the result is interpretable for clinicians and product stakeholders, not only statistically significant.",
  "Score/Lagrange multiplier test": "General explanation: Score/Lagrange multiplier test belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Score/Lagrange multiplier test (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Shapiro–Wilk": "General explanation: Shapiro–Wilk is typically used to assess whether a sample behaves like it came from a target distribution, most commonly to check normality before applying parametric methods. In healthcare studies, distribution checks matter because endpoints like biomarker concentrations, length of stay, and device error often have skewness or heavy tails. Normality or goodness-of-fit tests provide an objective signal, but they are sensitive to sample size: with large clinical datasets they can flag tiny, clinically irrelevant deviations, while with small pilot studies they may have low power. Practical options include choosing a test appropriate for the sample size and the type of deviation you care about (tail behavior vs central fit), using Q-Q plots alongside p-values, and considering transformations (log, Box-Cox) or robust/nonparametric alternatives when normality is implausible. The result should be interpreted in the context of the downstream method’s robustness, not as a standalone gatekeeper.\n\nExample: Example (medtech/healthcare): A lab team compares two assay protocols and plans to use a pooled-variance t-test on log-transformed concentrations. Before finalizing, they check whether the transformed concentrations are approximately normal within each protocol group. They run Shapiro–Wilk and inspect Q-Q plots. The test suggests mild tail deviations in a small subgroup of hemolyzed samples. Rather than deleting data, they document a sensitivity analysis: a robust comparison (e.g., Welch or nonparametric) and a bootstrap CI for the mean difference. If conclusions agree across approaches, they proceed with the pre-specified parametric analysis and note that the normality check did not materially change interpretation.",
  "Sign test": "General explanation: Sign test is a rank-based or distribution-free approach for comparing groups or paired conditions when parametric assumptions are doubtful. In healthcare, endpoints like pain scores, ordinal symptom scales, and skewed biomarker concentrations often violate normality, and sample sizes can be small in pilot studies. Nonparametric tests reduce sensitivity to outliers and heavy tails by operating on ranks or signs. Practical options include ensuring the design matches the test: paired vs unpaired, two groups vs many groups, and whether the test targets a shift in location (median) or a more general difference in distributions. Some methods (like Wilcoxon signed-rank) assume symmetric paired differences; alternatives (like the sign test) relax that but may be less powerful. In medtech reporting, these tests are often paired with effect measures like median difference or rank-biserial correlation so clinicians can interpret the magnitude.\n\nExample: Example (medtech/healthcare): A pilot study tests whether a new haptic cue in a rehabilitation device improves a 0–10 pain score immediately after therapy. Pain scores are ordinal and skewed, and the study enrolls only 18 patients, so parametric assumptions are shaky. You use Sign test to compare the pain scores between conditions (paired if the same patient tries both settings, unpaired if randomized to one setting). You report the median difference and the proportion of patients who improved by at least 2 points, which is clinically interpretable. Because nonparametric tests can still be sensitive to tied values, you document how ties are handled and confirm conclusions using a permutation test as a robustness check.",
  "Simple linear regression t-test on slope": "General explanation: Simple linear regression t-test on slope is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Simple linear regression t-test on slope, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Spearman": "General explanation: Spearman assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Spearman to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Spearman rank correlation": "General explanation: Spearman rank correlation assesses the strength of association between two variables, often to validate that a digital biomarker tracks a clinical reference or to explore relationships in observational data. In healthcare analytics, correlation is commonly used when you want a quick sense of whether higher values of one measure tend to coincide with higher (or lower) values of another, for example a wearable activity metric versus a patient‑reported outcome. Options depend on the specific correlation family: Pearson targets linear relationships and assumes roughly normal errors; Spearman and Kendall are rank‑based and more robust to outliers and nonlinear monotonic trends. Practical considerations include whether the relationship is confounded by age, sex, or disease severity, whether repeated measures per patient require mixed models rather than simple correlation, and whether correlation is sufficient for your claim. Correlation does not prove interchangeability or causal effect, so medtech submissions often pair it with agreement analyses, calibration, or model-based adjustment.\n\nExample: Example (medtech/healthcare): A digital therapeutic tracks daily step count from a phone and wants to show it aligns with functional status. In a 12-week study of patients with heart failure, you collect weekly 6-minute walk distance and average daily steps in the prior 7 days. You compute Spearman rank correlation to assess association, expecting a monotonic relationship but not necessarily linear because symptoms plateau at higher activity levels. You also stratify by beta-blocker use and adjust for age in a secondary analysis, because both can affect activity and walking distance. The correlation supports construct validity, but you also report whether step count changes predict clinically meaningful improvements, not only whether they correlate cross-sectionally.",
  "Stuart–Maxwell test (marginal homogeneity)": "General explanation: Stuart–Maxwell test (marginal homogeneity) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Stuart–Maxwell test (marginal homogeneity) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Stuart–Maxwell/Bowker (>2 categories)": "General explanation: Stuart–Maxwell/Bowker (>2 categories) applies when outcomes are categorical or binary—common in clinical studies (event/no event), diagnostic decisions (positive/negative), or discrete severity classes. These tests evaluate whether proportions differ across groups, whether two categorical variables are associated, or whether paired categorical outcomes are symmetric. Options depend on the structure of the data: chi-square tests are typical for large-sample contingency tables; Fisher’s exact and other exact procedures are preferred when counts are small; paired designs call for McNemar, Bowker, or Stuart–Maxwell-type methods; and ordered categories may justify trend tests like Cochran–Armitage. In medtech validation, it’s common to report both hypothesis tests and estimation—risk differences, risk ratios, odds ratios—with confidence intervals. You also need to define how you handle missing adjudications, repeated measurements per patient, and multiple comparisons across subgroups or endpoints.\n\nExample: Example (medtech/healthcare): A diagnostic device classifies urine samples as ‘infection’ vs ‘no infection’. You compare the device result to culture on 220 samples, and you also compare performance across two manufacturing lots. You apply Stuart–Maxwell/Bowker (>2 categories) to test whether the proportion of positive results differs by lot and to assess association between device and culture results. If cell counts are small (e.g., very few false positives), you switch to an exact method rather than chi-square. You report effect sizes—risk difference and odds ratio—with confidence intervals, and you stratify by symptom status to check for spectrum effects that can matter for regulatory claims.",
  "Student two-sample t-test (pooled variance)": "General explanation: Student two-sample t-test (pooled variance) is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Student two-sample t-test (pooled variance), the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Tukey HSD": "General explanation: Tukey HSD is a post-hoc multiple-comparison procedure used after an omnibus ANOVA-type analysis indicates that not all group means are equal. In a medtech context, imagine you tested four sensor firmware versions and saw an overall difference in mean absolute error; a post-hoc method tells you which pairs differ while controlling the familywise error rate. Tukey HSD is typically used when variances are roughly equal and sample sizes are balanced, while Games–Howell is a popular choice when variances or sample sizes differ. Options include which pairs to compare (all pairs vs a subset), whether to use adjusted confidence intervals, and whether you pre-specify clinically meaningful contrasts (e.g., new device vs current standard). These procedures help avoid overclaiming improvements when many comparisons are possible, which is especially important in regulated evidence where multiplicity control is scrutinized.\n\nExample: Example (medtech/healthcare): A hospital tests four alarm-threshold settings for a pulse oximeter to reduce nuisance alarms while maintaining safety. An ANOVA on alarm counts per patient-day suggests differences among thresholds. To find which settings differ, you apply Tukey HSD as a post-hoc procedure. You obtain adjusted confidence intervals for each pair of thresholds and identify that threshold C significantly reduces alarms compared with A and B, while C and D are not meaningfully different. You pair the statistical result with a clinical safety review of missed desaturation events, ensuring the chosen threshold is not just statistically different but clinically acceptable.",
  "Two-proportion z-test": "General explanation: Two-proportion z-test is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Two-proportion z-test, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Two-sample t-test equivalence": "General explanation: Two-sample t-test equivalence is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Two-sample t-test equivalence, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Wald test": "General explanation: Wald test belongs to the regression/modeling family used to explain or predict outcomes using multiple predictors, which is central to modern medtech evidence. Examples include logistic regression for a binary safety event, linear regression for a continuous biomarker, or mixed-effects models to account for site-to-site variation in multicenter studies. Model-based tests such as the likelihood ratio test, Wald test, or score/Lagrange multiplier test assess whether a predictor (or set of predictors) improves model fit or whether coefficients differ from zero. Key options include choosing the link function and distribution (e.g., logit vs probit), specifying interactions, deciding how to handle confounding and collinearity, and selecting a correlation structure for repeated measures (GEE) or random effects (LMM/GLMM). In healthcare, interpretability matters: you typically report adjusted effects (e.g., odds ratios) with confidence intervals and perform diagnostics to ensure assumptions are not violated in clinically important ways.\n\nExample: Example (medtech/healthcare): A remote patient monitoring program wants to predict 30-day readmission using vitals trends and comorbidities. The team fits Wald test (or uses it within a broader regression framework) to evaluate whether adding a new wearable-derived feature improves the model after adjusting for age, baseline risk, and site. They compare nested models using a likelihood-based test and also examine calibration and decision-curve impact at operational thresholds. Because readmissions cluster by hospital, they include random effects or use GEE to avoid overstated significance. The final report includes adjusted effect estimates, uncertainty intervals, and checks for model stability across demographics—key for safe deployment in healthcare settings.",
  "Welch ANOVA": "General explanation: Welch ANOVA compares mean outcomes across three or more groups (or conditions), which is common in medtech when evaluating multiple device configurations, dose levels, or clinical sites. The central idea is to partition variability into between-group and within-group components and test whether group means differ more than expected by random variation. Options include choosing classical ANOVA (assuming equal variances) versus Welch ANOVA (allowing unequal variances), deciding whether the design is one-way, two-way, or repeated measures, and specifying post-hoc comparisons (e.g., Tukey HSD or Games–Howell) if the omnibus test is significant. In healthcare reporting, it is important to pair ANOVA results with effect sizes (mean differences) and confidence intervals, because clinical relevance depends on magnitude. Also consider whether patient-level clustering or repeated measurements require mixed models rather than simple ANOVA.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply Welch ANOVA to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change.",
  "Welch two-sample t-test": "General explanation: Welch two-sample t-test is used to compare means under a parametric framework, typically assuming approximately normal errors (or relying on large-sample approximations). In healthcare R&D, mean comparisons appear everywhere: comparing average measurement error of two device algorithms, average change in a biomarker pre/post intervention, or average time-to-task completion in a usability study. The main ‘options’ are design-driven: one-sample vs two-sample vs paired designs, pooled-variance vs Welch approaches depending on variance equality, and two-sided vs one-sided hypotheses depending on whether you are testing superiority or non-inferiority/equivalence. A good medtech analysis also reports the estimated mean difference with a confidence interval and checks sensitivity to outliers and non-normality (e.g., via robust methods or transformations). When data are repeated measures per patient, a mixed model may be more appropriate than a simple t-test, because it accounts for within-patient correlation.\n\nExample: Example (medtech/healthcare): A usability study compares average task completion time for clinicians using two versions of a medication-dosing UI. Using Welch two-sample t-test, the team tests whether the mean time differs between versions while also reporting the mean difference and a confidence interval. They predefine acceptable performance thresholds, then interpret the statistical result alongside practical significance (seconds saved per clinician per shift). They also run a sensitivity analysis excluding sessions with documented interruptions, and they describe how the final inference aligns with human factors risk controls. In regulated medtech work, it also helps to document the pre-specified analysis plan and to justify any deviations, because reviewers often look for clear decision rules.",
  "Wilcoxon signed-rank": "General explanation: Wilcoxon signed-rank is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Wilcoxon signed-rank to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Wilcoxon signed-rank (requires symmetric differences)": "General explanation: Wilcoxon signed-rank (requires symmetric differences) is used for time-to-event outcomes—common in healthcare for time to complication, time to hospitalization, time to device failure, or time to viral suppression. Survival methods handle censoring, where some patients have not yet experienced the event by the end of follow-up. The log-rank test compares survival curves between groups without assuming a specific distribution, while Cox proportional hazards models estimate covariate-adjusted hazard ratios under a proportional hazards assumption. Accelerated failure time (AFT) and parametric proportional hazards models assume a distribution (e.g., Weibull) and can provide direct time-ratio interpretations. Options include defining the event precisely, selecting covariates and time windows, checking proportional hazards (and using stratification or time-varying effects if violated), and deciding whether competing risks require specialized methods. In medtech clinical evidence, survival analyses often support claims about durability and long-term safety.\n\nExample: Example (medtech/healthcare): A cardiac implant trial tracks time to first device-related serious adverse event over 24 months. Some patients are censored due to loss to follow-up. You apply Wilcoxon signed-rank (requires symmetric differences) to compare the new implant to the predicate device and to adjust for baseline covariates like age and ejection fraction. You check proportional hazards visually and with tests; if hazards cross, you consider stratified or time-varying approaches. You present Kaplan–Meier curves, hazard ratios (or time ratios for AFT models), and absolute risk differences at clinically relevant time points. This helps stakeholders understand both statistical evidence and practical durability, which are central in medtech safety and effectiveness evaluations.",
  "Zero-inflated Poisson/NB": "General explanation: Zero-inflated Poisson/NB is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Zero-inflated Poisson/NB with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "Zero-inflated models": "General explanation: Zero-inflated models is used when the outcome is a count (0, 1, 2, …), which appears in healthcare as number of hospital readmissions, adverse events, alarms, or lesions detected. Simple Poisson models assume the mean equals the variance, but medical counts often have overdispersion, motivating negative binomial models. If there are many zeros (e.g., most patients have zero adverse events), zero-inflated or hurdle-style models explicitly mix a ‘structural zero’ process with a count process. Options include selecting the distribution (Poisson vs NB), deciding whether zero inflation is clinically plausible (two subpopulations) versus simply overdispersion, and choosing covariates and offsets (e.g., follow-up time) to model rates. In medtech, these models help compare event rates between device versions while adjusting for exposure, patient risk, and site effects, and they support clinically meaningful outputs like incidence rate ratios.\n\nExample: Example (medtech/healthcare): After deploying a new infusion pump firmware, a hospital tracks the number of occlusion alarms per patient-day. Most patient-days have zero alarms, but a minority have multiple alarms during complex infusions. You fit Zero-inflated models with an offset for exposure time, adjusting for unit type and patient acuity. A zero-inflated approach separates the ‘never-alarm’ days from days where alarms follow a count process, which matches the clinical intuition that many patients are low risk. You interpret the incidence rate ratio for the firmware indicator and confirm that alarm reductions do not coincide with increased adverse events, combining statistical modeling with safety surveillance.",
  "t contrasts within ANOVA/GLM framework": "General explanation: t contrasts within ANOVA/GLM framework compares mean outcomes across three or more groups (or conditions), which is common in medtech when evaluating multiple device configurations, dose levels, or clinical sites. The central idea is to partition variability into between-group and within-group components and test whether group means differ more than expected by random variation. Options include choosing classical ANOVA (assuming equal variances) versus Welch ANOVA (allowing unequal variances), deciding whether the design is one-way, two-way, or repeated measures, and specifying post-hoc comparisons (e.g., Tukey HSD or Games–Howell) if the omnibus test is significant. In healthcare reporting, it is important to pair ANOVA results with effect sizes (mean differences) and confidence intervals, because clinical relevance depends on magnitude. Also consider whether patient-level clustering or repeated measurements require mixed models rather than simple ANOVA.\n\nExample: Example (medtech/healthcare): A device team evaluates three segmentation algorithms for cardiac MRI and measures Dice similarity against expert contours. Because there are three algorithms, they apply t contrasts within ANOVA/GLM framework to test whether mean Dice differs across algorithms. After a significant omnibus result, they plan pre-specified pairwise comparisons with multiplicity control and also report mean differences with CIs. They check whether results change when restricting to scans with arrhythmia artifacts, a clinically relevant subgroup. The analysis supports selecting an algorithm for deployment while documenting uncertainty and generalizability. If assumptions are shaky, consider a sensitivity analysis (for example, robust or nonparametric alternatives) and report whether conclusions change."
};
// ============================================================================

  // References: key -> { title, url, ama }
  const REFERENCES = {"JGF_SPEC":{"title":"JSON Graph Format Specification","url":"https://jsongraphformat.info/","ama":"JSON Graph Format Specification [Internet]. Accessed December 21, 2025. Available from: https://jsongraphformat.info/"},"JGF_GITHUB":{"title":"json-graph-specification (GitHub)","url":"https://github.com/jsongraph/json-graph-specification","ama":"json-graph-specification (GitHub) [Internet]. Accessed December 21, 2025. Available from: https://github.com/jsongraph/json-graph-specification"},"NIST_EHANDBOOK":{"title":"NIST/SEMATECH e-Handbook of Statistical Methods","url":"https://www.itl.nist.gov/div898/handbook/","ama":"Heckert NA, Filliben JJ, Croarkin CM, et al. NIST/SEMATECH e-Handbook of Statistical Methods [Internet]. National Institute of Standards and Technology; 2002. doi:10.18434/M32189. Accessed December 21, 2025. Available from: https://www.itl.nist.gov/div898/handbook/index.htm."},"UCLA_WHATSTAT":{"title":"UCLA OARC: What statistical analysis should I use?","url":"https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/","ama":"UCLA OARC: What statistical analysis should I use? [Internet]. Accessed December 21, 2025. Available from: https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/"},"NYU_CHOOSE_TEST":{"title":"NYU Quantitative Analysis Guide: Choosing a Statistical Test","url":"https://guides.nyu.edu/quant/choose_test_1DV","ama":"NYU Quantitative Analysis Guide: Choosing a Statistical Test [Internet]. Accessed December 21, 2025. Available from: https://guides.nyu.edu/quant/choose_test_1DV"},"NIST_NORMALITY":{"title":"NIST: Anderson-Darling and Shapiro-Wilk tests","url":"https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm","ama":"NIST: Anderson-Darling and Shapiro-Wilk tests [Internet]. Accessed December 21, 2025. Available from: https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm"},"NIST_BOXLJUNG":{"title":"NIST: Box-Ljung Test","url":"https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm","ama":"NIST: Box-Ljung Test [Internet]. Accessed December 21, 2025. Available from: https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4481.htm"},"LOGRANK_PMC":{"title":"The logrank test (Bland, 2004) - PMC","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/","ama":"The logrank test (Bland, 2004) - PMC [Internet]. Accessed December 21, 2025. Available from: https://pmc.ncbi.nlm.nih.gov/articles/PMC403858/"},"BLAND_ALTMAN_PUBMED":{"title":"Bland & Altman (1986) agreement paper - PubMed","url":"https://pubmed.ncbi.nlm.nih.gov/2868172/","ama":"Bland JM, Altman DG. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet. 1986;1(8476):307-310. PMID:2868172."},"DELONG_PUBMED":{"title":"DeLong et al. (1988) correlated ROC AUC comparison - PubMed","url":"https://pubmed.ncbi.nlm.nih.gov/3203132/","ama":"DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics. 1988;44(3):837-845. PMID:3203132."},"ICC_PUBMED":{"title":"Shrout & Fleiss (1979) intraclass correlations - PubMed","url":"https://pubmed.ncbi.nlm.nih.gov/18839484/","ama":"Shrout PE, Fleiss JL. Intraclass correlations: uses in assessing rater reliability. Psychol Bull. 1979;86(2):420-428."},"EGGER_BMJ":{"title":"Egger et al. (1997) publication bias test - BMJ","url":"https://www.bmj.com/content/315/7109/629","ama":"Egger et al. (1997) publication bias test - BMJ [Internet]. Accessed December 21, 2025. Available from: https://www.bmj.com/content/315/7109/629"},"BH_FDR":{"title":"Benjamini & Hochberg (1995) FDR procedure - JRSS B","url":"https://academic.oup.com/jrsssb/article/57/1/289/7035855","ama":"Benjamini & Hochberg (1995) FDR procedure - JRSS B [Internet]. Accessed December 21, 2025. Available from: https://academic.oup.com/jrsssb/article/57/1/289/7035855"},"GRANGER_1969":{"title":"Granger (1969) causality paper (PDF)","url":"https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf","ama":"Granger (1969) causality paper (PDF) [Internet]. Accessed December 21, 2025. Available from: https://www.mimuw.edu.pl/~noble/courses/TimeSeries/RESOURCES/69EconometricaGrangerCausality.pdf"},"DIEBOLD_MARIANO_1995":{"title":"Diebold & Mariano (1995) Comparing Predictive Accuracy","url":"https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599","ama":"Diebold & Mariano (1995) Comparing Predictive Accuracy [Internet]. Accessed December 21, 2025. Available from: https://www.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599"},"KPSS_1992":{"title":"Kwiatkowski et al. (1992) stationarity test paper","url":"https://www.sciencedirect.com/science/article/pii/030440769290104Y","ama":"Kwiatkowski et al. (1992) stationarity test paper [Internet]. Accessed December 21, 2025. Available from: https://www.sciencedirect.com/science/article/pii/030440769290104Y"},"COCHRANS_Q_NCSS":{"title":"NCSS: Cochran\u2019s Q Test (binary matched sets) (PDF)","url":"https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf","ama":"NCSS: Cochran\u2019s Q Test (binary matched sets) (PDF) [Internet]. Accessed December 21, 2025. Available from: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Cochrans_Q_Test.pdf"},"MCNEMAR_NCSS":{"title":"NCSS/PASS: McNemar test (paired proportions) (PDF)","url":"https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf","ama":"NCSS/PASS: McNemar test (paired proportions) (PDF) [Internet]. Accessed December 21, 2025. Available from: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Tests_for_Two_Correlated_Proportions-McNemar_Test.pdf"},"COCHRANE_HANDBOOK":{"title":"Cochrane Handbook (Chapter 10: meta-analysis)","url":"https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10","ama":"Cochrane Handbook (Chapter 10: meta-analysis) [Internet]. Accessed December 21, 2025. Available from: https://www.cochrane.org/authors/handbooks-and-manuals/handbook/current/chapter-10"},"MANTEL_HAENSZEL_WHO":{"title":"IARC/WHO text describing Mantel\u2013Haenszel method (PDF)","url":"https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf","ama":"IARC/WHO text describing Mantel\u2013Haenszel method (PDF) [Internet]. Accessed December 21, 2025. Available from: https://publications.iarc.who.int/_publications/media/download/3473/7eadbe75717a80e15eb69ab32f2ae7c95daa7aa5.pdf"},"COCHRAN_Q_PUBMED":{"title":"Hoaglin (2016) paper on Q and heterogeneity - PubMed","url":"https://pubmed.ncbi.nlm.nih.gov/26303773/","ama":"Hoaglin DC. Misunderstandings about Q and 'Cochran's Q test' in meta-analysis. Stat Med. 2016;35(4):485-495. doi:10.1002/sim.6632."},"CA_TREND_SCIDIR":{"title":"Neuh\u00e4user (1999) Cochran\u2013Armitage trend test note","url":"https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917","ama":"Neuh\u00e4user (1999) Cochran\u2013Armitage trend test note [Internet]. Accessed December 21, 2025. Available from: https://www.sciencedirect.com/science/article/abs/pii/S0167947398000917"}};

  // Stable ordered bibliography
  const REFERENCE_KEYS = Object.keys(REFERENCES);
  const REF_INDEX = Object.fromEntries(REFERENCE_KEYS.map((k, i) => [k, i + 1]));

  function escapeHtml(str) {
    return String(str).replace(/[&<>"']/g, s => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[s]));
  }

  // ===== Image helpers for modals (filename "shell paths") =====
  // Generates predictable .png filenames for each modal, so you can later drop real images in ./images/
  function slugifyFilename(input) {
    let s = String(input ?? '').trim().toLowerCase();
    if (!s) return '';
    // Normalize accents and a few common superscripts
    try { s = s.normalize('NFKD'); } catch {}
    s = s.replace(/[\u0300-\u036f]/g, '');
    s = s.replace(/²/g, '2').replace(/³/g, '3');
    // Common symbols/greek
    s = s.replace(/σ/g, 'sigma').replace(/μ/g, 'mu').replace(/β/g, 'beta').replace(/α/g, 'alpha');
    s = s.replace(/&/g, ' and ');
    // Collapse non-alphanumerics to underscores
    s = s.replace(/[^a-z0-9]+/g, '_');
    s = s.replace(/_+/g, '_').replace(/^_+|_+$/g, '');
    return s;
  }

  function truncateSlug(slug, maxLen = 70) {
    const s = String(slug ?? '');
    if (s.length <= maxLen) return s;
    return s.slice(0, maxLen).replace(/_+$/g, '');
  }

  // Tests get "clean" names like ttest.png, mcnemar_test.png, etc.
  function testImageFile(testName) {
    let s = String(testName ?? '').trim().toLowerCase();
    // Prefer compact "ttest" / "ztest" / "ftest" style for common hyphenated tokens
    s = s.replace(/\bt\s*[-–—]?\s*test\b/g, 'ttest');
    s = s.replace(/\bz\s*[-–—]?\s*test\b/g, 'ztest');
    s = s.replace(/\bf\s*[-–—]?\s*test\b/g, 'ftest');
    s = s.replace(/\bchi\s*[-–—]?\s*square\b/g, 'chisquare');
    const slug = truncateSlug(slugifyFilename(s), 80) || 'test';
    return `${slug}.png`;
  }

  // Nodes (questions/recommendations) use id + a shortened slug of the label.
  function nodeImageFile(node) {
    const id = String(node?.id || 'node').toLowerCase();
    const label = String(node?.label || node?.id || '').trim();
    const slug = truncateSlug(slugifyFilename(label), 70) || 'image';
    return `${id}_${slug}.png`;
  }

  // Options use source id + a slug of the option label (or option_code).
  function optionImageFile(sourceNode, edge, targetNode) {
    const src = String(sourceNode?.id || 'src').toLowerCase();
    const key = edge?.label || edge?.metadata?.option_code || targetNode?.label || edge?.target || 'option';
    const slug = truncateSlug(slugifyFilename(key), 70) || 'option';
    return `${src}_${slug}.png`;
  }

  // Produces the consistent modal image container. Files are assumed to live in ./images/
  function renderModalImage(fileName, altText) {
    const safeAlt = escapeHtml(altText || '');
    const safeFile = escapeHtml(fileName || 'image.png');
    return `<div class="media-ph"><img src="images/${safeFile}" alt="${safeAlt}" loading="lazy" /></div>`;
  }


  // ===== Helpers to render elaborated explanations (EXPLANATIONS) =====
  function toParagraphs(text) {
    if (!text) return '';
    const parts = String(text)
      .split(/\n\s*\n/)
      .map(s => s.trim())
      .filter(Boolean);
    return parts
      .map((p, i) => `<p class="text-base"${i ? ' style="margin-top:0.625rem;"' : ''}>${escapeHtml(p)}</p>`)
      .join('');
  }

  function getNodeExplanation(nodeId) {
    return EXPLANATIONS?.[nodeId]?.question ? String(EXPLANATIONS[nodeId].question) : '';
  }

  function getOptionExplanation(sourceNodeId, edge) {
    const opt = EXPLANATIONS?.[sourceNodeId]?.options || {};
    const byLabel = edge?.label && opt[edge.label];
    const byCode = edge?.metadata?.option_code && opt[edge.metadata.option_code];
    return byLabel || byCode || '';
  }

  function getTestExplanation(testName) {
    return TEST_EXPLANATIONS?.[testName] ? String(TEST_EXPLANATIONS[testName]) : '';
  }

  // ==================================================================



  // Modal helper
  const modalRoot = document.getElementById('modalRoot');


  // Motion helpers (respect prefers-reduced-motion)
  const REDUCE_MOTION = window.matchMedia && window.matchMedia('(prefers-reduced-motion: reduce)').matches;

  function bumpAnim(el, cls) {
    if (REDUCE_MOTION || !el) return;
    el.classList.remove(cls);
    // Force reflow so the animation restarts
    void el.offsetWidth;
    el.classList.add(cls);
  }

  function staggerAnim(container, selector, step = 45) {
    if (REDUCE_MOTION || !container) return;
    const items = [...container.querySelectorAll(selector)];
    items.forEach((it, i) => {
      it.style.setProperty('--delay', `${i * step}ms`);
      it.classList.remove('anim-stagger-item');
    });
    // Single reflow, then start stagger animations
    void container.offsetWidth;
    items.forEach(it => it.classList.add('anim-stagger-item'));
  }

  // Kick off initial page enter transition (body starts with class="preload")
  (function initEnter() {
    const cards = document.querySelectorAll('main > section.card');
    cards.forEach((c, i) => c.style.setProperty('--enter-delay', `${120 + i * 120}ms`));

    if (REDUCE_MOTION) {
      document.body.classList.remove('preload');
      return;
    }

    requestAnimationFrame(() => document.body.classList.remove('preload'));
  })();

  function closeModal() {
    const overlay = modalRoot.querySelector('.modal-overlay');
    if (!overlay) {
      modalRoot.innerHTML = '';
      document.body.style.overflow = '';
      return;
    }

    if (REDUCE_MOTION) {
      modalRoot.innerHTML = '';
      document.body.style.overflow = '';
      return;
    }

    overlay.classList.add('is-closing');

    let done = false;

    function cleanup() {
      if (done) return;
      done = true;
      overlay.removeEventListener('transitionend', onEnd);
      modalRoot.innerHTML = '';
      document.body.style.overflow = '';
    }

    function onEnd(e) {
      if (e && e.target !== overlay) return;
      cleanup();
    }

    // Wait for the overlay fade to finish; fallback timer in case transitionend doesn't fire.
    overlay.addEventListener('transitionend', onEnd);
    setTimeout(cleanup, 320);
  }


  function openModal({ title, html: innerHtml, refs = [], primaryLink = null }) {
    document.body.style.overflow = 'hidden';

    const refsUnique = [...new Set(refs)].filter(k => REFERENCES[k]);
    const refList = refsUnique.length
      ? `
        <div style="margin-top: 0.75rem;">
          <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">References</div>
          <ol class="ref-list text-sm font-normal">
            ${refsUnique.map(k => {
              const num = REF_INDEX[k] ?? '?';
              const cite = escapeHtml(REFERENCES[k].ama);
              const url = REFERENCES[k].url;
              const link = url ? ` <a class="a" href="${url}" target="_blank" rel="noreferrer">Open</a>` : '';
              return `<li><span style="opacity:0.78;">[${num}]</span> ${cite}${link}</li>`;
            }).join('')}
          </ol>
        </div>
      ` : '';

    const linkRow = primaryLink
      ? `<div style="margin-top:0.625rem;"><a class="btn btn-primary text-sm font-medium" style="text-decoration:none;" href="${primaryLink}" target="_blank" rel="noreferrer">Open source</a></div>`
      : '';

    modalRoot.innerHTML = `
      <div class="modal-overlay" role="dialog" aria-modal="true" aria-label="${escapeHtml(title)}">
        <div class="modal" tabindex="-1">
          <div class="modal-head">
            <div>
              <div class="text-xl">${escapeHtml(title)}</div>
              <div class="text-xs" style="opacity:0.78; margin-top:0.125rem;">Press Esc to close</div>
            </div>
            <button class="btn btn-ghost text-sm font-medium" id="modalClose">Close</button>
          </div>
          <div class="modal-body text-base">
            ${innerHtml}
            ${linkRow}
            ${refList}
          </div>
        </div>
      </div>
    `;

    const overlay = modalRoot.querySelector('.modal-overlay');
    const modal = modalRoot.querySelector('.modal');
    if (overlay && !REDUCE_MOTION) requestAnimationFrame(() => overlay.classList.add('is-open'));
    if (modal) modal.focus({ preventScroll: true });

    document.getElementById('modalClose').onclick = closeModal;
    modalRoot.querySelector('.modal-overlay').addEventListener('click', (e) => {
      if (e.target.classList.contains('modal-overlay')) closeModal();
    });
  }

  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape') closeModal();
  });

  // Build graph maps (augment leaf recommendations with explicit per-test leaves)
  const baseNodes = FLOW.nodes;
  const baseEdges = FLOW.edges;

  // Virtual "test" nodes are generated from leaf.metadata.tests so tests appear in the Flow Lens too.
  const virtualNodes = [];
  const virtualEdges = [];
  const testChildrenByLeaf = new Map();

  for (const n of baseNodes) {
    const kind = (n?.metadata?.kind) || 'unknown';
    const tests = Array.isArray(n?.metadata?.tests) ? n.metadata.tests.map(t => String(t)) : [];
    if (kind === 'leaf' && tests.length) {
      const ids = [];
      tests.forEach((t, i) => {
        const id = `${n.id}__TEST__${i + 1}`;
        ids.push(id);
        virtualNodes.push({
          id,
          label: t,
          metadata: {
            kind: 'test',
            parentLeafId: n.id,
            refs: (n.metadata?.refs || []),
            notes: (n.metadata?.notes || '')
          }
        });
        virtualEdges.push({
          source: n.id,
          target: id,
          label: '',
          metadata: { kind: 'virtual_test_edge' }
        });
      });
      testChildrenByLeaf.set(n.id, ids);
    }
  }

  const nodes = [...baseNodes, ...virtualNodes];
  const edges = [...baseEdges, ...virtualEdges];

  const nodeById = new Map(nodes.map(n => [n.id, n]));
  const outEdges = new Map();
  const inEdges = new Map();

  for (const e of edges) {
    if (!outEdges.has(e.source)) outEdges.set(e.source, []);
    outEdges.get(e.source).push(e);

    if (!inEdges.has(e.target)) inEdges.set(e.target, []);
    inEdges.get(e.target).push(e);
  }

  function nodeKind(n) {
    return (n?.metadata?.kind) || 'unknown';
  }

  // Initial state: START -> first edge (Begin) -> first question
  function initialNodeId() {
    const startId = nodeById.has('START') ? 'START' : nodes[0]?.id;
    const oe = outEdges.get(startId) || [];
    return oe[0]?.target || startId;
  }

  let history = []; // items: { nodeId, viaEdge? }
  let currentNodeId = initialNodeId();

  function shortLabel(label) {
    if (!label) return 'Step';
    const s = String(label).trim();
    if (s.length <= 26) return s;
    return s.slice(0, 26).trim() + '…';
  }

  // Render crumbs
  const crumbsEl = document.getElementById('crumbs');
  function renderCrumbs(animate = false) {
    const path = [...history.map(h => h.nodeId), currentNodeId].filter(Boolean);
    crumbsEl.innerHTML = path.map((id, idx) => {
      const n = nodeById.get(id);
      const isActive = idx === path.length - 1;
      return `<div class="chip ${isActive ? 'active' : ''}" data-idx="${idx}"><span class="text-xs">${idx+1}</span><span class="text-sm font-medium">${escapeHtml(shortLabel(n?.label))}</span></div>`;
    }).join('');

    [...crumbsEl.querySelectorAll('.chip')].forEach(chip => {
      chip.addEventListener('click', () => {
        const idx = Number(chip.getAttribute('data-idx'));
        jumpToIndex(idx);
      });
    });

    document.getElementById('stepCount').textContent = `Step ${path.length}`;
    if (animate) {
      bumpAnim(crumbsEl, 'anim-swap');
      staggerAnim(crumbsEl, '.chip', 30);
      bumpAnim(document.getElementById('stepCount'), 'anim-swap');
    }
  }

  function jumpToIndex(idx) {
    const path = [...history.map(h => h.nodeId), currentNodeId].filter(Boolean);
    if (idx < 0 || idx >= path.length) return;
    const targetNode = path[idx];

    // rebuild history to idx-1, set current to idx
    history = path.slice(0, idx).map((nodeId) => ({ nodeId, viaEdge: null }));
    currentNodeId = targetNode;
    renderAll(true);
  }

  // Decision panel
  const questionBox = document.getElementById('questionBox');
  const optionsEl = document.getElementById('options');

  function refsForNode(n) {
    const r = n?.metadata?.refs || [];
    return Array.isArray(r) ? r : [];
  }

  function renderPanel(animate = false) {
    const n = nodeById.get(currentNodeId);
    if (!n) return;

    const kind = nodeKind(n);
    const refs = refsForNode(n);

    // header content (question / leaf)
    const titleRow = `
      <div class="question-title">
        <div style="min-width:0">
          <div class="text-xs" style="opacity:0.78;">${kind === 'leaf' ? 'Recommendation' : (kind === 'test' ? 'Test' : (kind === 'question' ? 'Question' : 'Step'))}</div>
          <div class="text-base" style="margin-top: 0.125rem;"><span class="font-semibold" style="color:#0f172a;">${escapeHtml(n.label)}</span></div>
          <p class="helper text-sm font-normal">${(kind === 'leaf' || kind === 'test')
            ? 'This is a terminal node in the tree — copy the summary or explore the references.'
            : 'Choose the option that best matches your data and design.'}</p>
        </div>
        <button class="icon-btn" id="qInfoBtn" aria-label="More information">
          <svg class="icon" viewBox="0 0 24 24" fill="none" aria-hidden="true">
            <path d="M12 17v-6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
            <path d="M12 7h.01" stroke="currentColor" stroke-width="3" stroke-linecap="round"/>
            <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2" opacity="0.6"/>
          </svg>
        </button>
      </div>
    `;

    let bodyHtml = titleRow;

    if (kind === 'leaf') {
      const tests = (n.metadata?.tests || []).map(t => String(t));
      const notes = (n.metadata?.notes || '').trim();

      bodyHtml += `
        <div style="margin-top: 0.625rem;">
          <div class="text-xs" style="opacity:0.78; margin-bottom: 0.375rem;">Suggested tests / methods</div>
          <div class="btn-row">
            ${tests.map((t) => `
              <button class="btn btn-ghost text-sm font-medium" data-test="${escapeHtml(t)}">
                ${escapeHtml(t)}
                <span style="opacity:0.75;" aria-hidden="true">↗</span>
              </button>
            `).join('')}
          </div>
        </div>
      `;

      if (notes) {
        bodyHtml += `
          <div style="margin-top: 0.75rem;">
            <div class="text-xs" style="opacity:0.78;">Notes</div>
            <div class="text-base" style="margin-top: 0.375rem;">${escapeHtml(notes)}</div>
          </div>
        `;
      }
    if (kind === 'test') {
      const parentId = n.metadata?.parentLeafId;
      const parent = parentId ? nodeById.get(parentId) : null;
      const notes = (n.metadata?.notes || '').trim();

      bodyHtml += `
        <div style="margin-top: 0.625rem;">
          <div class="text-xs" style="opacity:0.78; margin-bottom: 0.375rem;">Recommendation group</div>
          <div class="text-base"><span class="font-semibold">${escapeHtml(parent?.label || parentId || '—')}</span></div>
        </div>
      `;

      if (notes) {
        bodyHtml += `
          <div style="margin-top: 0.75rem;">
            <div class="text-xs" style="opacity:0.78;">Notes</div>
            <div class="text-base" style="margin-top: 0.375rem;">${escapeHtml(notes)}</div>
          </div>
        `;
      }
    }

    }

    questionBox.innerHTML = bodyHtml;
    if (animate) {
      bumpAnim(questionBox, 'anim-swap');
      // If this view includes test chips, stagger them in.
      staggerAnim(questionBox, '[data-test]', 55);
    }

    // info modal content
    const infoBtn = document.getElementById('qInfoBtn');
    infoBtn.onclick = () => {
      const n = nodeById.get(currentNodeId);
      const kind = nodeKind(n);
      const refs = refsForNode(n);

      let extra = '';
      if (kind === 'leaf') {
        const tests = (n.metadata?.tests || []).map(t => String(t));
        const notes = (n.metadata?.notes || '').trim();

        extra = `
          <p class="text-base">This is an endpoint recommendation in the decision tree. It groups statistical methods that match the choices you made on data type, study design, and clinical question. Use the test-specific modal for each method to see the full medtech-focused general explanation and example.</p>

          ${tests.length ? `
            <div style="margin-top:0.75rem;">
              <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">Suggested tests / procedures</div>
              <ul class="text-base" style="margin:0; padding-left:1.125rem;">
                ${tests.map(t => `<li>${escapeHtml(t)}</li>`).join('')}
              </ul>
              <p class="text-sm font-normal" style="margin-top:0.625rem; opacity:0.82;">Tip: Click any test chip shown in the panel to open its detailed explanation and example.</p>
            </div>
          ` : ''}

          ${notes ? `
            <div style="margin-top:0.75rem;">
              <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">Notes</div>
              ${toParagraphs(notes)}
            </div>
          ` : ''}
        `;
      } else if (kind === 'test') {
        const testName = String(n?.label || '').trim();
        const tExpl = getTestExplanation(testName);
        extra = tExpl
          ? `
            <div style="margin-top:0.625rem;">
              ${toParagraphs(tExpl)}
            </div>
          `
          : `<p class="text-base">No explanation was found for this test in the injected documentation.</p>`;
      } else {
        const qExpl = getNodeExplanation(n.id);
        extra = qExpl
          ? `
            <div style="margin-top:0.625rem;">
              ${toParagraphs(qExpl)}
            </div>
          `
          : `<p class="text-base">No explanation was found for this question in the injected documentation.</p>`;
      }

      const modalTitle =
        (kind === 'test') ? (String(n?.label || 'Suggested test')) :
        (kind === 'leaf') ? 'Recommendation details' :
        'Question details';

      const imgFile = (kind === 'test') ? testImageFile(String(n?.label || modalTitle)) : nodeImageFile(n);

      openModal({
        title: modalTitle,
        html: `
          ${renderModalImage(imgFile, modalTitle)}
          ${extra}
        `,
        refs,
        primaryLink: null
      });
    };

    // tests click => open modal
    if (kind === 'leaf') {
      [...questionBox.querySelectorAll('[data-test]')].forEach(btn => {
        btn.addEventListener('click', () => {
          const test = btn.getAttribute('data-test');
          const tExpl = getTestExplanation(test);
          const tBlock = tExpl
            ? `
              <div style="margin-top:0.625rem;">
                ${toParagraphs(tExpl)}
              </div>
            `
            : `<p class="text-base">No explanation was found for this test in the injected documentation.</p>`;

          const imgFile = testImageFile(test);

          openModal({
            title: test,
            html: `
              ${renderModalImage(imgFile, test)}
              ${tBlock}
            `,
            refs,
            primaryLink: null
          });
        });
      });
    }

    // render options
    const oe = outEdges.get(currentNodeId) || [];
    if (kind === 'leaf' || oe.length === 0) {
      optionsEl.innerHTML = `
        <button class="btn btn-primary option-btn text-sm font-medium" id="btnToRefs">
          <span class="option-left">
            <span class="option-badge">★</span>
            <span class="option-text">
              <span class="text-sm font-medium text-white">Open full references</span>
            </span>
          </span>
          <span aria-hidden="true">→</span>
        </button>
      `;
      document.getElementById('btnToRefs').onclick = () => openReferencesModal();
      if (animate) {
        bumpAnim(optionsEl, 'anim-swap');
        staggerAnim(optionsEl, ':scope > *', 55);
      }
      return;
    }

    optionsEl.innerHTML = oe.map((e, idx) => {
      const target = nodeById.get(e.target);
      const tKind = nodeKind(target);
      const badge = String(idx + 1);
      const hint = tKind === 'leaf' ? 'Leads to a recommendation' : 'Leads to another question';
      return `
        <div style="display:flex; gap:0.625rem; align-items:stretch;">
          <button class="btn option-btn text-sm font-medium" data-idx="${idx}">
            <span class="option-left">
              <span class="option-badge">${badge}</span>
              <span class="option-text">
                <span class="label text-sm font-medium">${escapeHtml(e.label || 'Choose')}</span>
              </span>
            </span>
            <span aria-hidden="true">→</span>
          </button>
          <button class="icon-btn" aria-label="Option info" data-info="${idx}">
            <svg class="icon" viewBox="0 0 24 24" fill="none" aria-hidden="true">
              <path d="M12 17v-6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
              <path d="M12 7h.01" stroke="currentColor" stroke-width="3" stroke-linecap="round"/>
              <circle cx="12" cy="12" r="10" stroke="currentColor" stroke-width="2" opacity="0.6"/>
            </svg>
          </button>
        </div>
      `;
    }).join('');

    // option click handlers
    [...optionsEl.querySelectorAll('[data-idx]')].forEach(btn => {
      btn.addEventListener('click', () => {
        const idx = Number(btn.getAttribute('data-idx'));
        chooseOption(idx);
      });
    });

    // option info handlers
    [...optionsEl.querySelectorAll('[data-info]')].forEach(btn => {
      btn.addEventListener('click', (ev) => {
        ev.stopPropagation();
        const idx = Number(btn.getAttribute('data-info'));
        const edge = oe[idx];
        const target = nodeById.get(edge.target);
        const refsUnion = [...new Set([...(refsForNode(n)), ...(refsForNode(target))])];
        const optExpl = getOptionExplanation(n.id, edge);
        const optBlock = optExpl
          ? `
            <div style="margin-top:0.75rem;">
              <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">What this choice means</div>
              ${toParagraphs(optExpl)}
            </div>
          `
          : `<p class=\"text-base\" style=\"margin-top:0.625rem; opacity:0.88;\">No injected documentation was found for this choice.</p>`;
        const imgFile = optionImageFile(n, edge, target);

        openModal({
          title: 'Option info',
          html: `
            ${renderModalImage(imgFile, edge?.label || edge?.metadata?.option_code || target?.label || 'Option')}
${optBlock}
          `,
          refs: refsUnion
        });
      });
    });

    if (animate) {
      bumpAnim(optionsEl, 'anim-swap');
      staggerAnim(optionsEl, ':scope > *', 55);
    }
  }

  function chooseOption(idx) {
    const oe = outEdges.get(currentNodeId) || [];
    const edge = oe[idx];
    if (!edge) return;

    history.push({ nodeId: currentNodeId, viaEdge: edge });
    currentNodeId = edge.target;
    renderAll(true);
  }

  // Copy path summary
  async function copyPathSummary() {
    const path = [...history.map(h => h.nodeId), currentNodeId].filter(Boolean);
    const lines = path.map((id, i) => {
      const n = nodeById.get(id);
      return `${i+1}. ${n?.label || id} (${id})`;
    });
    const current = nodeById.get(currentNodeId);
    const tests = (current?.metadata?.tests || []).join(', ');
    if (tests) lines.push(`
Recommended: ${tests}`);
    const txt = lines.join('\n');
    try {
      await navigator.clipboard.writeText(txt);
      openModal({ title: 'Copied!', html: `<p class="text-base">Your path summary is on the clipboard.</p><pre class="text-xs" style="white-space:pre-wrap; background:rgba(24,75,68,0.06); border:0.0625rem solid rgba(24,75,68,0.12); padding:0.625rem; border-radius:1rem; margin-top:0.625rem;">${escapeHtml(txt)}</pre>` });
    } catch {
      openModal({ title: 'Copy path summary', html: `<p class="text-base">Copy manually:</p><pre class="text-xs" style="white-space:pre-wrap; background:rgba(24,75,68,0.06); border:0.0625rem solid rgba(24,75,68,0.12); padding:0.625rem; border-radius:1rem; margin-top:0.625rem;">${escapeHtml(txt)}</pre>` });
    }
  }

  // References modal
  function openReferencesModal() {
    const list = REFERENCE_KEYS.map(k => {
      const num = REF_INDEX[k];
      const cite = escapeHtml(REFERENCES[k].ama);
      const url = REFERENCES[k].url;
      const link = url ? ` <a class="a" href="${url}" target="_blank" rel="noreferrer">Open</a>` : '';
      return `<li><span style="opacity:0.78;">[${num}]</span> ${cite}${link}</li>`;
    }).join('');

    openModal({
      title: 'References',
      html: `
        <p class="text-base">These are all of the references used to create this decision tree.</p>
        <ol class="ref-list text-sm font-normal" style="margin-top:0.625rem;">
          ${list}
        </ol>
      `
    });
  }

  // About modal
  function openAboutModal() {
    openModal({
      title: 'How this site works',
      html: `
        <p class="text-base">Akinator leads you to a test, by analysing what your research question is like</p>
        <ul class="text-base" style="margin:0.625rem 0 0 0; padding-left: 1.125rem;">
          <li><span class="font-semibold">Decision Panel</span> (right): shows the current question and its answer options.</li>
          <li><span class="font-semibold">Flow Lens</span> (left): an animated DAG map. The current node pulses; chosen edges stream.</li>
          <li><span class="font-semibold">Info icons</span> on questions and options open modals with citations.</li>
        </ul>
        <p class="text-base" style="margin-top:0.625rem;">Design intent: reduce cognitive load by keeping the full graph visible while focusing attention on one decision at a time.</p>
      `
    });
  }

  // Wire up header controls
  document.getElementById('btnRefs').onclick = openReferencesModal;
  document.getElementById('btnAbout').onclick = openAboutModal;
  document.getElementById('btnExport').onclick = copyPathSummary;

  document.getElementById('btnBack').onclick = () => {
    if (history.length === 0) return;
    const prev = history.pop();
    currentNodeId = prev.nodeId;
    renderAll(true);
  };
  document.getElementById('btnRestart').onclick = () => {
    history = [];
    currentNodeId = initialNodeId();
    renderAll(true);
  };

  // Logo fallback
  const logo = document.getElementById('logo');
  const logoFallback = document.getElementById('logoFallback');
  logo.addEventListener('error', () => {
    logo.style.display = 'none';
    logoFallback.style.display = 'inline-flex';
    logoFallback.setAttribute('aria-hidden', 'false');
  });

  // Search
  const searchInput = document.getElementById('searchInput');
  searchInput.addEventListener('keydown', (e) => {
    if (e.key !== 'Enter') return;
    const q = searchInput.value.trim().toLowerCase();
    if (!q) return;
    const hit = nodes.find(n => (n.label || '').toLowerCase().includes(q) || n.id.toLowerCase().includes(q));
    if (!hit) {
      openModal({ title: 'No match', html: `<p class="text-base">No node matched <span class="kbd">${escapeHtml(q)}</span>. Try a shorter keyword (e.g., “ROC”, “normal”, “paired”).</p>` });
      return;
    }
    history = [];
    currentNodeId = hit.id;
    renderAll(true);
  });


  // Graph rendering (focused, progressive disclosure)
  const svg = d3.select('#graphSvg');
  const defs = svg.append('defs');
  const gRoot = svg.append('g').attr('class', 'gRoot');
  const gEdges = gRoot.append('g').attr('class', 'edges');
  const gNodes = gRoot.append('g').attr('class', 'nodes');

  // Text measurement helpers (canvas) for stable wrapping/truncation
  const _measureCanvas = document.createElement('canvas');
  const _measureCtx = _measureCanvas.getContext('2d');
  function textWidth(txt) {
    // Match .node-text (12px Inter, weight 400)
    _measureCtx.font = `400 12px Inter, sans-serif`;
    return _measureCtx.measureText(txt).width || 0;
  }
  function fitEllipsis(str, maxW) {
    const ell = '…';
    if (textWidth(str) <= maxW) return str;
    let s = String(str);
    while (s.length > 0 && textWidth(s + ell) > maxW) s = s.slice(0, -1);
    return (s.length ? s : '') + ell;
  }

  function viewportSize() {
    const host = document.querySelector('.graph-canvas');
    const w = Math.max(320, host?.clientWidth || 900);
    const h = Math.max(260, host?.clientHeight || 560);
    return { w, h };
  }

  let zoom = d3.zoom()
    .scaleExtent([0.2, 24])
    .on('zoom', (event) => {
      gRoot.attr('transform', event.transform);
    });
  svg.call(zoom);

  let didInitialFit = false;

  function edgeId(e) { return `${e.source}→${e.target}`; }

  function safeId(id) { return String(id).replace(/[^a-zA-Z0-9_-]/g, '_'); }
  function clipId(id) { return `clip-${safeId(id)}`; }

  function focusSubgraph() {
    const nodeSet = new Set();
    const edgeSet = new Set();

    const path = [...history.map(h => h.nodeId), currentNodeId].filter(Boolean);

    // Always include the visited path
    for (const id of path) nodeSet.add(id);

    // Include edges along the chosen path
    for (const h of history) {
      if (h.viaEdge) edgeSet.add(edgeId(h.viaEdge));
    }

    // If we jumped in via Search, we may not have history — show predecessors for context
    if (history.length === 0) {
      const preds = (inEdges.get(currentNodeId) || []).slice(0, 6);
      for (const e of preds) {
        nodeSet.add(e.source);
        edgeSet.add(edgeId(e));
      }
    }

    // Context window: show options from the last two steps (parent + current)
    const lastK = 2;
    const ctx = path.slice(Math.max(0, path.length - lastK));
    for (const id of ctx) {
      const oes = outEdges.get(id) || [];
      for (const e of oes) {
        nodeSet.add(e.target);
        edgeSet.add(edgeId(e));
      }
    }

    // Preview one additional hop from current options (so terminal leaves are visible)
    const oes = outEdges.get(currentNodeId) || [];
    for (const e of oes) {
      const next = e.target;
      const oes2 = outEdges.get(next) || [];
      if (oes2.length && oes2.length <= 6) {
        for (const e2 of oes2) {
          nodeSet.add(e2.target);
          edgeSet.add(edgeId(e2));
        }
      }
    }

    const fNodes = nodes.filter(n => nodeSet.has(n.id));
    const fEdges = edges.filter(e => edgeSet.has(edgeId(e)) && nodeSet.has(e.source) && nodeSet.has(e.target));

    return { fNodes, fEdges, nodeSet, edgeSet };
  }

  // Compute dagre layout for the focused subgraph
  let layout = null;
  let focused = null;

  function computeLayout(fNodes, fEdges) {
    const g = new dagre.graphlib.Graph();
    g.setGraph({
      rankdir: 'LR',
      ranker: 'tight-tree',
      nodesep: 28,
      ranksep: 86,
      marginx: 30,
      marginy: 30
    });
    g.setDefaultEdgeLabel(() => ({}));

    for (const n of fNodes) {
      const label = n.label || n.id;
      const w = Math.min(300, 130 + Math.max(0, label.length - 16) * 4.2);
      const knd = nodeKind(n);
      const h = (knd === 'leaf' || knd === 'test') ? 60 : 74;
      g.setNode(n.id, { label, width: w, height: h });
    }
    for (const e of fEdges) {
      if (!g.hasNode(e.source) || !g.hasNode(e.target)) continue;
      g.setEdge(e.source, e.target, { label: e.label || '' });
    }

    dagre.layout(g);

    let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
    for (const id of g.nodes()) {
      const n = g.node(id);
      minX = Math.min(minX, n.x - n.width / 2);
      minY = Math.min(minY, n.y - n.height / 2);
      maxX = Math.max(maxX, n.x + n.width / 2);
      maxY = Math.max(maxY, n.y + n.height / 2);
    }

    return { g, minX, minY, maxX, maxY };
  }

  // Draw focused graph
  function drawGraph() {
    focused = focusSubgraph();
    layout = computeLayout(focused.fNodes, focused.fEdges);

    const { w, h } = viewportSize();
    svg.attr('viewBox', `0 0 ${w} ${h}`);

    // Clip paths keep node text strictly inside node bodies (prevents spillover)
    defs.selectAll('*').remove();
    for (const id of layout.g.nodes()) {
      const n = layout.g.node(id);
      const cp = defs.append('clipPath').attr('id', clipId(id));
      cp.append('rect')
        .attr('x', -n.width/2)
        .attr('y', -n.height/2)
        .attr('width', n.width)
        .attr('height', n.height)
        .attr('rx', 18)
        .attr('ry', 18);
    }

    const edgeData = focused.fEdges.map((e) => {
      const ed = layout.g.edge(e.source, e.target);
      return {
        id: edgeId(e),
        source: e.source,
        target: e.target,
        label: e.label || '',
        points: ed?.points || []
      };
    });

    const line = d3.line()
      .x(d => d.x)
      .y(d => d.y)
      .curve(d3.curveBasis);

    const edgeSel = gEdges.selectAll('path.edge').data(edgeData, d => d.id);

    edgeSel.enter()
      .append('path')
      .attr('class', 'edge')
      .attr('data-eid', d => d.id)
      .attr('d', d => line(d.points))
      .attr('opacity', 0.0)
      .transition()
      .duration(380)
      .attr('opacity', 1.0);

    edgeSel.attr('d', d => line(d.points));
    edgeSel.exit().remove();

    const nodeData = focused.fNodes.map(n => {
      const ln = layout.g.node(n.id);
      return {
        id: n.id,
        label: n.label || n.id,
        kind: nodeKind(n),
        x: ln?.x || 0,
        y: ln?.y || 0,
        width: ln?.width || 160,
        height: ln?.height || 70
      };
    });

    const nodeSel = gNodes.selectAll('g.node').data(nodeData, d => d.id);

    const nodeEnter = nodeSel.enter()
      .append('g')
      .attr('class', d => `node ${(d.kind === 'question') ? 'question' : ''} ${(d.kind === 'leaf') ? 'leaf' : ''} ${(d.kind === 'test') ? 'test leaf' : ''}`)
      .attr('transform', d => `translate(${d.x},${d.y})`)
      .style('cursor', 'pointer');

    nodeEnter.append('circle')
      .attr('class', 'node-ring')
      .attr('r', d => Math.max(26, Math.min(36, d.height/2)));

    nodeEnter.append('rect')
      .attr('class', 'node-body')
      .attr('x', d => -d.width/2)
      .attr('y', d => -d.height/2)
      .attr('width', d => d.width)
      .attr('height', d => d.height)
      .attr('rx', 18)
      .attr('ry', 18);

    nodeEnter.append('text')
      .attr('class', 'node-text')
      .attr('text-anchor', 'middle')
      .attr('dominant-baseline', 'central')
      .attr('x', 0)
      .attr('y', 0)
      .attr('clip-path', d => `url(#${clipId(d.id)})`)
      .each(function(d) {
        const maxWidth = d.width - 18;
        const maxLines = 3;
        const lineH = 14;

        const raw = String(d.label || '').trim();
        const words = raw ? raw.split(/\s+/).filter(Boolean) : [];
        const lines = [];
        let cur = '';
        let truncated = false;

        for (let i = 0; i < words.length; i++) {
          if (lines.length >= maxLines) { truncated = true; break; }
          const w = words[i];
          const cand = cur ? (cur + ' ' + w) : w;

          if (textWidth(cand) <= maxWidth) {
            cur = cand;
            continue;
          }

          if (!cur) {
            // Single token too wide; hard-truncate it.
            lines.push(fitEllipsis(w, maxWidth));
            continue;
          }

          lines.push(cur);
          cur = w;

          if (textWidth(cur) > maxWidth) cur = fitEllipsis(cur, maxWidth);
        }

        if (cur && lines.length < maxLines) lines.push(cur);
        if (!lines.length) lines.push('');

        // If anything didn't fit, ellipsize the last visible line.
        if (truncated) {
          const lastIdx = Math.min(lines.length, maxLines) - 1;
          lines[lastIdx] = fitEllipsis(lines[lastIdx], maxWidth);
        }

        const displayed = lines.slice(0, maxLines);
        const textSel = d3.select(this);
        textSel.selectAll('tspan').remove();

        const startDy = -((displayed.length - 1) * lineH) / 2;
        displayed.forEach((ln, i) => {
          textSel.append('tspan')
            .attr('x', 0)
            .attr('dy', i === 0 ? startDy : lineH)
            .text(ln);
        });
      });

    nodeEnter.on('click', (event, d) => {
      const n = nodeById.get(d.id);
      const kind = nodeKind(n);
      const refs = refsForNode(n);

      let body = '';
      if (kind === 'question') {
        const qExpl = getNodeExplanation(n.id);
        body = qExpl
          ? `
            <div style="margin-top:0.625rem;">
              ${toParagraphs(qExpl)}
            </div>
          `
          : `<p class="text-base">No explanation was found for this question in the injected documentation.</p>`;
      } else if (kind === 'leaf') {
        const tests = (n.metadata?.tests || []).map(t => String(t));
        const notes = (n.metadata?.notes || '').trim();
        body = `
          <p class="text-base"><span class="font-semibold">Recommendation:</span> ${escapeHtml(n?.label || d.id)}</p>

          ${tests.length ? `
            <div style="margin-top:0.75rem;">
              <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">Suggested tests / procedures</div>
              <ul class="text-base" style="margin:0; padding-left:1.125rem;">
                ${tests.map(t => `<li>${escapeHtml(t)}</li>`).join('')}
              </ul>
            </div>
          ` : ''}

          ${notes ? `
            <div style="margin-top:0.75rem;">
              <div class="text-base font-bold" style="opacity:0.92; margin-bottom:0.5rem;">Notes</div>
              ${toParagraphs(notes)}
            </div>
          ` : ''}
        `;
      } else if (kind === 'test') {
        const testName = String(n?.label || '').trim();
        const tExpl = getTestExplanation(testName);
        body = tExpl
          ? `
            <div style="margin-top:0.625rem;">
              ${toParagraphs(tExpl)}
            </div>
          `
          : `<p class="text-base">No explanation was found for this test in the injected documentation.</p>`;
      } else {
        body = `<p class="text-base">No additional details are available for this node.</p>`;
      }

      const modalTitle =
        (kind === 'test') ? (String(n?.label || 'Suggested test')) :
        (kind === 'leaf') ? 'Recommendation node' :
        (kind === 'question') ? 'Question node' :
        'Node';

      const imgFile = (kind === 'test') ? testImageFile(String(n?.label || modalTitle)) : nodeImageFile(n);

      openModal({
        title: modalTitle,
        html: `
          ${renderModalImage(imgFile, modalTitle)}
          ${body}
        `,
        refs
      });
    });

    nodeSel
      .attr('transform', d => `translate(${d.x},${d.y})`)
      .attr('class', d => `node ${(d.kind === 'question') ? 'question' : ''} ${(d.kind === 'leaf') ? 'leaf' : ''} ${(d.kind === 'test') ? 'test leaf' : ''}`);

    nodeSel.exit().remove();
  }


  function immediateFocusSet() {
    const focus = new Set();
    const cur = currentNodeId;
    if (cur) focus.add(cur);

    // The immediate predecessor is the most recent decision in history (if any)
    const prev = history.length ? history[history.length - 1].nodeId : null;
    if (prev) focus.add(prev);

    // The immediate successors are the option targets from the current question
    const oes = outEdges.get(cur) || [];
    for (const e of oes) {
      focus.add(e.target);

      // If the immediate successor is a recommendation (leaf), include its explicit per-test children too
      const next = nodeById.get(e.target);
      if (next && nodeKind(next) === 'leaf') {
        const oes2 = outEdges.get(e.target) || [];
        for (const e2 of oes2) {
          const child = nodeById.get(e2.target);
          if (child && nodeKind(child) === 'test') focus.add(e2.target);
        }
      }
    }

    return focus;
  }

  function renderHighlights() {
    // Active (chosen) edges along the visited path
    const activeEdgeIds = new Set(
      history
        .filter(h => h.viaEdge)
        .map(h => edgeId(h.viaEdge))
    );

    const focus = immediateFocusSet();

    gEdges.selectAll('path.edge')
      .classed('active', d => activeEdgeIds.has(d.id))
      .classed('is-muted', d => !activeEdgeIds.has(d.id) && !(focus.has(d.source) || focus.has(d.target)));

    gNodes.selectAll('g.node')
      .classed('active', d => d.id === currentNodeId)
      .classed('is-muted', d => !focus.has(d.id));
  }

  function fitToGraph(animate = false) {
    if (!layout) return;
    const { w, h } = viewportSize();
    const pad = 46;
    const gw = (layout.maxX - layout.minX) + pad * 2;
    const gh = (layout.maxY - layout.minY) + pad * 2;
    const k = Math.min(1.15, Math.max(0.30, Math.min(w / gw, h / gh)));

    const tx = (w - k * gw) / 2 - k * (layout.minX - pad);
    const ty = (h - k * gh) / 2 - k * (layout.minY - pad);

    const t = d3.zoomIdentity.translate(tx, ty).scale(k);
    if (animate) svg.transition().duration(420).call(zoom.transform, t);
    else svg.call(zoom.transform, t);
  }

  function centerOnNode(id, animate = false) {
    if (!layout) return;
    const n = layout.g.node(id);
    if (!n) return;

    const { w, h } = viewportSize();
    const currentT = d3.zoomTransform(svg.node());
    const k = currentT.k || 1;

    const tx = w / 2 - n.x * k;
    const ty = h / 2 - n.y * k;

    const t = d3.zoomIdentity.translate(tx, ty).scale(k);
    if (animate) svg.transition().duration(420).call(zoom.transform, t);
    else svg.call(zoom.transform, t);
  }

  

  function focusOnCurrent(animate = false) {
    if (!layout) return;

    const cur = layout.g.node(currentNodeId);
    if (!cur) return;

    // Compute a bounding box for the *immediate* focus window (prev/current/next),
    // but choose a zoom level that is readable (not tiny) and place the current
    // question near the left-center of the viewport.
    const focusIds = Array.from(immediateFocusSet());
    const { w, h } = viewportSize();
    const margin = 24;
    const anchorY = h / 2;

    let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
    for (const id of focusIds) {
      const n = layout.g.node(id);
      if (!n) continue;
      minX = Math.min(minX, n.x - n.width / 2);
      minY = Math.min(minY, n.y - n.height / 2);
      maxX = Math.max(maxX, n.x + n.width / 2);
      maxY = Math.max(maxY, n.y + n.height / 2);
    }

    if (!Number.isFinite(minX)) {
      centerOnNode(currentNodeId, animate);
      return;
    }

    // Readability target: node text is 12px; aim for >= ~14px effective.
    const readableMinK = 14 / 12;     // ~1.1667
    const preferredK   = 1.85;        // comfortable "zoomed in" for reading
    const maxK = 24;                  // matches scaleExtent max
    const minK = 0.2;

    // Start near "far-left" and adjust to avoid clipping at the chosen scale.
    let anchorX = Math.max(w * 0.22, margin + 8);

    function kLimitForAnchor(ax) {
      const dxLeft  = Math.max(0, cur.x - minX);
      const dxRight = Math.max(0, maxX - cur.x);
      const dyTop   = Math.max(0, cur.y - minY);
      const dyBot   = Math.max(0, maxY - cur.y);

      let kLim = maxK;

      if (dxLeft  > 0) kLim = Math.min(kLim, (ax - margin) / dxLeft);
      if (dxRight > 0) kLim = Math.min(kLim, (w - margin - ax) / dxRight);
      if (dyTop   > 0) kLim = Math.min(kLim, (anchorY - margin) / dyTop);
      if (dyBot   > 0) kLim = Math.min(kLim, (h - margin - anchorY) / dyBot);

      if (!Number.isFinite(kLim) || kLim <= 0) kLim = readableMinK;
      return Math.max(minK, Math.min(maxK, kLim));
    }

    // A tiny 2-iteration refinement so the node never gets clipped on the left.
    let k = readableMinK;
    for (let i = 0; i < 2; i++) {
      const kLim = kLimitForAnchor(anchorX);
      k = Math.max(readableMinK, Math.min(preferredK, kLim));

      // Ensure the node-body is fully visible at the chosen scale.
      const minAnchorForNode = margin + (cur.width * k) / 2 + 10;
      anchorX = Math.max(anchorX, minAnchorForNode);
    }

    // Translate so current node center lands at (anchorX, anchorY).
    const tx = anchorX - cur.x * k;
    const ty = anchorY - cur.y * k;

    const t = d3.zoomIdentity.translate(tx, ty).scale(k);
    if (animate) svg.transition().duration(420).call(zoom.transform, t);
    else svg.call(zoom.transform, t);
  }

document.getElementById('btnCenter').onclick = () => focusOnCurrent(true);

  // Render everything
    function renderAll(animate = false) {
    renderCrumbs(animate);
    renderPanel(animate);
    drawGraph();
    renderHighlights();

    // Establish an initial fit once, but afterwards keep the user's zoom level.
    if (!didInitialFit) {
      fitToGraph(false);
      didInitialFit = true;
    }

    // Always keep the current question/node centered when a question is being displayed.
    focusOnCurrent(animate);
  }
// First paint
  renderAll(true);
  // Re-render once fonts are ready so wrapping/truncation is maximally accurate
  if (document.fonts && document.fonts.ready) document.fonts.ready.then(() => renderAll(false));
  </script>
</body>
</html>
